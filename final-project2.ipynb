{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "998891eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "import zipfile\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6406d2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c06d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not hasattr(np, 'int'):\n",
    "    np.int = int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd63e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model class (must match the architecture used in final-project1)\n",
    "class CNN_LSTM_Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=4, lstm_hidden=256):\n",
    "        super(CNN_LSTM_Classifier, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.3),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.4),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        )\n",
    "        self.feature_size = 128 * 16\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.feature_size,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.3,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=lstm_hidden * 2,\n",
    "            num_heads=8,\n",
    "            dropout=0.3,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden * 2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = x.contiguous().view(batch_size, x.size(1), -1)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        pooled = torch.mean(attn_out, dim=1)\n",
    "        output = self.classifier(pooled)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89e8b887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded: saved_models/original_cnn_lstm.pth\n",
      "âœ… Loaded: saved_models/rhythm_augmented_cnn_lstm.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the models\n",
    "original_path = os.path.join('saved_models', 'original_cnn_lstm.pth')\n",
    "rhythm_path = os.path.join('saved_models', 'rhythm_augmented_cnn_lstm.pth')\n",
    "\n",
    "model = CNN_LSTM_Classifier(num_classes=4, lstm_hidden=256).to(device)\n",
    "model.load_state_dict(torch.load(original_path, map_location=device))\n",
    "print(f\"âœ… Loaded: {original_path}\")\n",
    "\n",
    "rhythm_model = CNN_LSTM_Classifier(num_classes=4, lstm_hidden=256).to(device)\n",
    "rhythm_model.load_state_dict(torch.load(rhythm_path, map_location=device))\n",
    "print(f\"âœ… Loaded: {rhythm_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afa70024",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COMPOSERS = [\n",
    "    'Bach',\n",
    "    'Beethoven',\n",
    "    'Chopin',\n",
    "    'Mozart',\n",
    "]\n",
    "\n",
    "path = kagglehub.dataset_download(\"blanderbuss/midi-classic-music\")\n",
    "\n",
    "zip_path = os.path.join(path, 'midiclassics.zip')\n",
    "extract_path = os.path.join('data', 'kaggle', 'midiclassics')\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "# list files in extract_path that contain the target composers in name\n",
    "for composer in TARGET_COMPOSERS:\n",
    "    composer_files = [f for f in os.listdir(extract_path) if composer.lower() in f.lower()]\n",
    "\n",
    "# Only keep directories that contain a target composer's name\n",
    "for item in os.listdir(extract_path):\n",
    "    item_path = os.path.join(extract_path, item)\n",
    "    if not any(composer.lower() in item.lower() for composer in TARGET_COMPOSERS):\n",
    "        if os.path.isfile(item_path):\n",
    "            os.remove(item_path)\n",
    "        elif os.path.isdir(item_path):\n",
    "            shutil.rmtree(item_path)\n",
    "\n",
    "# also delete \"C.P.E.Bach\" files. This was the son of J.S. Bach, and we want to keep only the main composers\n",
    "for item in os.listdir(extract_path):\n",
    "    if 'C.P.E.Bach' in item:\n",
    "        item_path = os.path.join(extract_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            os.remove(item_path)\n",
    "        elif os.path.isdir(item_path):\n",
    "            shutil.rmtree(item_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7eb056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PianoRollDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        # Add channel dimension for CNN: (1, 128, T)\n",
    "        return self.data[idx].unsqueeze(0), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad781104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_piano_roll_improved(midi_path, fs=100, target_duration=45.0):\n",
    "    \"\"\"\n",
    "    Improved MIDI to piano roll conversion with musical awareness\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pm = pretty_midi.PrettyMIDI(midi_path)\n",
    "        \n",
    "        # Get the actual duration of the piece\n",
    "        actual_duration = pm.get_end_time()\n",
    "        \n",
    "        # If piece is very short, skip it\n",
    "        if actual_duration < 10.0:  # Less than 10 seconds\n",
    "            return None\n",
    "            \n",
    "        # For long pieces, extract multiple segments\n",
    "        if actual_duration > target_duration * 1.5:\n",
    "            # Extract from different parts of the piece\n",
    "            segments = []\n",
    "            num_segments = min(3, int(actual_duration // target_duration))\n",
    "            \n",
    "            for i in range(num_segments):\n",
    "                start_time = i * (actual_duration / num_segments)\n",
    "                end_time = start_time + target_duration\n",
    "                \n",
    "                # Create a copy and trim\n",
    "                pm_segment = pretty_midi.PrettyMIDI()\n",
    "                for instrument in pm.instruments:\n",
    "                    new_instrument = pretty_midi.Instrument(\n",
    "                        program=instrument.program,\n",
    "                        is_drum=instrument.is_drum,\n",
    "                        name=instrument.name\n",
    "                    )\n",
    "                    \n",
    "                    for note in instrument.notes:\n",
    "                        if start_time <= note.start < end_time:\n",
    "                            new_note = pretty_midi.Note(\n",
    "                                velocity=note.velocity,\n",
    "                                pitch=note.pitch,\n",
    "                                start=note.start - start_time,\n",
    "                                end=min(note.end - start_time, target_duration)\n",
    "                            )\n",
    "                            new_instrument.notes.append(new_note)\n",
    "                    \n",
    "                    if new_instrument.notes:\n",
    "                        pm_segment.instruments.append(new_instrument)\n",
    "                \n",
    "                if pm_segment.instruments:\n",
    "                    piano_roll = pm_segment.get_piano_roll(fs=fs)\n",
    "                    target_length = int(target_duration * fs)\n",
    "                    \n",
    "                    if piano_roll.shape[1] > target_length:\n",
    "                        piano_roll = piano_roll[:, :target_length]\n",
    "                    else:\n",
    "                        pad_width = target_length - piano_roll.shape[1]\n",
    "                        piano_roll = np.pad(piano_roll, ((0,0),(0,pad_width)), mode='constant')\n",
    "                    \n",
    "                    segments.append(piano_roll)\n",
    "            \n",
    "            return segments\n",
    "        \n",
    "        else:\n",
    "            # For normal length pieces, use the whole piece\n",
    "            piano_roll = pm.get_piano_roll(fs=fs)\n",
    "            target_length = int(target_duration * fs)\n",
    "            \n",
    "            if piano_roll.shape[1] > target_length:\n",
    "                # Take from the middle rather than truncating end\n",
    "                start_idx = (piano_roll.shape[1] - target_length) // 2\n",
    "                piano_roll = piano_roll[:, start_idx:start_idx + target_length]\n",
    "            else:\n",
    "                pad_width = target_length - piano_roll.shape[1]\n",
    "                piano_roll = np.pad(piano_roll, ((0,0),(0,pad_width)), mode='constant')\n",
    "            \n",
    "            return [piano_roll]\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {midi_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "230bdd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_piano_roll(piano_roll):\n",
    "    \"\"\"\n",
    "    Apply musical normalization to piano roll\n",
    "    \"\"\"\n",
    "    # 1. Velocity normalization (already 0-1 from pretty_midi)\n",
    "    normalized = piano_roll.copy()\n",
    "    \n",
    "    # 2. Optional: Focus on active pitch range\n",
    "    active_pitches = np.any(normalized > 0, axis=1)\n",
    "    if np.any(active_pitches):\n",
    "        first_active = np.argmax(active_pitches)\n",
    "        last_active = len(active_pitches) - 1 - np.argmax(active_pitches[::-1])\n",
    "        \n",
    "        # Ensure we keep a reasonable range (at least 60 semitones = 5 octaves)\n",
    "        min_range = 60\n",
    "        current_range = last_active - first_active + 1\n",
    "        \n",
    "        if current_range < min_range:\n",
    "            expand = (min_range - current_range) // 2\n",
    "            first_active = max(0, first_active - expand)\n",
    "            last_active = min(127, last_active + expand)\n",
    "    \n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1b1f0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Improved data processing functions defined!\n",
      "Key improvements:\n",
      "â€¢ Intelligent segment extraction for long pieces\n",
      "â€¢ Musical boundary awareness\n",
      "â€¢ Better normalization\n",
      "â€¢ Feature extraction for analysis\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_musical_features(piano_roll):\n",
    "    \"\"\"\n",
    "    Extract features that capture musical style\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Temporal features\n",
    "    note_density_timeline = np.sum(piano_roll > 0, axis=0)\n",
    "    features['avg_notes_per_time'] = np.mean(note_density_timeline)\n",
    "    features['note_density_variance'] = np.var(note_density_timeline)\n",
    "    \n",
    "    # Pitch features\n",
    "    pitch_activity = np.sum(piano_roll > 0, axis=1)\n",
    "    active_pitches = pitch_activity > 0\n",
    "    if np.any(active_pitches):\n",
    "        features['pitch_range'] = np.sum(active_pitches)\n",
    "        features['lowest_pitch'] = np.argmax(active_pitches)\n",
    "        features['highest_pitch'] = 127 - np.argmax(active_pitches[::-1])\n",
    "    else:\n",
    "        features['pitch_range'] = 0\n",
    "        features['lowest_pitch'] = 60  # Middle C\n",
    "        features['highest_pitch'] = 60\n",
    "    \n",
    "    # Rhythmic features\n",
    "    onset_pattern = np.diff(note_density_timeline > 0).astype(int)\n",
    "    features['onset_density'] = np.sum(onset_pattern == 1) / len(onset_pattern)\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"âœ… Improved data processing functions defined!\")\n",
    "print(\"Key improvements:\")\n",
    "print(\"â€¢ Intelligent segment extraction for long pieces\")\n",
    "print(\"â€¢ Musical boundary awareness\")\n",
    "print(\"â€¢ Better normalization\")\n",
    "print(\"â€¢ Feature extraction for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9771e8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting improved data loading...\n",
      "ðŸŽµ LOADING DATASET WITH IMPROVED PROCESSING...\n",
      "Improvements over original:\n",
      "â€¢ Intelligent segment extraction for long pieces\n",
      "â€¢ Better handling of piece lengths\n",
      "â€¢ Musical feature extraction\n",
      "â€¢ Quality filtering\n",
      "\n",
      "--- Processing Bach ---\n",
      "  Processed 10 files, created 27 segments...\n",
      "  Processed 20 files, created 54 segments...\n",
      "  Processed 30 files, created 82 segments...\n",
      "  Processed 40 files, created 110 segments...\n",
      "  Processed 50 files, created 131 segments...\n",
      "âœ… Bach: 50 files â†’ 131 segments\n",
      "  Final data shape: (131, 128, 4500)\n",
      "\n",
      "--- Processing Beethoven ---\n",
      "  Processed 10 files, created 27 segments...\n",
      "  Processed 20 files, created 55 segments...\n",
      "  Processed 30 files, created 81 segments...\n",
      "  Processed 40 files, created 104 segments...\n",
      "  Processed 50 files, created 128 segments...\n",
      "âœ… Beethoven: 50 files â†’ 128 segments\n",
      "  Final data shape: (128, 128, 4500)\n",
      "\n",
      "--- Processing Chopin ---\n",
      "  Processed 10 files, created 23 segments...\n",
      "  Processed 20 files, created 44 segments...\n",
      "  Processed 30 files, created 67 segments...\n",
      "  Processed 40 files, created 93 segments...\n",
      "  Processed 50 files, created 119 segments...\n",
      "âœ… Chopin: 50 files â†’ 119 segments\n",
      "  Final data shape: (119, 128, 4500)\n",
      "\n",
      "--- Processing Mozart ---\n",
      "  Processed 10 files, created 30 segments...\n",
      "  Processed 20 files, created 59 segments...\n",
      "  Processed 30 files, created 82 segments...\n",
      "  Processed 40 files, created 107 segments...\n",
      "  Processed 50 files, created 132 segments...\n",
      "âœ… Mozart: 50 files â†’ 132 segments\n",
      "  Final data shape: (132, 128, 4500)\n",
      "\n",
      "ðŸŽ¯ FINAL IMPROVED DATASET:\n",
      "Total samples: 510\n",
      "Data shape: (510, 128, 4500)\n",
      "Label distribution: [131 128 119 132]\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# IMPROVED DATA LOADING WITH BETTER PROCESSING\n",
    "# =====================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_improved_dataset(extract_path, target_composers, target_duration=45.0, max_files_per_composer=None):\n",
    "    \"\"\"\n",
    "    Load dataset with improved processing that addresses previous shortcomings\n",
    "    \"\"\"\n",
    "    print(\"ðŸŽµ LOADING DATASET WITH IMPROVED PROCESSING...\")\n",
    "    print(\"Improvements over original:\")\n",
    "    print(\"â€¢ Intelligent segment extraction for long pieces\")\n",
    "    print(\"â€¢ Better handling of piece lengths\")\n",
    "    print(\"â€¢ Musical feature extraction\")\n",
    "    print(\"â€¢ Quality filtering\")\n",
    "    \n",
    "    composer_to_idx = {c: i for i, c in enumerate(target_composers)}\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    all_features = []\n",
    "    \n",
    "    for composer in target_composers:\n",
    "        print(f\"\\n--- Processing {composer} ---\")\n",
    "        composer_dir = os.path.join(extract_path, composer)\n",
    "        \n",
    "        if not os.path.isdir(composer_dir):\n",
    "            print(f\"Directory not found: {composer_dir}\")\n",
    "            continue\n",
    "            \n",
    "        composer_data = []\n",
    "        composer_labels = []\n",
    "        composer_features = []\n",
    "        files_processed = 0\n",
    "        segments_created = 0\n",
    "        \n",
    "        midi_files = [f for f in os.listdir(composer_dir) \n",
    "                     if f.lower().endswith(('.mid', '.midi'))]\n",
    "        \n",
    "        if max_files_per_composer:\n",
    "            midi_files = midi_files[:max_files_per_composer]\n",
    "        \n",
    "        for file in midi_files:\n",
    "            midi_path = os.path.join(composer_dir, file)\n",
    "            \n",
    "            try:\n",
    "                # Use improved processing\n",
    "                segments = get_piano_roll_improved(midi_path, target_duration=target_duration)\n",
    "                \n",
    "                if segments is None:\n",
    "                    continue\n",
    "                    \n",
    "                for segment in segments:\n",
    "                    # Normalize the segment\n",
    "                    normalized_segment = normalize_piano_roll(segment)\n",
    "                    \n",
    "                    # Extract musical features\n",
    "                    features = extract_musical_features(normalized_segment)\n",
    "                    \n",
    "                    # Quality check: skip if too sparse\n",
    "                    note_density = features['avg_notes_per_time']\n",
    "                    if note_density < 0.1:  # Very sparse, likely poor quality\n",
    "                        continue\n",
    "                    \n",
    "                    composer_data.append(normalized_segment)\n",
    "                    composer_labels.append(composer_to_idx[composer])\n",
    "                    composer_features.append(features)\n",
    "                    segments_created += 1\n",
    "                \n",
    "                files_processed += 1\n",
    "                \n",
    "                if files_processed % 10 == 0:\n",
    "                    print(f\"  Processed {files_processed} files, created {segments_created} segments...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"âœ… {composer}: {files_processed} files â†’ {segments_created} segments\")\n",
    "        \n",
    "        if composer_data:\n",
    "            composer_data = np.array(composer_data)\n",
    "            composer_labels = np.array(composer_labels)\n",
    "            \n",
    "            all_data.append(composer_data)\n",
    "            all_labels.append(composer_labels) \n",
    "            all_features.extend(composer_features)\n",
    "            \n",
    "            print(f\"  Final data shape: {composer_data.shape}\")\n",
    "    \n",
    "    # Combine all data\n",
    "    if all_data:\n",
    "        data = np.concatenate(all_data, axis=0)\n",
    "        labels = np.concatenate(all_labels, axis=0)\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ FINAL IMPROVED DATASET:\")\n",
    "        print(f\"Total samples: {len(data)}\")\n",
    "        print(f\"Data shape: {data.shape}\")\n",
    "        print(f\"Label distribution: {np.bincount(labels)}\")\n",
    "        \n",
    "        return data, labels, all_features\n",
    "    else:\n",
    "        print(\"âŒ No data loaded!\")\n",
    "        return None, None, None\n",
    "\n",
    "# Load the improved dataset\n",
    "print(\"ðŸš€ Starting improved data loading...\")\n",
    "improved_data, improved_labels, features = load_improved_dataset(\n",
    "    extract_path, \n",
    "    TARGET_COMPOSERS,\n",
    "    target_duration=45.0,  # 45 seconds per segment\n",
    "    max_files_per_composer=50  # Limit for testing - remove for full dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80f24ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª TESTING MODELS ON IMPROVED DATASET\n",
      "==================================================\n",
      "\n",
      "ðŸ“Š Original Model Results:\n",
      "Accuracy: 70.59%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach      0.708     0.908     0.796       131\n",
      "   Beethoven      0.697     0.484     0.571       128\n",
      "      Chopin      0.746     0.765     0.755       119\n",
      "      Mozart      0.672     0.667     0.669       132\n",
      "\n",
      "    accuracy                          0.706       510\n",
      "   macro avg      0.706     0.706     0.698       510\n",
      "weighted avg      0.705     0.706     0.697       510\n",
      "\n",
      "\n",
      "ðŸ“Š Rhythm Model Results:\n",
      "Accuracy: 73.14%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Bach      0.797     0.809     0.803       131\n",
      "   Beethoven      0.633     0.727     0.676       128\n",
      "      Chopin      0.847     0.790     0.817       119\n",
      "      Mozart      0.672     0.606     0.637       132\n",
      "\n",
      "    accuracy                          0.731       510\n",
      "   macro avg      0.737     0.733     0.734       510\n",
      "weighted avg      0.735     0.731     0.732       510\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# TEST TRAINED MODELS ON IMPROVED DATASET\n",
    "# =====================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create dataset and dataloader for improved data\n",
    "improved_dataset = PianoRollDataset(improved_data, improved_labels)\n",
    "test_loader = DataLoader(improved_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def evaluate_model(model, dataloader, model_name):\n",
    "    \"\"\"Evaluate a single model\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * np.mean(np.array(all_preds) == np.array(all_targets))\n",
    "    \n",
    "    print(f\"\\nðŸ“Š {model_name} Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_targets, all_preds, \n",
    "                              target_names=TARGET_COMPOSERS, \n",
    "                              digits=3))\n",
    "    \n",
    "    return all_preds, all_targets, all_probs, accuracy\n",
    "\n",
    "# Test both models\n",
    "print(\"ðŸ§ª TESTING MODELS ON IMPROVED DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test original model\n",
    "orig_preds, targets, orig_probs, orig_acc = evaluate_model(model, test_loader, \"Original Model\")\n",
    "\n",
    "# Test rhythm model  \n",
    "rhythm_preds, _, rhythm_probs, rhythm_acc = evaluate_model(rhythm_model, test_loader, \"Rhythm Model\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "orig_probs = np.array(orig_probs)\n",
    "rhythm_probs = np.array(rhythm_probs)\n",
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0aefe6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Non-overlapping segmentation function defined!\n"
     ]
    }
   ],
   "source": [
    "def get_piano_roll_non_overlapping_segments(midi_path, fs=100, segment_duration=45.0):\n",
    "    \"\"\"\n",
    "    Extract non-overlapping segments to avoid data leakage with LSTM\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pm = pretty_midi.PrettyMIDI(midi_path)\n",
    "        \n",
    "        # Get the actual duration of the piece\n",
    "        actual_duration = pm.get_end_time()\n",
    "        \n",
    "        # If piece is very short, skip it\n",
    "        if actual_duration < 15.0:  # Less than 15 seconds\n",
    "            return None\n",
    "            \n",
    "        segments = []\n",
    "        segment_size = segment_duration\n",
    "        \n",
    "        # Calculate number of non-overlapping segments\n",
    "        num_segments = int(actual_duration // segment_size)\n",
    "        \n",
    "        # If piece is shorter than one segment, use the whole piece (padded)\n",
    "        if num_segments == 0:\n",
    "            piano_roll = pm.get_piano_roll(fs=fs)\n",
    "            target_length = int(segment_duration * fs)\n",
    "            \n",
    "            if piano_roll.shape[1] < target_length:\n",
    "                pad_width = target_length - piano_roll.shape[1]\n",
    "                piano_roll = np.pad(piano_roll, ((0,0),(0,pad_width)), mode='constant')\n",
    "            else:\n",
    "                piano_roll = piano_roll[:, :target_length]\n",
    "            \n",
    "            segments.append(piano_roll)\n",
    "            return segments\n",
    "        \n",
    "        # Extract non-overlapping segments\n",
    "        for i in range(num_segments):\n",
    "            start_time = i * segment_size\n",
    "            end_time = start_time + segment_size\n",
    "            \n",
    "            # Create a copy and trim\n",
    "            pm_segment = pretty_midi.PrettyMIDI()\n",
    "            for instrument in pm.instruments:\n",
    "                new_instrument = pretty_midi.Instrument(\n",
    "                    program=instrument.program,\n",
    "                    is_drum=instrument.is_drum,\n",
    "                    name=instrument.name\n",
    "                )\n",
    "                \n",
    "                for note in instrument.notes:\n",
    "                    if start_time <= note.start < end_time:\n",
    "                        new_note = pretty_midi.Note(\n",
    "                            velocity=note.velocity,\n",
    "                            pitch=note.pitch,\n",
    "                            start=note.start - start_time,\n",
    "                            end=min(note.end - start_time, segment_duration)\n",
    "                        )\n",
    "                        new_instrument.notes.append(new_note)\n",
    "                \n",
    "                if new_instrument.notes:\n",
    "                    pm_segment.instruments.append(new_instrument)\n",
    "            \n",
    "            if pm_segment.instruments:\n",
    "                piano_roll = pm_segment.get_piano_roll(fs=fs)\n",
    "                target_length = int(segment_duration * fs)\n",
    "                \n",
    "                if piano_roll.shape[1] > target_length:\n",
    "                    piano_roll = piano_roll[:, :target_length]\n",
    "                else:\n",
    "                    pad_width = target_length - piano_roll.shape[1]\n",
    "                    piano_roll = np.pad(piano_roll, ((0,0),(0,pad_width)), mode='constant')\n",
    "                \n",
    "                segments.append(piano_roll)\n",
    "        \n",
    "        return segments if segments else None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {midi_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"âœ… Non-overlapping segmentation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43715e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting FULL non-overlapping segment data loading...\n",
      "ðŸŽµ LOADING FULL DATASET WITH NON-OVERLAPPING SEGMENTS...\n",
      "Benefits for LSTM training:\n",
      "â€¢ Segment duration: 45.0s (no overlap)\n",
      "â€¢ Using ALL available files (no limits)\n",
      "â€¢ No data leakage between train/test\n",
      "â€¢ Cleaner temporal boundaries\n",
      "â€¢ Will address class imbalance later during training\n",
      "\n",
      "--- Processing Bach ---\n",
      "  Found 131 MIDI files for Bach\n",
      "  Processed 50/131 files, created 355 segments...\n",
      "  Processed 100/131 files, created 733 segments...\n",
      "âœ… Bach: 131/131 files â†’ 977 segments\n",
      "  Final data shape: (977, 128, 4500)\n",
      "\n",
      "--- Processing Beethoven ---\n",
      "  Found 134 MIDI files for Beethoven\n",
      "  Processed 50/134 files, created 399 segments...\n",
      "  Processed 100/134 files, created 765 segments...\n",
      "Error processing data/kaggle/midiclassics/Beethoven/Anhang 14-3.mid: Could not decode key with 3 flats and mode 255\n",
      "âœ… Beethoven: 133/134 files â†’ 987 segments\n",
      "  Final data shape: (987, 128, 4500)\n",
      "\n",
      "--- Processing Chopin ---\n",
      "  Found 136 MIDI files for Chopin\n",
      "  Processed 50/136 files, created 235 segments...\n",
      "  Processed 100/136 files, created 467 segments...\n",
      "âœ… Chopin: 136/136 files â†’ 610 segments\n",
      "  Final data shape: (610, 128, 4500)\n",
      "\n",
      "--- Processing Mozart ---\n",
      "  Found 90 MIDI files for Mozart\n",
      "  Processed 50/90 files, created 263 segments...\n",
      "âœ… Mozart: 90/90 files â†’ 524 segments\n",
      "  Final data shape: (524, 128, 4500)\n",
      "\n",
      "ðŸŽ¯ FINAL FULL NON-OVERLAPPING DATASET:\n",
      "Total files processed: 490\n",
      "Total samples: 3098\n",
      "Data shape: (3098, 128, 4500)\n",
      "Label distribution: [977 987 610 524]\n",
      "  Bach: 977 samples (31.5%)\n",
      "  Beethoven: 987 samples (31.9%)\n",
      "  Chopin: 610 samples (19.7%)\n",
      "  Mozart: 524 samples (16.9%)\n"
     ]
    }
   ],
   "source": [
    "def load_dataset_non_overlapping_full(extract_path, target_composers, segment_duration=45.0):\n",
    "    \"\"\"\n",
    "    Load FULL dataset with non-overlapping segments - using ALL available files\n",
    "    \"\"\"\n",
    "    print(\"ðŸŽµ LOADING FULL DATASET WITH NON-OVERLAPPING SEGMENTS...\")\n",
    "    print(\"Benefits for LSTM training:\")\n",
    "    print(f\"â€¢ Segment duration: {segment_duration}s (no overlap)\")\n",
    "    print(\"â€¢ Using ALL available files (no limits)\")\n",
    "    print(\"â€¢ No data leakage between train/test\")\n",
    "    print(\"â€¢ Cleaner temporal boundaries\")\n",
    "    print(\"â€¢ Will address class imbalance later during training\")\n",
    "    \n",
    "    composer_to_idx = {c: i for i, c in enumerate(target_composers)}\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    all_features = []\n",
    "    \n",
    "    total_files_processed = 0\n",
    "    total_segments_created = 0\n",
    "    \n",
    "    for composer in target_composers:\n",
    "        print(f\"\\n--- Processing {composer} ---\")\n",
    "        composer_dir = os.path.join(extract_path, composer)\n",
    "        \n",
    "        if not os.path.isdir(composer_dir):\n",
    "            print(f\"Directory not found: {composer_dir}\")\n",
    "            continue\n",
    "            \n",
    "        composer_data = []\n",
    "        composer_labels = []\n",
    "        composer_features = []\n",
    "        files_processed = 0\n",
    "        segments_created = 0\n",
    "        \n",
    "        # Get ALL MIDI files - no limit\n",
    "        midi_files = [f for f in os.listdir(composer_dir) \n",
    "                     if f.lower().endswith(('.mid', '.midi'))]\n",
    "        \n",
    "        print(f\"  Found {len(midi_files)} MIDI files for {composer}\")\n",
    "        \n",
    "        for file in midi_files:\n",
    "            midi_path = os.path.join(composer_dir, file)\n",
    "            \n",
    "            try:\n",
    "                # Use non-overlapping segmentation\n",
    "                segments = get_piano_roll_non_overlapping_segments(\n",
    "                    midi_path, \n",
    "                    segment_duration=segment_duration\n",
    "                )\n",
    "                \n",
    "                if segments is None:\n",
    "                    continue\n",
    "                    \n",
    "                for segment in segments:\n",
    "                    # Normalize the segment\n",
    "                    normalized_segment = normalize_piano_roll(segment)\n",
    "                    \n",
    "                    # Extract musical features\n",
    "                    features = extract_musical_features(normalized_segment)\n",
    "                    \n",
    "                    # Quality check: skip if too sparse\n",
    "                    note_density = features['avg_notes_per_time']\n",
    "                    if note_density < 0.1:  # Very sparse, likely poor quality\n",
    "                        continue\n",
    "                    \n",
    "                    composer_data.append(normalized_segment)\n",
    "                    composer_labels.append(composer_to_idx[composer])\n",
    "                    composer_features.append(features)\n",
    "                    segments_created += 1\n",
    "                \n",
    "                files_processed += 1\n",
    "                \n",
    "                # Progress update every 50 files for full dataset\n",
    "                if files_processed % 50 == 0:\n",
    "                    print(f\"  Processed {files_processed}/{len(midi_files)} files, created {segments_created} segments...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"âœ… {composer}: {files_processed}/{len(midi_files)} files â†’ {segments_created} segments\")\n",
    "        \n",
    "        if composer_data:\n",
    "            composer_data = np.array(composer_data)\n",
    "            composer_labels = np.array(composer_labels)\n",
    "            \n",
    "            all_data.append(composer_data)\n",
    "            all_labels.append(composer_labels) \n",
    "            all_features.extend(composer_features)\n",
    "            \n",
    "            print(f\"  Final data shape: {composer_data.shape}\")\n",
    "        \n",
    "        total_files_processed += files_processed\n",
    "        total_segments_created += segments_created\n",
    "    \n",
    "    # Combine all data\n",
    "    if all_data:\n",
    "        data = np.concatenate(all_data, axis=0)\n",
    "        labels = np.concatenate(all_labels, axis=0)\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ FINAL FULL NON-OVERLAPPING DATASET:\")\n",
    "        print(f\"Total files processed: {total_files_processed}\")\n",
    "        print(f\"Total samples: {len(data)}\")\n",
    "        print(f\"Data shape: {data.shape}\")\n",
    "        print(f\"Label distribution: {np.bincount(labels)}\")\n",
    "        \n",
    "        # Show class distribution percentages\n",
    "        for i, composer in enumerate(target_composers):\n",
    "            count = np.sum(labels == i)\n",
    "            percentage = (count / len(labels)) * 100\n",
    "            print(f\"  {composer}: {count} samples ({percentage:.1f}%)\")\n",
    "        \n",
    "        return data, labels, all_features\n",
    "    else:\n",
    "        print(\"âŒ No data loaded!\")\n",
    "        return None, None, None\n",
    "\n",
    "# Load the FULL dataset with non-overlapping segments\n",
    "print(\"ðŸš€ Starting FULL non-overlapping segment data loading...\")\n",
    "full_data, full_labels, full_features = load_dataset_non_overlapping_full(\n",
    "    extract_path, \n",
    "    TARGET_COMPOSERS,\n",
    "    segment_duration=45.0  # 45-second segments, no overlap, ALL files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ecadfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Creating AGGRESSIVE model for A100 40GB...\n",
      "ðŸš€ Building AGGRESSIVE CNN-LSTM-Transformer for A100 40GB...\n",
      "â€¢ Deep CNN feature extraction (6 blocks)\n",
      "â€¢ Large LSTM temporal modeling (hidden: 512)\n",
      "â€¢ Deep Transformer self-attention (dim: 1024, heads: 16, layers: 8)\n",
      "â€¢ Multi-scale feature fusion\n",
      "â€¢ Advanced attention mechanisms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arr/miniconda3/envs/ds_base/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AGGRESSIVE CNN-LSTM-Transformer architecture built!\n",
      "Input shape: torch.Size([4, 1, 128, 4500])\n",
      "Main output shape: torch.Size([4, 4])\n",
      "Auxiliary output shape: torch.Size([4, 4])\n",
      "âœ… Aggressive model forward pass successful!\n",
      "\n",
      "ðŸ“Š AGGRESSIVE Model Statistics:\n",
      "Total parameters: 185,688,776\n",
      "Trainable parameters: 185,688,776\n",
      "Model size: ~708.3 MB\n",
      "Estimated GPU memory (training): ~2833.4 MB\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# AGGRESSIVE CNN-LSTM-TRANSFORMER FOR A100 40GB\n",
    "# =====================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Enhanced positional encoding for transformer\"\"\"\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class AggressiveCNN_LSTM_Transformer(nn.Module):\n",
    "    def __init__(self, num_classes=4, lstm_hidden=512, transformer_dim=1024, num_heads=16, num_layers=8):\n",
    "        super(AggressiveCNN_LSTM_Transformer, self).__init__()\n",
    "        \n",
    "        print(\"ðŸš€ Building AGGRESSIVE CNN-LSTM-Transformer for A100 40GB...\")\n",
    "        print(f\"â€¢ Deep CNN feature extraction (6 blocks)\")\n",
    "        print(f\"â€¢ Large LSTM temporal modeling (hidden: {lstm_hidden})\")\n",
    "        print(f\"â€¢ Deep Transformer self-attention (dim: {transformer_dim}, heads: {num_heads}, layers: {num_layers})\")\n",
    "        print(f\"â€¢ Multi-scale feature fusion\")\n",
    "        print(f\"â€¢ Advanced attention mechanisms\")\n",
    "        \n",
    "        # ==========================================\n",
    "        # DEEP CNN BACKBONE - 6 BLOCKS\n",
    "        # ==========================================\n",
    "        \n",
    "        # Block 1: Initial feature extraction\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))  # 128x64 -> 64x32\n",
    "        )\n",
    "        \n",
    "        # Block 2: Deeper features\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.15),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))  # 64x32 -> 32x16\n",
    "        )\n",
    "        \n",
    "        # Block 3: More complex patterns\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))  # 32x16 -> 16x8\n",
    "        )\n",
    "        \n",
    "        # Block 4: High-level features\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))  # 16x8 -> 8x4\n",
    "        )\n",
    "        \n",
    "        # Block 5: Abstract features\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 768, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(768),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(768, 768, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(768),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.3),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))  # 8x4 -> 4x2\n",
    "        )\n",
    "        \n",
    "        # Block 6: Final feature extraction\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(768, 1024, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.35),\n",
    "            nn.AdaptiveAvgPool2d((2, 1))  # Ensure consistent output: 2x1\n",
    "        )\n",
    "        \n",
    "        # ==========================================\n",
    "        # LARGE BIDIRECTIONAL LSTM\n",
    "        # ==========================================\n",
    "        self.feature_size = 1024 * 2  # 1024 channels * 2x1 spatial\n",
    "        self.lstm_hidden = lstm_hidden\n",
    "        \n",
    "        # Multi-layer LSTM with larger capacity\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.feature_size,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=4,  # Deeper LSTM\n",
    "            batch_first=True,\n",
    "            dropout=0.3,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Additional LSTM for temporal refinement\n",
    "        self.lstm_refine = nn.LSTM(\n",
    "            input_size=lstm_hidden * 2,\n",
    "            hidden_size=lstm_hidden // 2,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.2,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # ==========================================\n",
    "        # DEEP TRANSFORMER ENCODER\n",
    "        # ==========================================\n",
    "        self.transformer_dim = transformer_dim\n",
    "        \n",
    "        # Project LSTM output to transformer dimension\n",
    "        self.lstm_to_transformer = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden, transformer_dim),\n",
    "            nn.LayerNorm(transformer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(transformer_dim, max_len=10000)\n",
    "        \n",
    "        # Deep transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=transformer_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=transformer_dim * 4,  # Large feedforward\n",
    "            dropout=0.1,\n",
    "            activation='gelu',  # GELU activation for better performance\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Pre-norm for better training stability\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, \n",
    "            num_layers=num_layers,\n",
    "            norm=nn.LayerNorm(transformer_dim)\n",
    "        )\n",
    "        \n",
    "        # ==========================================\n",
    "        # MULTI-SCALE ATTENTION & FUSION\n",
    "        # ==========================================\n",
    "        \n",
    "        # Multi-scale attention heads\n",
    "        self.global_attention = nn.MultiheadAttention(\n",
    "            embed_dim=transformer_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.local_attention = nn.MultiheadAttention(\n",
    "            embed_dim=transformer_dim,\n",
    "            num_heads=num_heads // 2,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Cross-attention between global and local features\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=transformer_dim,\n",
    "            num_heads=num_heads // 2,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Feature fusion\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.Linear(transformer_dim * 3, transformer_dim),\n",
    "            nn.LayerNorm(transformer_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # ==========================================\n",
    "        # ADVANCED CLASSIFICATION HEAD\n",
    "        # ==========================================\n",
    "        \n",
    "        # Hierarchical classification with multiple paths\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(transformer_dim),\n",
    "            nn.Linear(transformer_dim, 2048),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Auxiliary classifier for regularization\n",
    "        self.aux_classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… AGGRESSIVE CNN-LSTM-Transformer architecture built!\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # ==========================================\n",
    "        # DEEP CNN FEATURE EXTRACTION\n",
    "        # ==========================================\n",
    "        x = self.conv1(x)      # (batch, 64, 64, T/2)\n",
    "        x = self.conv2(x)      # (batch, 128, 32, T/4)\n",
    "        x = self.conv3(x)      # (batch, 256, 16, T/8)\n",
    "        x = self.conv4(x)      # (batch, 512, 8, T/16)\n",
    "        x = self.conv5(x)      # (batch, 768, 4, T/32)\n",
    "        x = self.conv6(x)      # (batch, 1024, 2, T/64)\n",
    "        \n",
    "        # Reshape for LSTM: (batch, time_steps, features)\n",
    "        x = x.permute(0, 3, 1, 2)  # (batch, T/64, 1024, 2)\n",
    "        x = x.contiguous().view(batch_size, x.size(1), -1)  # (batch, T/64, 1024*2)\n",
    "        \n",
    "        # ==========================================\n",
    "        # LARGE LSTM PROCESSING\n",
    "        # ==========================================\n",
    "        lstm_out, _ = self.lstm(x)  # (batch, T/64, lstm_hidden*2)\n",
    "        lstm_refined, _ = self.lstm_refine(lstm_out)  # (batch, T/64, lstm_hidden)\n",
    "        \n",
    "        # Auxiliary classification from LSTM features (for regularization)\n",
    "        lstm_pooled = torch.mean(lstm_refined, dim=1)\n",
    "        aux_output = self.aux_classifier(lstm_pooled)\n",
    "        \n",
    "        # ==========================================\n",
    "        # DEEP TRANSFORMER PROCESSING\n",
    "        # ==========================================\n",
    "        \n",
    "        # Project to transformer dimension\n",
    "        transformer_input = self.lstm_to_transformer(lstm_refined)  # (batch, T/64, transformer_dim)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        transformer_input = transformer_input.transpose(0, 1)  # (T/64, batch, transformer_dim)\n",
    "        transformer_input = self.pos_encoding(transformer_input)\n",
    "        transformer_input = transformer_input.transpose(0, 1)  # (batch, T/64, transformer_dim)\n",
    "        \n",
    "        # Deep transformer encoding\n",
    "        transformer_out = self.transformer_encoder(transformer_input)  # (batch, T/64, transformer_dim)\n",
    "        \n",
    "        # ==========================================\n",
    "        # MULTI-SCALE ATTENTION & FUSION\n",
    "        # ==========================================\n",
    "        \n",
    "        # Global attention (full sequence)\n",
    "        global_attended, _ = self.global_attention(\n",
    "            transformer_out, transformer_out, transformer_out\n",
    "        )\n",
    "        \n",
    "        # Local attention (sliding window - simulate by chunking)\n",
    "        seq_len = transformer_out.size(1)\n",
    "        if seq_len > 16:\n",
    "            # Use overlapping windows\n",
    "            local_features = []\n",
    "            window_size = min(16, seq_len)\n",
    "            for i in range(0, max(1, seq_len - window_size + 1), window_size // 2):\n",
    "                end_idx = min(i + window_size, seq_len)\n",
    "                window = transformer_out[:, i:end_idx, :]\n",
    "                local_att, _ = self.local_attention(window, window, window)\n",
    "                local_features.append(torch.mean(local_att, dim=1, keepdim=True))\n",
    "            local_attended = torch.cat(local_features, dim=1)\n",
    "        else:\n",
    "            local_attended, _ = self.local_attention(\n",
    "                transformer_out, transformer_out, transformer_out\n",
    "            )\n",
    "        \n",
    "        # Cross attention between global and local\n",
    "        cross_attended, _ = self.cross_attention(\n",
    "            global_attended, local_attended, local_attended\n",
    "        )\n",
    "        \n",
    "        # Fusion of multi-scale features\n",
    "        # Pool to same size for concatenation\n",
    "        global_pooled = torch.mean(global_attended, dim=1)\n",
    "        local_pooled = torch.mean(local_attended, dim=1)\n",
    "        cross_pooled = torch.mean(cross_attended, dim=1)\n",
    "        \n",
    "        fused_features = torch.cat([global_pooled, local_pooled, cross_pooled], dim=1)\n",
    "        final_features = self.feature_fusion(fused_features)\n",
    "        \n",
    "        # ==========================================\n",
    "        # CLASSIFICATION\n",
    "        # ==========================================\n",
    "        main_output = self.classifier(final_features)\n",
    "        \n",
    "        return main_output, aux_output\n",
    "\n",
    "# Create the aggressive model\n",
    "print(\"ðŸš€ Creating AGGRESSIVE model for A100 40GB...\")\n",
    "aggressive_model = AggressiveCNN_LSTM_Transformer(\n",
    "    num_classes=4,\n",
    "    lstm_hidden=512,        # Doubled from 256\n",
    "    transformer_dim=1024,   # Doubled from 512  \n",
    "    num_heads=16,          # Doubled from 8\n",
    "    num_layers=8           # Doubled from 4\n",
    ").to(device)\n",
    "\n",
    "# Test with dummy input\n",
    "test_input = torch.randn(4, 1, 128, 4500).to(device)  # Larger batch\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    main_out, aux_out = aggressive_model(test_input)\n",
    "    print(f\"Main output shape: {main_out.shape}\")\n",
    "    print(f\"Auxiliary output shape: {aux_out.shape}\")\n",
    "    print(f\"âœ… Aggressive model forward pass successful!\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in aggressive_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in aggressive_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nðŸ“Š AGGRESSIVE Model Statistics:\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n",
    "print(f\"Estimated GPU memory (training): ~{total_params * 16 / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "145287c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting piece-tracked data loading...\n",
      "ðŸŽµ LOADING DATASET WITH PIECE TRACKING...\n",
      "Key improvements:\n",
      "â€¢ Track piece identity for each segment\n",
      "â€¢ Enable consecutive segment modeling\n",
      "â€¢ Support contrastive learning approaches\n",
      "â€¢ Segment duration: 45.0s\n",
      "\n",
      "--- Processing Bach ---\n",
      "  Found 131 MIDI files for Bach\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arr/miniconda3/envs/ds_base/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:97: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/131 files, created 355 segments...\n",
      "  Processed 100/131 files, created 733 segments...\n",
      "âœ… Bach: 131/131 files â†’ 977 segments\n",
      "\n",
      "--- Processing Beethoven ---\n",
      "  Found 134 MIDI files for Beethoven\n",
      "  Processed 50/134 files, created 399 segments...\n",
      "  Processed 100/134 files, created 765 segments...\n",
      "Error processing data/kaggle/midiclassics/Beethoven/Anhang 14-3.mid: Could not decode key with 3 flats and mode 255\n",
      "âœ… Beethoven: 133/134 files â†’ 987 segments\n",
      "\n",
      "--- Processing Chopin ---\n",
      "  Found 136 MIDI files for Chopin\n",
      "  Processed 50/136 files, created 235 segments...\n",
      "  Processed 100/136 files, created 467 segments...\n",
      "âœ… Chopin: 136/136 files â†’ 610 segments\n",
      "\n",
      "--- Processing Mozart ---\n",
      "  Found 90 MIDI files for Mozart\n",
      "  Processed 50/90 files, created 263 segments...\n",
      "âœ… Mozart: 90/90 files â†’ 524 segments\n",
      "\n",
      "ðŸŽ¯ FINAL PIECE-TRACKED DATASET:\n",
      "Total files processed: 490\n",
      "Total segments: 3098\n",
      "Total unique pieces: 490\n",
      "Data shape: (3098, 128, 4500)\n",
      "Label distribution: [977 987 610 524]\n",
      "Segments per piece - Mean: 6.3, Min: 1, Max: 30\n",
      "  Bach: 977 segments (31.5%)\n",
      "  Beethoven: 987 segments (31.9%)\n",
      "  Chopin: 610 segments (19.7%)\n",
      "  Mozart: 524 segments (16.9%)\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# SEQUENCE-AWARE DATA LOADING WITH PIECE TRACKING\n",
    "# =====================================================\n",
    "\n",
    "def load_dataset_with_piece_tracking(extract_path, target_composers, segment_duration=45.0):\n",
    "    \"\"\"\n",
    "    Load dataset while tracking which segments belong to the same piece\n",
    "    This enables sequence-aware training that distinguishes:\n",
    "    - Consecutive segments from SAME piece vs DIFFERENT pieces\n",
    "    - Same composer vs different composer relationships\n",
    "    \"\"\"\n",
    "    print(\"ðŸŽµ LOADING DATASET WITH PIECE TRACKING...\")\n",
    "    print(\"Key improvements:\")\n",
    "    print(f\"â€¢ Track piece identity for each segment\")\n",
    "    print(f\"â€¢ Enable consecutive segment modeling\")\n",
    "    print(f\"â€¢ Support contrastive learning approaches\")\n",
    "    print(f\"â€¢ Segment duration: {segment_duration}s\")\n",
    "    \n",
    "    composer_to_idx = {c: i for i, c in enumerate(target_composers)}\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    all_piece_ids = []  # NEW: Track which piece each segment comes from\n",
    "    all_piece_names = []  # NEW: Track piece names for analysis\n",
    "    \n",
    "    piece_id_counter = 0\n",
    "    total_files_processed = 0\n",
    "    total_segments_created = 0\n",
    "    \n",
    "    for composer in target_composers:\n",
    "        print(f\"\\n--- Processing {composer} ---\")\n",
    "        composer_dir = os.path.join(extract_path, composer)\n",
    "        \n",
    "        if not os.path.isdir(composer_dir):\n",
    "            print(f\"Directory not found: {composer_dir}\")\n",
    "            continue\n",
    "            \n",
    "        midi_files = [f for f in os.listdir(composer_dir) \n",
    "                     if f.lower().endswith(('.mid', '.midi'))]\n",
    "        \n",
    "        print(f\"  Found {len(midi_files)} MIDI files for {composer}\")\n",
    "        files_processed = 0\n",
    "        segments_created = 0\n",
    "        \n",
    "        for file in midi_files:\n",
    "            midi_path = os.path.join(composer_dir, file)\n",
    "            \n",
    "            try:\n",
    "                # Use non-overlapping segmentation\n",
    "                segments = get_piano_roll_non_overlapping_segments(\n",
    "                    midi_path, segment_duration=segment_duration\n",
    "                )\n",
    "                \n",
    "                if segments is None or len(segments) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # All segments from this file get the same piece_id\n",
    "                current_piece_id = piece_id_counter\n",
    "                piece_name = f\"{composer}_{file}\"\n",
    "                piece_id_counter += 1\n",
    "                \n",
    "                valid_segments_from_piece = 0\n",
    "                \n",
    "                for segment_idx, segment in enumerate(segments):\n",
    "                    # Normalize the segment\n",
    "                    normalized_segment = normalize_piano_roll(segment)\n",
    "                    \n",
    "                    # Extract musical features\n",
    "                    features = extract_musical_features(normalized_segment)\n",
    "                    \n",
    "                    # Quality check: skip if too sparse\n",
    "                    if features['avg_notes_per_time'] < 0.1:\n",
    "                        continue\n",
    "                    \n",
    "                    all_data.append(normalized_segment)\n",
    "                    all_labels.append(composer_to_idx[composer])\n",
    "                    all_piece_ids.append(current_piece_id)\n",
    "                    all_piece_names.append(f\"{piece_name}_seg{segment_idx}\")\n",
    "                    \n",
    "                    valid_segments_from_piece += 1\n",
    "                    segments_created += 1\n",
    "                \n",
    "                if valid_segments_from_piece > 0:\n",
    "                    files_processed += 1\n",
    "                \n",
    "                # Progress update\n",
    "                if files_processed % 50 == 0:\n",
    "                    print(f\"  Processed {files_processed}/{len(midi_files)} files, created {segments_created} segments...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"âœ… {composer}: {files_processed}/{len(midi_files)} files â†’ {segments_created} segments\")\n",
    "        total_files_processed += files_processed\n",
    "        total_segments_created += segments_created\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    data = np.array(all_data)\n",
    "    labels = np.array(all_labels)\n",
    "    piece_ids = np.array(all_piece_ids)\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ FINAL PIECE-TRACKED DATASET:\")\n",
    "    print(f\"Total files processed: {total_files_processed}\")\n",
    "    print(f\"Total segments: {len(data)}\")\n",
    "    print(f\"Total unique pieces: {len(np.unique(piece_ids))}\")\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    print(f\"Label distribution: {np.bincount(labels)}\")\n",
    "    \n",
    "    # Analyze segments per piece\n",
    "    segments_per_piece = []\n",
    "    for piece_id in np.unique(piece_ids):\n",
    "        count = np.sum(piece_ids == piece_id)\n",
    "        segments_per_piece.append(count)\n",
    "    \n",
    "    print(f\"Segments per piece - Mean: {np.mean(segments_per_piece):.1f}, \"\n",
    "          f\"Min: {np.min(segments_per_piece)}, Max: {np.max(segments_per_piece)}\")\n",
    "    \n",
    "    # Show class distribution percentages\n",
    "    for i, composer in enumerate(target_composers):\n",
    "        count = np.sum(labels == i)\n",
    "        percentage = (count / len(labels)) * 100\n",
    "        print(f\"  {composer}: {count} segments ({percentage:.1f}%)\")\n",
    "    \n",
    "    return data, labels, piece_ids, all_piece_names\n",
    "\n",
    "# Load the dataset with piece tracking\n",
    "print(\"ðŸš€ Starting piece-tracked data loading...\")\n",
    "tracked_data, tracked_labels, tracked_piece_ids, piece_names = load_dataset_with_piece_tracking(\n",
    "    extract_path, \n",
    "    TARGET_COMPOSERS,\n",
    "    segment_duration=45.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a85def4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš–ï¸ COMPUTING CLASS WEIGHTS...\n",
      "Original distribution:\n",
      "  Bach: 977 samples (31.5%)\n",
      "  Beethoven: 987 samples (31.9%)\n",
      "  Chopin: 610 samples (19.7%)\n",
      "  Mozart: 524 samples (16.9%)\n",
      "\n",
      "Computed class weights:\n",
      "  Bach: 0.793\n",
      "  Beethoven: 0.785\n",
      "  Chopin: 1.270\n",
      "  Mozart: 1.478\n",
      "\n",
      "âœ… Class weights ready for CrossEntropyLoss\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# CLASS WEIGHTS IMPLEMENTATION\n",
    "# =====================================================\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn as nn\n",
    "\n",
    "# Compute class weights for our imbalanced dataset\n",
    "def compute_class_weights(labels, target_composers):\n",
    "    \"\"\"\n",
    "    Compute class weights to handle imbalance\n",
    "    \"\"\"\n",
    "    print(\"âš–ï¸ COMPUTING CLASS WEIGHTS...\")\n",
    "    \n",
    "    # Get unique labels and compute balanced weights\n",
    "    unique_labels = np.unique(labels)\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced', \n",
    "        classes=unique_labels, \n",
    "        y=labels\n",
    "    )\n",
    "    \n",
    "    print(f\"Original distribution:\")\n",
    "    for i, composer in enumerate(target_composers):\n",
    "        count = np.sum(labels == i)\n",
    "        percentage = (count / len(labels)) * 100\n",
    "        print(f\"  {composer}: {count} samples ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nComputed class weights:\")\n",
    "    for i, (composer, weight) in enumerate(zip(target_composers, class_weights)):\n",
    "        print(f\"  {composer}: {weight:.3f}\")\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "    \n",
    "    print(f\"\\nâœ… Class weights ready for CrossEntropyLoss\")\n",
    "    return class_weights_tensor\n",
    "\n",
    "# Compute class weights for our tracked dataset\n",
    "class_weights = compute_class_weights(tracked_labels, TARGET_COMPOSERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5673124f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Creating sequence-aware dataset...\n",
      "ðŸ“Š SEQUENCE DATASET CREATED:\n",
      "â€¢ Sequence length: 2\n",
      "â€¢ Total pieces: 490\n",
      "â€¢ Total sequences: 5706\n",
      "â€¢ Include single segments: True\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# SEQUENCE-AWARE DATASET FOR CONSECUTIVE SEGMENTS\n",
    "# =====================================================\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class SequenceAwareDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that creates sequences of consecutive segments from same pieces\n",
    "    This enables the model to learn temporal relationships within compositions\n",
    "    \"\"\"\n",
    "    def __init__(self, data, labels, piece_ids, sequence_length=2, include_singles=True):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.piece_ids = torch.tensor(piece_ids, dtype=torch.long)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.include_singles = include_singles\n",
    "        \n",
    "        # Group segments by piece\n",
    "        self.piece_segments = {}\n",
    "        for idx, piece_id in enumerate(piece_ids):\n",
    "            piece_id = int(piece_id)\n",
    "            if piece_id not in self.piece_segments:\n",
    "                self.piece_segments[piece_id] = []\n",
    "            self.piece_segments[piece_id].append(idx)\n",
    "        \n",
    "        # Create sequence indices\n",
    "        self.sequence_indices = self._create_sequence_indices()\n",
    "        \n",
    "        print(f\"ðŸ“Š SEQUENCE DATASET CREATED:\")\n",
    "        print(f\"â€¢ Sequence length: {sequence_length}\")\n",
    "        print(f\"â€¢ Total pieces: {len(self.piece_segments)}\")\n",
    "        print(f\"â€¢ Total sequences: {len(self.sequence_indices)}\")\n",
    "        print(f\"â€¢ Include single segments: {include_singles}\")\n",
    "    \n",
    "    def _create_sequence_indices(self):\n",
    "        \"\"\"Create indices for all possible sequences\"\"\"\n",
    "        sequences = []\n",
    "        \n",
    "        # Add consecutive sequences from same pieces\n",
    "        for piece_id, segment_indices in self.piece_segments.items():\n",
    "            if len(segment_indices) >= self.sequence_length:\n",
    "                # Create all possible consecutive sequences from this piece\n",
    "                for start_idx in range(len(segment_indices) - self.sequence_length + 1):\n",
    "                    seq_indices = segment_indices[start_idx:start_idx + self.sequence_length]\n",
    "                    sequences.append({\n",
    "                        'type': 'sequence',\n",
    "                        'indices': seq_indices,\n",
    "                        'piece_id': piece_id,\n",
    "                        'label': int(self.labels[seq_indices[0]])  # All should have same label\n",
    "                    })\n",
    "        \n",
    "        # Optionally add single segments\n",
    "        if self.include_singles:\n",
    "            for piece_id, segment_indices in self.piece_segments.items():\n",
    "                for idx in segment_indices:\n",
    "                    sequences.append({\n",
    "                        'type': 'single',\n",
    "                        'indices': [idx],\n",
    "                        'piece_id': piece_id,\n",
    "                        'label': int(self.labels[idx])\n",
    "                    })\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequence_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence_info = self.sequence_indices[idx]\n",
    "        indices = sequence_info['indices']\n",
    "        \n",
    "        if len(indices) == 1:\n",
    "            # Single segment\n",
    "            segment = self.data[indices[0]].unsqueeze(0)  # Add channel dim\n",
    "            return segment, sequence_info['label'], sequence_info['piece_id'], 'single'\n",
    "        else:\n",
    "            # Multiple segments - stack them\n",
    "            segments = []\n",
    "            for seg_idx in indices:\n",
    "                segments.append(self.data[seg_idx].unsqueeze(0))  # Add channel dim\n",
    "            \n",
    "            # Stack segments: (sequence_length, 1, 128, T)\n",
    "            sequence = torch.stack(segments, dim=0)\n",
    "            \n",
    "            return sequence, sequence_info['label'], sequence_info['piece_id'], 'sequence'\n",
    "\n",
    "# Create sequence-aware dataset\n",
    "print(\"ðŸ”— Creating sequence-aware dataset...\")\n",
    "sequence_dataset = SequenceAwareDataset(\n",
    "    tracked_data, \n",
    "    tracked_labels, \n",
    "    tracked_piece_ids,\n",
    "    sequence_length=2,  # Start with pairs\n",
    "    include_singles=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7683e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ CREATING BALANCED TRAINING SETUP...\n",
      "Using class weights: tensor([0.7927, 0.7847, 1.2697, 1.4781], device='mps:0')\n",
      "ðŸ“Š DATA SPLIT:\n",
      "Train pieces: 318 | segments: 2015\n",
      "Val pieces:   74 | segments: 471\n",
      "Test pieces:  98 | segments: 612\n",
      "\n",
      "Train distribution:\n",
      "  Bach: 602 (29.9%)\n",
      "  Beethoven: 643 (31.9%)\n",
      "  Chopin: 433 (21.5%)\n",
      "  Mozart: 337 (16.7%)\n",
      "\n",
      "Val distribution:\n",
      "  Bach: 171 (36.3%)\n",
      "  Beethoven: 175 (37.2%)\n",
      "  Chopin: 68 (14.4%)\n",
      "  Mozart: 57 (12.1%)\n",
      "\n",
      "Test distribution:\n",
      "  Bach: 204 (33.3%)\n",
      "  Beethoven: 169 (27.6%)\n",
      "  Chopin: 109 (17.8%)\n",
      "  Mozart: 130 (21.2%)\n",
      "\n",
      "âœ… Weighted loss function created with class weights\n",
      "\n",
      "ðŸ“š DATA LOADERS CREATED:\n",
      "â€¢ Batch size: 16\n",
      "â€¢ Train batches: 126\n",
      "â€¢ Val batches: 30\n",
      "â€¢ Test batches: 39\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# TRAINING SETUP WITH CLASS WEIGHTS\n",
    "# =====================================================\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_balanced_training_setup(data, labels, piece_ids, class_weights, test_size=0.2, val_size=0.15):\n",
    "    \"\"\"\n",
    "    Create training setup with class weights for imbalance handling\n",
    "    \"\"\"\n",
    "    print(\"ðŸŽ¯ CREATING BALANCED TRAINING SETUP...\")\n",
    "    print(f\"Using class weights: {class_weights}\")\n",
    "    \n",
    "    # Split data at the piece level to avoid data leakage\n",
    "    unique_pieces = np.unique(piece_ids)\n",
    "    piece_labels = np.array([labels[piece_ids == pid][0] for pid in unique_pieces])\n",
    "    \n",
    "    # Split pieces into train/val/test\n",
    "    train_pieces, test_pieces, _, _ = train_test_split(\n",
    "        unique_pieces, piece_labels, \n",
    "        test_size=test_size, \n",
    "        stratify=piece_labels, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    train_pieces, val_pieces, _, _ = train_test_split(\n",
    "        train_pieces, piece_labels[np.isin(unique_pieces, train_pieces)], \n",
    "        test_size=val_size/(1-test_size), \n",
    "        stratify=piece_labels[np.isin(unique_pieces, train_pieces)], \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create masks for train/val/test\n",
    "    train_mask = np.isin(piece_ids, train_pieces)\n",
    "    val_mask = np.isin(piece_ids, val_pieces)\n",
    "    test_mask = np.isin(piece_ids, test_pieces)\n",
    "    \n",
    "    print(f\"ðŸ“Š DATA SPLIT:\")\n",
    "    print(f\"Train pieces: {len(train_pieces)} | segments: {np.sum(train_mask)}\")\n",
    "    print(f\"Val pieces:   {len(val_pieces)} | segments: {np.sum(val_mask)}\")\n",
    "    print(f\"Test pieces:  {len(test_pieces)} | segments: {np.sum(test_mask)}\")\n",
    "    \n",
    "    # Show class distribution per split\n",
    "    for split_name, mask in [(\"Train\", train_mask), (\"Val\", val_mask), (\"Test\", test_mask)]:\n",
    "        split_labels = labels[mask]\n",
    "        print(f\"\\n{split_name} distribution:\")\n",
    "        for i, composer in enumerate(TARGET_COMPOSERS):\n",
    "            count = np.sum(split_labels == i)\n",
    "            percentage = (count / len(split_labels)) * 100 if len(split_labels) > 0 else 0\n",
    "            print(f\"  {composer}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = PianoRollDataset(data[train_mask], labels[train_mask])\n",
    "    val_dataset = PianoRollDataset(data[val_mask], labels[val_mask])\n",
    "    test_dataset = PianoRollDataset(data[test_mask], labels[test_mask])\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset, train_mask, val_mask, test_mask\n",
    "\n",
    "# Create the balanced training setup\n",
    "train_dataset, val_dataset, test_dataset, train_mask, val_mask, test_mask = create_balanced_training_setup(\n",
    "    tracked_data, tracked_labels, tracked_piece_ids, class_weights\n",
    ")\n",
    "\n",
    "# Create weighted loss function\n",
    "weighted_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "print(f\"\\nâœ… Weighted loss function created with class weights\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16  # Smaller batch size for stability\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\nðŸ“š DATA LOADERS CREATED:\")\n",
    "print(f\"â€¢ Batch size: {batch_size}\")\n",
    "print(f\"â€¢ Train batches: {len(train_loader)}\")\n",
    "print(f\"â€¢ Val batches: {len(val_loader)}\")\n",
    "print(f\"â€¢ Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd4b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# ADVANCED TRAINING LOOP FOR AGGRESSIVE MODEL\n",
    "# =====================================================\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_aggressive_model(model, train_loader, val_loader, class_weights, \n",
    "                          epochs=50, initial_lr=1e-3, save_path='aggressive_model.pth'):\n",
    "    \"\"\"\n",
    "    Advanced training with:\n",
    "    - Auxiliary loss for regularization\n",
    "    - Cosine annealing with warm restarts\n",
    "    - Mixed precision training\n",
    "    - Advanced metrics tracking\n",
    "    - Early stopping with patience\n",
    "    \"\"\"\n",
    "    print(\"ðŸš€ STARTING AGGRESSIVE MODEL TRAINING...\")\n",
    "    print(f\"â€¢ Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"â€¢ Training samples: {len(train_loader.dataset)}\")\n",
    "    print(f\"â€¢ Validation samples: {len(val_loader.dataset)}\")\n",
    "    print(f\"â€¢ Epochs: {epochs}\")\n",
    "    print(f\"â€¢ Initial learning rate: {initial_lr}\")\n",
    "    print(f\"â€¢ Using mixed precision: True\")\n",
    "    print(f\"â€¢ Class weights: {class_weights}\")\n",
    "    \n",
    "    # Setup optimizers and schedulers\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=initial_lr, \n",
    "        weight_decay=1e-4,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    # Cosine annealing with warm restarts\n",
    "    scheduler = CosineAnnealingWarmRestarts(\n",
    "        optimizer, \n",
    "        T_0=10,  # Restart every 10 epochs\n",
    "        T_mult=2,  # Double the period after each restart\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # Mixed precision scaler\n",
    "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
    "    \n",
    "    # Loss functions\n",
    "    main_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    aux_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    # Training tracking\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'train_main_loss': [], 'train_aux_loss': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_main_loss': [], 'val_aux_loss': [],\n",
    "        'lr': [], 'epoch_time': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    patience = 15  # Early stopping patience\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ Training Configuration:\")\n",
    "    print(f\"â€¢ Optimizer: AdamW (lr={initial_lr}, weight_decay=1e-4)\")\n",
    "    print(f\"â€¢ Scheduler: CosineAnnealingWarmRestarts (T_0=10, T_mult=2)\")\n",
    "    print(f\"â€¢ Main loss weight: 0.7, Auxiliary loss weight: 0.3\")\n",
    "    print(f\"â€¢ Early stopping patience: {patience}\")\n",
    "    print(f\"â€¢ Mixed precision: {'CUDA' if scaler else 'Disabled'}\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # ==========================================\n",
    "        # TRAINING PHASE\n",
    "        # ==========================================\n",
    "        model.train()\n",
    "        train_metrics = defaultdict(float)\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        print(f\"\\nðŸ“ˆ Epoch {epoch+1}/{epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            if scaler:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    main_output, aux_output = model(data)\n",
    "                    main_loss = main_criterion(main_output, target)\n",
    "                    aux_loss = aux_criterion(aux_output, target)\n",
    "                    # Combine losses with weights\n",
    "                    total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
    "                \n",
    "                # Mixed precision backward pass\n",
    "                scaler.scale(total_loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                main_output, aux_output = model(data)\n",
    "                main_loss = main_criterion(main_output, target)\n",
    "                aux_loss = aux_criterion(aux_output, target)\n",
    "                total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
    "                \n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            train_metrics['total_loss'] += total_loss.item()\n",
    "            train_metrics['main_loss'] += main_loss.item()\n",
    "            train_metrics['aux_loss'] += aux_loss.item()\n",
    "            \n",
    "            # Accuracy from main output\n",
    "            _, predicted = torch.max(main_output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "            \n",
    "            # Progress update\n",
    "            if batch_idx % 20 == 0:\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                print(f\"  Batch {batch_idx:3d}/{len(train_loader)} | \"\n",
    "                      f\"Loss: {total_loss.item():.4f} | \"\n",
    "                      f\"Acc: {100.*train_correct/train_total:.2f}% | \"\n",
    "                      f\"LR: {current_lr:.2e}\")\n",
    "        \n",
    "        # ==========================================\n",
    "        # VALIDATION PHASE\n",
    "        # ==========================================\n",
    "        model.eval()\n",
    "        val_metrics = defaultdict(float)\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                \n",
    "                if scaler:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        main_output, aux_output = model(data)\n",
    "                        main_loss = main_criterion(main_output, target)\n",
    "                        aux_loss = aux_criterion(aux_output, target)\n",
    "                        total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
    "                else:\n",
    "                    main_output, aux_output = model(data)\n",
    "                    main_loss = main_criterion(main_output, target)\n",
    "                    aux_loss = aux_criterion(aux_output, target)\n",
    "                    total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
    "                \n",
    "                val_metrics['total_loss'] += total_loss.item()\n",
    "                val_metrics['main_loss'] += main_loss.item()\n",
    "                val_metrics['aux_loss'] += aux_loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(main_output.data, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        train_loss = train_metrics['total_loss'] / len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        val_loss = val_metrics['total_loss'] / len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['train_main_loss'].append(train_metrics['main_loss'] / len(train_loader))\n",
    "        history['train_aux_loss'].append(train_metrics['aux_loss'] / len(train_loader))\n",
    "        \n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_main_loss'].append(val_metrics['main_loss'] / len(val_loader))\n",
    "        history['val_aux_loss'].append(val_metrics['aux_loss'] / len(val_loader))\n",
    "        \n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nðŸ“Š Epoch {epoch+1} Summary:\")\n",
    "        print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
    "        print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
    "        print(f\"  LR: {current_lr:.2e}, Time: {epoch_time:.1f}s\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'history': history\n",
    "            }, save_path)\n",
    "            print(f\"  ðŸ’¾ New best model saved! Val Acc: {val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  â³ Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nðŸ›‘ Early stopping triggered! Best Val Acc: {best_val_acc:.2f}%\")\n",
    "            break\n",
    "        \n",
    "        # Memory cleanup\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\nâœ… Training completed!\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"Model saved to: {save_path}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Create save directory\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "# Start training the aggressive model\n",
    "print(\"ðŸš€ Starting training of AGGRESSIVE CNN-LSTM-Transformer...\")\n",
    "history = train_aggressive_model(\n",
    "    aggressive_model,\n",
    "    train_loader,\n",
    "    val_loader, \n",
    "    class_weights,\n",
    "    epochs=50,\n",
    "    initial_lr=1e-3,\n",
    "    save_path='saved_models/aggressive_cnn_lstm_transformer.pth'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
