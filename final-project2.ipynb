{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "caJC4Lf_md2R",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caJC4Lf_md2R",
        "outputId": "ca8cc17c-6ebb-4d10-934d-8e0796ae5b98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m178.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (2.0.2)\n",
            "Collecting mido>=1.1.16 (from pretty_midi)\n",
            "  Downloading mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.7.14)\n",
            "Downloading mido-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pretty_midi\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592286 sha256=6b7d68d4fa0805eb47fc8952dc81a689bab8826193208904e3ba2c3a5a908348\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/95/ac/15ceaeb2823b04d8e638fd1495357adb8d26c00ccac9d7782e\n",
            "Successfully built pretty_midi\n",
            "Installing collected packages: mido, pretty_midi\n",
            "Successfully installed mido-1.3.3 pretty_midi-0.2.10\n"
          ]
        }
      ],
      "source": [
        "%pip install pretty_midi kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "998891eb",
      "metadata": {
        "id": "998891eb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import zipfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import numpy as np\n",
        "import pretty_midi\n",
        "import torch\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6406d2f8",
      "metadata": {
        "id": "6406d2f8"
      },
      "outputs": [],
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c06d7fe",
      "metadata": {
        "id": "2c06d7fe"
      },
      "outputs": [],
      "source": [
        "if not hasattr(np, 'int'):\n",
        "    np.int = int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd63e1d1",
      "metadata": {
        "id": "dd63e1d1"
      },
      "outputs": [],
      "source": [
        "# Define your model class (must match the architecture used in final-project1)\n",
        "class CNN_LSTM_Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=4, lstm_hidden=256):\n",
        "        super(CNN_LSTM_Classifier, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.2),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.3),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.4),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "        self.feature_size = 128 * 16\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.feature_size,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=0.3,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=lstm_hidden * 2,\n",
        "            num_heads=8,\n",
        "            dropout=0.3,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(lstm_hidden * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = x.contiguous().view(batch_size, x.size(1), -1)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
        "        pooled = torch.mean(attn_out, dim=1)\n",
        "        output = self.classifier(pooled)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89e8b887",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89e8b887",
        "outputId": "4f57181d-aee5-414e-d78f-387317635aa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running in Google Colab.\n",
            "✅ Loaded: /content/saved_models/original_cnn_lstm.pth\n",
            "✅ Loaded: /content/saved_models/rhythm_augmented_cnn_lstm.pth\n"
          ]
        }
      ],
      "source": [
        "# Load the models\n",
        "from IPython import get_ipython\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    print(\"Running in Google Colab.\")\n",
        "    original_path = os.path.join('/content/saved_models', 'original_cnn_lstm.pth')\n",
        "    rhythm_path = os.path.join('/content/saved_models', 'rhythm_augmented_cnn_lstm.pth')\n",
        "else:\n",
        "    print(\"Not running in Google Colab.\")\n",
        "    original_path = os.path.join('saved_models', 'original_cnn_lstm.pth')\n",
        "    rhythm_path = os.path.join('saved_models', 'rhythm_augmented_cnn_lstm.pth')\n",
        "\n",
        "model = CNN_LSTM_Classifier(num_classes=4, lstm_hidden=256).to(device)\n",
        "model.load_state_dict(torch.load(original_path, map_location=device))\n",
        "print(f\"✅ Loaded: {original_path}\")\n",
        "\n",
        "rhythm_model = CNN_LSTM_Classifier(num_classes=4, lstm_hidden=256).to(device)\n",
        "rhythm_model.load_state_dict(torch.load(rhythm_path, map_location=device))\n",
        "print(f\"✅ Loaded: {rhythm_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "afa70024",
      "metadata": {
        "id": "afa70024"
      },
      "outputs": [],
      "source": [
        "TARGET_COMPOSERS = [\n",
        "    'Bach',\n",
        "    'Beethoven',\n",
        "    'Chopin',\n",
        "    'Mozart',\n",
        "]\n",
        "\n",
        "path = kagglehub.dataset_download(\"blanderbuss/midi-classic-music\")\n",
        "\n",
        "zip_path = os.path.join(path, 'midiclassics.zip')\n",
        "extract_path = os.path.join('data', 'kaggle', 'midiclassics')\n",
        "\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# list files in extract_path that contain the target composers in name\n",
        "for composer in TARGET_COMPOSERS:\n",
        "    composer_files = [f for f in os.listdir(extract_path) if composer.lower() in f.lower()]\n",
        "\n",
        "# Only keep directories that contain a target composer's name\n",
        "for item in os.listdir(extract_path):\n",
        "    item_path = os.path.join(extract_path, item)\n",
        "    if not any(composer.lower() in item.lower() for composer in TARGET_COMPOSERS):\n",
        "        if os.path.isfile(item_path):\n",
        "            os.remove(item_path)\n",
        "        elif os.path.isdir(item_path):\n",
        "            shutil.rmtree(item_path)\n",
        "\n",
        "# also delete \"C.P.E.Bach\" files. This was the son of J.S. Bach, and we want to keep only the main composers\n",
        "for item in os.listdir(extract_path):\n",
        "    if 'C.P.E.Bach' in item:\n",
        "        item_path = os.path.join(extract_path, item)\n",
        "        if os.path.isfile(item_path):\n",
        "            os.remove(item_path)\n",
        "        elif os.path.isdir(item_path):\n",
        "            shutil.rmtree(item_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c7eb056a",
      "metadata": {
        "id": "c7eb056a"
      },
      "outputs": [],
      "source": [
        "class PianoRollDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        # Add channel dimension for CNN: (1, 128, T)\n",
        "        return self.data[idx].unsqueeze(0), self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ad781104",
      "metadata": {
        "id": "ad781104"
      },
      "outputs": [],
      "source": [
        "def get_piano_roll_improved(midi_path, fs=100, target_duration=45.0):\n",
        "    \"\"\"\n",
        "    Improved MIDI to piano roll conversion with musical awareness\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pm = pretty_midi.PrettyMIDI(midi_path)\n",
        "\n",
        "        # Get the actual duration of the piece\n",
        "        actual_duration = pm.get_end_time()\n",
        "\n",
        "        # If piece is very short, skip it\n",
        "        if actual_duration < 10.0:  # Less than 10 seconds\n",
        "            return None\n",
        "\n",
        "        # For long pieces, extract multiple segments\n",
        "        if actual_duration > target_duration * 1.5:\n",
        "            # Extract from different parts of the piece\n",
        "            segments = []\n",
        "            num_segments = min(3, int(actual_duration // target_duration))\n",
        "\n",
        "            for i in range(num_segments):\n",
        "                start_time = i * (actual_duration / num_segments)\n",
        "                end_time = start_time + target_duration\n",
        "\n",
        "                # Create a copy and trim\n",
        "                pm_segment = pretty_midi.PrettyMIDI()\n",
        "                for instrument in pm.instruments:\n",
        "                    new_instrument = pretty_midi.Instrument(\n",
        "                        program=instrument.program,\n",
        "                        is_drum=instrument.is_drum,\n",
        "                        name=instrument.name\n",
        "                    )\n",
        "\n",
        "                    for note in instrument.notes:\n",
        "                        if start_time <= note.start < end_time:\n",
        "                            new_note = pretty_midi.Note(\n",
        "                                velocity=note.velocity,\n",
        "                                pitch=note.pitch,\n",
        "                                start=note.start - start_time,\n",
        "                                end=min(note.end - start_time, target_duration)\n",
        "                            )\n",
        "                            new_instrument.notes.append(new_note)\n",
        "\n",
        "                    if new_instrument.notes:\n",
        "                        pm_segment.instruments.append(new_instrument)\n",
        "\n",
        "                if pm_segment.instruments:\n",
        "                    piano_roll = pm_segment.get_piano_roll(fs=fs)\n",
        "                    target_length = int(target_duration * fs)\n",
        "\n",
        "                    if piano_roll.shape[1] > target_length:\n",
        "                        piano_roll = piano_roll[:, :target_length]\n",
        "                    else:\n",
        "                        pad_width = target_length - piano_roll.shape[1]\n",
        "                        piano_roll = np.pad(piano_roll, ((0,0),(0,pad_width)), mode='constant')\n",
        "\n",
        "                    segments.append(piano_roll)\n",
        "\n",
        "            return segments\n",
        "\n",
        "        else:\n",
        "            # For normal length pieces, use the whole piece\n",
        "            piano_roll = pm.get_piano_roll(fs=fs)\n",
        "            target_length = int(target_duration * fs)\n",
        "\n",
        "            if piano_roll.shape[1] > target_length:\n",
        "                # Take from the middle rather than truncating end\n",
        "                start_idx = (piano_roll.shape[1] - target_length) // 2\n",
        "                piano_roll = piano_roll[:, start_idx:start_idx + target_length]\n",
        "            else:\n",
        "                pad_width = target_length - piano_roll.shape[1]\n",
        "                piano_roll = np.pad(piano_roll, ((0,0),(0,pad_width)), mode='constant')\n",
        "\n",
        "            return [piano_roll]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {midi_path}: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "230bdd12",
      "metadata": {
        "id": "230bdd12"
      },
      "outputs": [],
      "source": [
        "def normalize_piano_roll(piano_roll):\n",
        "    \"\"\"\n",
        "    Apply musical normalization to piano roll\n",
        "    \"\"\"\n",
        "    # 1. Velocity normalization (already 0-1 from pretty_midi)\n",
        "    normalized = piano_roll.copy()\n",
        "\n",
        "    # 2. Optional: Focus on active pitch range\n",
        "    active_pitches = np.any(normalized > 0, axis=1)\n",
        "    if np.any(active_pitches):\n",
        "        first_active = np.argmax(active_pitches)\n",
        "        last_active = len(active_pitches) - 1 - np.argmax(active_pitches[::-1])\n",
        "\n",
        "        # Ensure we keep a reasonable range (at least 60 semitones = 5 octaves)\n",
        "        min_range = 60\n",
        "        current_range = last_active - first_active + 1\n",
        "\n",
        "        if current_range < min_range:\n",
        "            expand = (min_range - current_range) // 2\n",
        "            first_active = max(0, first_active - expand)\n",
        "            last_active = min(127, last_active + expand)\n",
        "\n",
        "    return normalized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b1b1f0c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1b1f0c8",
        "outputId": "9b42d5aa-6afe-41b2-d758-6d4047f65bba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Improved data processing functions defined!\n",
            "Key improvements:\n",
            "• Intelligent segment extraction for long pieces\n",
            "• Musical boundary awareness\n",
            "• Better normalization\n",
            "• Feature extraction for analysis\n"
          ]
        }
      ],
      "source": [
        "def extract_musical_features(piano_roll):\n",
        "    \"\"\"\n",
        "    Extract features that capture musical style\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "\n",
        "    # Temporal features\n",
        "    note_density_timeline = np.sum(piano_roll > 0, axis=0)\n",
        "    features['avg_notes_per_time'] = np.mean(note_density_timeline)\n",
        "    features['note_density_variance'] = np.var(note_density_timeline)\n",
        "\n",
        "    # Pitch features\n",
        "    pitch_activity = np.sum(piano_roll > 0, axis=1)\n",
        "    active_pitches = pitch_activity > 0\n",
        "    if np.any(active_pitches):\n",
        "        features['pitch_range'] = np.sum(active_pitches)\n",
        "        features['lowest_pitch'] = np.argmax(active_pitches)\n",
        "        features['highest_pitch'] = 127 - np.argmax(active_pitches[::-1])\n",
        "    else:\n",
        "        features['pitch_range'] = 0\n",
        "        features['lowest_pitch'] = 60  # Middle C\n",
        "        features['highest_pitch'] = 60\n",
        "\n",
        "    # Rhythmic features\n",
        "    onset_pattern = np.diff(note_density_timeline > 0).astype(int)\n",
        "    features['onset_density'] = np.sum(onset_pattern == 1) / len(onset_pattern)\n",
        "\n",
        "    return features\n",
        "\n",
        "print(\"✅ Improved data processing functions defined!\")\n",
        "print(\"Key improvements:\")\n",
        "print(\"• Intelligent segment extraction for long pieces\")\n",
        "print(\"• Musical boundary awareness\")\n",
        "print(\"• Better normalization\")\n",
        "print(\"• Feature extraction for analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d0aefe6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0aefe6d",
        "outputId": "9ef3f32c-b7f5-460a-8a50-c4bc45a946a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Non-overlapping segmentation function defined!\n"
          ]
        }
      ],
      "source": [
        "def get_piano_roll_non_overlapping_segments(midi_path, fs=100, segment_duration=45.0):\n",
        "    \"\"\"\n",
        "    Extract non-overlapping segments to avoid data leakage with LSTM\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pm = pretty_midi.PrettyMIDI(midi_path)\n",
        "\n",
        "        # Get the actual duration of the piece\n",
        "        actual_duration = pm.get_end_time()\n",
        "\n",
        "        # If piece is very short, skip it\n",
        "        if actual_duration < 15.0:  # Less than 15 seconds\n",
        "            return None\n",
        "\n",
        "        segments = []\n",
        "        segment_size = segment_duration\n",
        "\n",
        "        # Calculate number of non-overlapping segments\n",
        "        num_segments = int(actual_duration // segment_size)\n",
        "\n",
        "        # If piece is shorter than one segment, use the whole piece (padded)\n",
        "        if num_segments == 0:\n",
        "            piano_roll = pm.get_piano_roll(fs=fs)\n",
        "            target_length = int(segment_duration * fs)\n",
        "\n",
        "            if piano_roll.shape[1] < target_length:\n",
        "                pad_width = target_length - piano_roll.shape[1]\n",
        "                piano_roll = np.pad(piano_roll, ((0,0),(0,pad_width)), mode='constant')\n",
        "            else:\n",
        "                piano_roll = piano_roll[:, :target_length]\n",
        "\n",
        "            segments.append(piano_roll)\n",
        "            return segments\n",
        "\n",
        "        # Extract non-overlapping segments\n",
        "        for i in range(num_segments):\n",
        "            start_time = i * segment_size\n",
        "            end_time = start_time + segment_size\n",
        "\n",
        "            # Create a copy and trim\n",
        "            pm_segment = pretty_midi.PrettyMIDI()\n",
        "            for instrument in pm.instruments:\n",
        "                new_instrument = pretty_midi.Instrument(\n",
        "                    program=instrument.program,\n",
        "                    is_drum=instrument.is_drum,\n",
        "                    name=instrument.name\n",
        "                )\n",
        "\n",
        "                for note in instrument.notes:\n",
        "                    if start_time <= note.start < end_time:\n",
        "                        new_note = pretty_midi.Note(\n",
        "                            velocity=note.velocity,\n",
        "                            pitch=note.pitch,\n",
        "                            start=note.start - start_time,\n",
        "                            end=min(note.end - start_time, segment_duration)\n",
        "                        )\n",
        "                        new_instrument.notes.append(new_note)\n",
        "\n",
        "                if new_instrument.notes:\n",
        "                    pm_segment.instruments.append(new_instrument)\n",
        "\n",
        "            if pm_segment.instruments:\n",
        "                piano_roll = pm_segment.get_piano_roll(fs=fs)\n",
        "                target_length = int(segment_duration * fs)\n",
        "\n",
        "                if piano_roll.shape[1] > target_length:\n",
        "                    piano_roll = piano_roll[:, :target_length]\n",
        "                else:\n",
        "                    pad_width = target_length - piano_roll.shape[1]\n",
        "                    piano_roll = np.pad(piano_roll, ((0,0),(0,pad_width)), mode='constant')\n",
        "\n",
        "                segments.append(piano_roll)\n",
        "\n",
        "        return segments if segments else None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {midi_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"✅ Non-overlapping segmentation function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "43715e17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43715e17",
        "outputId": "e347e176-4aae-4075-9db9-0ff41242e076"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting FULL non-overlapping segment data loading...\n",
            "🎵 LOADING FULL DATASET WITH NON-OVERLAPPING SEGMENTS...\n",
            "Benefits for LSTM training:\n",
            "• Segment duration: 45.0s (no overlap)\n",
            "• Using ALL available files (no limits)\n",
            "• No data leakage between train/test\n",
            "• Cleaner temporal boundaries\n",
            "• Will address class imbalance later during training\n",
            "\n",
            "--- Processing Bach ---\n",
            "  Found 131 MIDI files for Bach\n",
            "  Processed 50/131 files, created 394 segments...\n",
            "  Processed 100/131 files, created 715 segments...\n",
            "✅ Bach: 131/131 files → 977 segments\n",
            "  Final data shape: (977, 128, 4500)\n",
            "\n",
            "--- Processing Beethoven ---\n",
            "  Found 134 MIDI files for Beethoven\n",
            "Error processing data/kaggle/midiclassics/Beethoven/Anhang 14-3.mid: Could not decode key with 3 flats and mode 255\n",
            "  Processed 50/134 files, created 357 segments...\n",
            "  Processed 100/134 files, created 765 segments...\n",
            "✅ Beethoven: 133/134 files → 987 segments\n",
            "  Final data shape: (987, 128, 4500)\n",
            "\n",
            "--- Processing Chopin ---\n",
            "  Found 136 MIDI files for Chopin\n",
            "  Processed 50/136 files, created 218 segments...\n",
            "  Processed 100/136 files, created 449 segments...\n",
            "✅ Chopin: 136/136 files → 610 segments\n",
            "  Final data shape: (610, 128, 4500)\n",
            "\n",
            "--- Processing Mozart ---\n",
            "  Found 90 MIDI files for Mozart\n",
            "  Processed 50/90 files, created 271 segments...\n",
            "✅ Mozart: 90/90 files → 524 segments\n",
            "  Final data shape: (524, 128, 4500)\n",
            "\n",
            "🎯 FINAL FULL NON-OVERLAPPING DATASET:\n",
            "Total files processed: 490\n",
            "Total samples: 3098\n",
            "Data shape: (3098, 128, 4500)\n",
            "Label distribution: [977 987 610 524]\n",
            "  Bach: 977 samples (31.5%)\n",
            "  Beethoven: 987 samples (31.9%)\n",
            "  Chopin: 610 samples (19.7%)\n",
            "  Mozart: 524 samples (16.9%)\n"
          ]
        }
      ],
      "source": [
        "def load_dataset_non_overlapping_full(extract_path, target_composers, segment_duration=45.0):\n",
        "    \"\"\"\n",
        "    Load FULL dataset with non-overlapping segments - using ALL available files\n",
        "    \"\"\"\n",
        "    print(\"🎵 LOADING FULL DATASET WITH NON-OVERLAPPING SEGMENTS...\")\n",
        "    print(\"Benefits for LSTM training:\")\n",
        "    print(f\"• Segment duration: {segment_duration}s (no overlap)\")\n",
        "    print(\"• Using ALL available files (no limits)\")\n",
        "    print(\"• No data leakage between train/test\")\n",
        "    print(\"• Cleaner temporal boundaries\")\n",
        "    print(\"• Will address class imbalance later during training\")\n",
        "\n",
        "    composer_to_idx = {c: i for i, c in enumerate(target_composers)}\n",
        "    all_data = []\n",
        "    all_labels = []\n",
        "    all_features = []\n",
        "\n",
        "    total_files_processed = 0\n",
        "    total_segments_created = 0\n",
        "\n",
        "    for composer in target_composers:\n",
        "        print(f\"\\n--- Processing {composer} ---\")\n",
        "        composer_dir = os.path.join(extract_path, composer)\n",
        "\n",
        "        if not os.path.isdir(composer_dir):\n",
        "            print(f\"Directory not found: {composer_dir}\")\n",
        "            continue\n",
        "\n",
        "        composer_data = []\n",
        "        composer_labels = []\n",
        "        composer_features = []\n",
        "        files_processed = 0\n",
        "        segments_created = 0\n",
        "\n",
        "        # Get ALL MIDI files - no limit\n",
        "        midi_files = [f for f in os.listdir(composer_dir)\n",
        "                     if f.lower().endswith(('.mid', '.midi'))]\n",
        "\n",
        "        print(f\"  Found {len(midi_files)} MIDI files for {composer}\")\n",
        "\n",
        "        for file in midi_files:\n",
        "            midi_path = os.path.join(composer_dir, file)\n",
        "\n",
        "            try:\n",
        "                # Use non-overlapping segmentation\n",
        "                segments = get_piano_roll_non_overlapping_segments(\n",
        "                    midi_path,\n",
        "                    segment_duration=segment_duration\n",
        "                )\n",
        "\n",
        "                if segments is None:\n",
        "                    continue\n",
        "\n",
        "                for segment in segments:\n",
        "                    # Normalize the segment\n",
        "                    normalized_segment = normalize_piano_roll(segment)\n",
        "\n",
        "                    # Extract musical features\n",
        "                    features = extract_musical_features(normalized_segment)\n",
        "\n",
        "                    # Quality check: skip if too sparse\n",
        "                    note_density = features['avg_notes_per_time']\n",
        "                    if note_density < 0.1:  # Very sparse, likely poor quality\n",
        "                        continue\n",
        "\n",
        "                    composer_data.append(normalized_segment)\n",
        "                    composer_labels.append(composer_to_idx[composer])\n",
        "                    composer_features.append(features)\n",
        "                    segments_created += 1\n",
        "\n",
        "                files_processed += 1\n",
        "\n",
        "                # Progress update every 50 files for full dataset\n",
        "                if files_processed % 50 == 0:\n",
        "                    print(f\"  Processed {files_processed}/{len(midi_files)} files, created {segments_created} segments...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error processing {file}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"✅ {composer}: {files_processed}/{len(midi_files)} files → {segments_created} segments\")\n",
        "\n",
        "        if composer_data:\n",
        "            composer_data = np.array(composer_data)\n",
        "            composer_labels = np.array(composer_labels)\n",
        "\n",
        "            all_data.append(composer_data)\n",
        "            all_labels.append(composer_labels)\n",
        "            all_features.extend(composer_features)\n",
        "\n",
        "            print(f\"  Final data shape: {composer_data.shape}\")\n",
        "\n",
        "        total_files_processed += files_processed\n",
        "        total_segments_created += segments_created\n",
        "\n",
        "    # Combine all data\n",
        "    if all_data:\n",
        "        data = np.concatenate(all_data, axis=0)\n",
        "        labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "        print(f\"\\n🎯 FINAL FULL NON-OVERLAPPING DATASET:\")\n",
        "        print(f\"Total files processed: {total_files_processed}\")\n",
        "        print(f\"Total samples: {len(data)}\")\n",
        "        print(f\"Data shape: {data.shape}\")\n",
        "        print(f\"Label distribution: {np.bincount(labels)}\")\n",
        "\n",
        "        # Show class distribution percentages\n",
        "        for i, composer in enumerate(target_composers):\n",
        "            count = np.sum(labels == i)\n",
        "            percentage = (count / len(labels)) * 100\n",
        "            print(f\"  {composer}: {count} samples ({percentage:.1f}%)\")\n",
        "\n",
        "        return data, labels, all_features\n",
        "    else:\n",
        "        print(\"❌ No data loaded!\")\n",
        "        return None, None, None\n",
        "\n",
        "# Load the FULL dataset with non-overlapping segments\n",
        "print(\"🚀 Starting FULL non-overlapping segment data loading...\")\n",
        "full_data, full_labels, full_features = load_dataset_non_overlapping_full(\n",
        "    extract_path,\n",
        "    TARGET_COMPOSERS,\n",
        "    segment_duration=45.0  # 45-second segments, no overlap, ALL files\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ttJ-s25RI_aV",
      "metadata": {
        "id": "ttJ-s25RI_aV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
