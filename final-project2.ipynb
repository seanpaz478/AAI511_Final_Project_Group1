{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "caJC4Lf_md2R",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caJC4Lf_md2R",
        "outputId": "ca8cc17c-6ebb-4d10-934d-8e0796ae5b98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m178.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (2.0.2)\n",
            "Collecting mido>=1.1.16 (from pretty_midi)\n",
            "  Downloading mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.7.14)\n",
            "Downloading mido-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pretty_midi\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592286 sha256=6b7d68d4fa0805eb47fc8952dc81a689bab8826193208904e3ba2c3a5a908348\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/95/ac/15ceaeb2823b04d8e638fd1495357adb8d26c00ccac9d7782e\n",
            "Successfully built pretty_midi\n",
            "Installing collected packages: mido, pretty_midi\n",
            "Successfully installed mido-1.3.3 pretty_midi-0.2.10\n"
          ]
        }
      ],
      "source": [
        "%pip install pretty_midi kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "998891eb",
      "metadata": {
        "id": "998891eb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import zipfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import numpy as np\n",
        "import pretty_midi\n",
        "import torch\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6406d2f8",
      "metadata": {
        "id": "6406d2f8"
      },
      "outputs": [],
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c06d7fe",
      "metadata": {
        "id": "2c06d7fe"
      },
      "outputs": [],
      "source": [
        "if not hasattr(np, 'int'):\n",
        "    np.int = int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd63e1d1",
      "metadata": {
        "id": "dd63e1d1"
      },
      "outputs": [],
      "source": [
        "# Define your model class (must match the architecture used in final-project1)\n",
        "class CNN_LSTM_Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=4, lstm_hidden=256):\n",
        "        super(CNN_LSTM_Classifier, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.2),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.3),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.4),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "        self.feature_size = 128 * 16\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.feature_size,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=0.3,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=lstm_hidden * 2,\n",
        "            num_heads=8,\n",
        "            dropout=0.3,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(lstm_hidden * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = x.contiguous().view(batch_size, x.size(1), -1)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
        "        pooled = torch.mean(attn_out, dim=1)\n",
        "        output = self.classifier(pooled)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89e8b887",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89e8b887",
        "outputId": "4f57181d-aee5-414e-d78f-387317635aa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running in Google Colab.\n",
            "✅ Loaded: /content/saved_models/original_cnn_lstm.pth\n",
            "✅ Loaded: /content/saved_models/rhythm_augmented_cnn_lstm.pth\n"
          ]
        }
      ],
      "source": [
        "# # Load the models\n",
        "# from IPython import get_ipython\n",
        "\n",
        "# if 'google.colab' in str(get_ipython()):\n",
        "#     print(\"Running in Google Colab.\")\n",
        "#     original_path = os.path.join('/content/saved_models', 'original_cnn_lstm.pth')\n",
        "#     rhythm_path = os.path.join('/content/saved_models', 'rhythm_augmented_cnn_lstm.pth')\n",
        "# else:\n",
        "#     print(\"Not running in Google Colab.\")\n",
        "#     original_path = os.path.join('saved_models', 'original_cnn_lstm.pth')\n",
        "#     rhythm_path = os.path.join('saved_models', 'rhythm_augmented_cnn_lstm.pth')\n",
        "\n",
        "# model = CNN_LSTM_Classifier(num_classes=4, lstm_hidden=256).to(device)\n",
        "# model.load_state_dict(torch.load(original_path, map_location=device))\n",
        "# print(f\"✅ Loaded: {original_path}\")\n",
        "\n",
        "# rhythm_model = CNN_LSTM_Classifier(num_classes=4, lstm_hidden=256).to(device)\n",
        "# rhythm_model.load_state_dict(torch.load(rhythm_path, map_location=device))\n",
        "# print(f\"✅ Loaded: {rhythm_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "afa70024",
      "metadata": {
        "id": "afa70024"
      },
      "outputs": [],
      "source": [
        "TARGET_COMPOSERS = [\n",
        "    'Bach',\n",
        "    'Beethoven',\n",
        "    'Chopin',\n",
        "    'Mozart',\n",
        "]\n",
        "\n",
        "path = kagglehub.dataset_download(\"blanderbuss/midi-classic-music\")\n",
        "\n",
        "zip_path = os.path.join(path, 'midiclassics.zip')\n",
        "extract_path = os.path.join('data', 'kaggle', 'midiclassics')\n",
        "\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# list files in extract_path that contain the target composers in name\n",
        "for composer in TARGET_COMPOSERS:\n",
        "    composer_files = [f for f in os.listdir(extract_path) if composer.lower() in f.lower()]\n",
        "\n",
        "# Only keep directories that contain a target composer's name\n",
        "for item in os.listdir(extract_path):\n",
        "    item_path = os.path.join(extract_path, item)\n",
        "    if not any(composer.lower() in item.lower() for composer in TARGET_COMPOSERS):\n",
        "        if os.path.isfile(item_path):\n",
        "            os.remove(item_path)\n",
        "        elif os.path.isdir(item_path):\n",
        "            shutil.rmtree(item_path)\n",
        "\n",
        "# also delete \"C.P.E.Bach\" files. This was the son of J.S. Bach, and we want to keep only the main composers\n",
        "for item in os.listdir(extract_path):\n",
        "    if 'C.P.E.Bach' in item:\n",
        "        item_path = os.path.join(extract_path, item)\n",
        "        if os.path.isfile(item_path):\n",
        "            os.remove(item_path)\n",
        "        elif os.path.isdir(item_path):\n",
        "            shutil.rmtree(item_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c7eb056a",
      "metadata": {
        "id": "c7eb056a"
      },
      "outputs": [],
      "source": [
        "class PianoRollDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        # Add channel dimension for CNN: (1, 128, T)\n",
        "        return self.data[idx].unsqueeze(0), self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ad781104",
      "metadata": {
        "id": "ad781104"
      },
      "outputs": [],
      "source": [
        "def get_piano_roll_improved(midi_path, fs=100, target_duration=45.0):\n",
        "    \"\"\"\n",
        "    Improved MIDI to piano roll conversion with musical awareness\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pm = pretty_midi.PrettyMIDI(midi_path)\n",
        "\n",
        "        # Get the actual duration of the piece\n",
        "        actual_duration = pm.get_end_time()\n",
        "\n",
        "        # If piece is very short, skip it\n",
        "        if actual_duration < 10.0:  # Less than 10 seconds\n",
        "            return None\n",
        "\n",
        "        # For long pieces, extract multiple segments\n",
        "        if actual_duration > target_duration * 1.5:\n",
        "            # Extract from different parts of the piece\n",
        "            segments = []\n",
        "            num_segments = min(3, int(actual_duration // target_duration))\n",
        "\n",
        "            for i in range(num_segments):\n",
        "                start_time = i * (actual_duration / num_segments)\n",
        "                end_time = start_time + target_duration\n",
        "\n",
        "                # Create a copy and trim\n",
        "                pm_segment = pretty_midi.PrettyMIDI()\n",
        "                for instrument in pm.instruments:\n",
        "                    new_instrument = pretty_midi.Instrument(\n",
        "                        program=instrument.program,\n",
        "                        is_drum=instrument.is_drum,\n",
        "                        name=instrument.name\n",
        "                    )\n",
        "\n",
        "                    for note in instrument.notes:\n",
        "                        if start_time <= note.start < end_time:\n",
        "                            new_note = pretty_midi.Note(\n",
        "                                velocity=note.velocity,\n",
        "                                pitch=note.pitch,\n",
        "                                start=note.start - start_time,\n",
        "                                end=min(note.end - start_time, target_duration)\n",
        "                            )\n",
        "                            new_instrument.notes.append(new_note)\n",
        "\n",
        "                    if new_instrument.notes:\n",
        "                        pm_segment.instruments.append(new_instrument)\n",
        "\n",
        "                if pm_segment.instruments:\n",
        "                    piano_roll = pm_segment.get_piano_roll(fs=fs)\n",
        "                    target_length = int(target_duration * fs)\n",
        "\n",
        "                    if piano_roll.shape[1] > target_length:\n",
        "                        piano_roll = piano_roll[:, :target_length]\n",
        "                    else:\n",
        "                        pad_width = target_length - piano_roll.shape[1]\n",
        "                        piano_roll = np.pad(piano_roll, ((0,0),(0,pad_width)), mode='constant')\n",
        "\n",
        "                    segments.append(piano_roll)\n",
        "\n",
        "            return segments\n",
        "\n",
        "        else:\n",
        "            # For normal length pieces, use the whole piece\n",
        "            piano_roll = pm.get_piano_roll(fs=fs)\n",
        "            target_length = int(target_duration * fs)\n",
        "\n",
        "            if piano_roll.shape[1] > target_length:\n",
        "                # Take from the middle rather than truncating end\n",
        "                start_idx = (piano_roll.shape[1] - target_length) // 2\n",
        "                piano_roll = piano_roll[:, start_idx:start_idx + target_length]\n",
        "            else:\n",
        "                pad_width = target_length - piano_roll.shape[1]\n",
        "                piano_roll = np.pad(piano_roll, ((0,0),(0,pad_width)), mode='constant')\n",
        "\n",
        "            return [piano_roll]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {midi_path}: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "230bdd12",
      "metadata": {
        "id": "230bdd12"
      },
      "outputs": [],
      "source": [
        "def normalize_piano_roll(piano_roll):\n",
        "    \"\"\"\n",
        "    Apply musical normalization to piano roll\n",
        "    \"\"\"\n",
        "    # 1. Velocity normalization (already 0-1 from pretty_midi)\n",
        "    normalized = piano_roll.copy()\n",
        "\n",
        "    # 2. Optional: Focus on active pitch range\n",
        "    active_pitches = np.any(normalized > 0, axis=1)\n",
        "    if np.any(active_pitches):\n",
        "        first_active = np.argmax(active_pitches)\n",
        "        last_active = len(active_pitches) - 1 - np.argmax(active_pitches[::-1])\n",
        "\n",
        "        # Ensure we keep a reasonable range (at least 60 semitones = 5 octaves)\n",
        "        min_range = 60\n",
        "        current_range = last_active - first_active + 1\n",
        "\n",
        "        if current_range < min_range:\n",
        "            expand = (min_range - current_range) // 2\n",
        "            first_active = max(0, first_active - expand)\n",
        "            last_active = min(127, last_active + expand)\n",
        "\n",
        "    return normalized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b1b1f0c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1b1f0c8",
        "outputId": "9b42d5aa-6afe-41b2-d758-6d4047f65bba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Improved data processing functions defined!\n",
            "Key improvements:\n",
            "• Intelligent segment extraction for long pieces\n",
            "• Musical boundary awareness\n",
            "• Better normalization\n",
            "• Feature extraction for analysis\n"
          ]
        }
      ],
      "source": [
        "def extract_musical_features(piano_roll):\n",
        "    \"\"\"\n",
        "    Extract features that capture musical style\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "\n",
        "    # Temporal features\n",
        "    note_density_timeline = np.sum(piano_roll > 0, axis=0)\n",
        "    features['avg_notes_per_time'] = np.mean(note_density_timeline)\n",
        "    features['note_density_variance'] = np.var(note_density_timeline)\n",
        "\n",
        "    # Pitch features\n",
        "    pitch_activity = np.sum(piano_roll > 0, axis=1)\n",
        "    active_pitches = pitch_activity > 0\n",
        "    if np.any(active_pitches):\n",
        "        features['pitch_range'] = np.sum(active_pitches)\n",
        "        features['lowest_pitch'] = np.argmax(active_pitches)\n",
        "        features['highest_pitch'] = 127 - np.argmax(active_pitches[::-1])\n",
        "    else:\n",
        "        features['pitch_range'] = 0\n",
        "        features['lowest_pitch'] = 60  # Middle C\n",
        "        features['highest_pitch'] = 60\n",
        "\n",
        "    # Rhythmic features\n",
        "    onset_pattern = np.diff(note_density_timeline > 0).astype(int)\n",
        "    features['onset_density'] = np.sum(onset_pattern == 1) / len(onset_pattern)\n",
        "\n",
        "    return features\n",
        "\n",
        "print(\"✅ Improved data processing functions defined!\")\n",
        "print(\"Key improvements:\")\n",
        "print(\"• Intelligent segment extraction for long pieces\")\n",
        "print(\"• Musical boundary awareness\")\n",
        "print(\"• Better normalization\")\n",
        "print(\"• Feature extraction for analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ttJ-s25RI_aV",
      "metadata": {
        "id": "ttJ-s25RI_aV"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# IMPROVED DATA LOADING WITH BETTER PROCESSING\n",
        "# =====================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def load_improved_dataset(extract_path, target_composers, target_duration=45.0, max_files_per_composer=None):\n",
        "    \"\"\"\n",
        "    Load dataset with improved processing that addresses previous shortcomings\n",
        "    \"\"\"\n",
        "    print(\"🎵 LOADING DATASET WITH IMPROVED PROCESSING...\")\n",
        "    print(\"Improvements over original:\")\n",
        "    print(\"• Intelligent segment extraction for long pieces\")\n",
        "    print(\"• Better handling of piece lengths\")\n",
        "    print(\"• Musical feature extraction\")\n",
        "    print(\"• Quality filtering\")\n",
        "\n",
        "    composer_to_idx = {c: i for i, c in enumerate(target_composers)}\n",
        "    all_data = []\n",
        "    all_labels = []\n",
        "    all_features = []\n",
        "\n",
        "    for composer in target_composers:\n",
        "        print(f\"\\n--- Processing {composer} ---\")\n",
        "        composer_dir = os.path.join(extract_path, composer)\n",
        "\n",
        "        if not os.path.isdir(composer_dir):\n",
        "            print(f\"Directory not found: {composer_dir}\")\n",
        "            continue\n",
        "\n",
        "        composer_data = []\n",
        "        composer_labels = []\n",
        "        composer_features = []\n",
        "        files_processed = 0\n",
        "        segments_created = 0\n",
        "\n",
        "        midi_files = [f for f in os.listdir(composer_dir)\n",
        "                     if f.lower().endswith(('.mid', '.midi'))]\n",
        "\n",
        "        if max_files_per_composer:\n",
        "            midi_files = midi_files[:max_files_per_composer]\n",
        "\n",
        "        for file in midi_files:\n",
        "            midi_path = os.path.join(composer_dir, file)\n",
        "\n",
        "            try:\n",
        "                # Use improved processing\n",
        "                segments = get_piano_roll_improved(midi_path, target_duration=target_duration)\n",
        "\n",
        "                if segments is None:\n",
        "                    continue\n",
        "\n",
        "                for segment in segments:\n",
        "                    # Normalize the segment\n",
        "                    normalized_segment = normalize_piano_roll(segment)\n",
        "\n",
        "                    # Extract musical features\n",
        "                    features = extract_musical_features(normalized_segment)\n",
        "\n",
        "                    # Quality check: skip if too sparse\n",
        "                    note_density = features['avg_notes_per_time']\n",
        "                    if note_density < 0.1:  # Very sparse, likely poor quality\n",
        "                        continue\n",
        "\n",
        "                    composer_data.append(normalized_segment)\n",
        "                    composer_labels.append(composer_to_idx[composer])\n",
        "                    composer_features.append(features)\n",
        "                    segments_created += 1\n",
        "\n",
        "                files_processed += 1\n",
        "\n",
        "                if files_processed % 10 == 0:\n",
        "                    print(f\"  Processed {files_processed} files, created {segments_created} segments...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error processing {file}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"✅ {composer}: {files_processed} files → {segments_created} segments\")\n",
        "\n",
        "        if composer_data:\n",
        "            composer_data = np.array(composer_data)\n",
        "            composer_labels = np.array(composer_labels)\n",
        "\n",
        "            all_data.append(composer_data)\n",
        "            all_labels.append(composer_labels)\n",
        "            all_features.extend(composer_features)\n",
        "\n",
        "            print(f\"  Final data shape: {composer_data.shape}\")\n",
        "\n",
        "    # Combine all data\n",
        "    if all_data:\n",
        "        data = np.concatenate(all_data, axis=0)\n",
        "        labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "        print(f\"\\n🎯 FINAL IMPROVED DATASET:\")\n",
        "        print(f\"Total samples: {len(data)}\")\n",
        "        print(f\"Data shape: {data.shape}\")\n",
        "        print(f\"Label distribution: {np.bincount(labels)}\")\n",
        "\n",
        "        return data, labels, all_features\n",
        "    else:\n",
        "        print(\"❌ No data loaded!\")\n",
        "        return None, None, None\n",
        "\n",
        "# Load the improved dataset\n",
        "print(\"🚀 Starting improved data loading...\")\n",
        "improved_data, improved_labels, features = load_improved_dataset(\n",
        "    extract_path,\n",
        "    TARGET_COMPOSERS,\n",
        "    target_duration=45.0,  # 45 seconds per segment\n",
        "    max_files_per_composer=120  # Limit for testing - remove for full dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1fdc1ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# IMPROVED NON-OVERLAPPING SEGMENTATION\n",
        "# =====================================================\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def balance_classes_song_aware(segments_by_composer, target_samples_per_class=None):\n",
        "    \"\"\"\n",
        "    Balance classes by selecting complete songs (with all their segments)\n",
        "    This preserves temporal relationships within songs\n",
        "    \"\"\"\n",
        "    # Calculate current distribution\n",
        "    composer_counts = {composer: len(segments) for composer, segments in segments_by_composer.items()}\n",
        "    max_count = max(composer_counts.values()) if target_samples_per_class is None else target_samples_per_class\n",
        "    \n",
        "    print(f\"📊 Original segment distribution: {composer_counts}\")\n",
        "    print(f\"🎯 Target segments per class: {max_count}\")\n",
        "    \n",
        "    balanced_segments = {}\n",
        "    \n",
        "    for composer, segments in segments_by_composer.items():\n",
        "        current_count = len(segments)\n",
        "        \n",
        "        if current_count >= max_count:\n",
        "            # If we have enough, randomly select songs to reach target\n",
        "            songs_dict = defaultdict(list)\n",
        "            for segment in segments:\n",
        "                songs_dict[segment['song_id']].append(segment)\n",
        "            \n",
        "            # Randomly select songs until we reach target count\n",
        "            selected_segments = []\n",
        "            song_ids = list(songs_dict.keys())\n",
        "            np.random.shuffle(song_ids)\n",
        "            \n",
        "            for song_id in song_ids:\n",
        "                song_segments = songs_dict[song_id]\n",
        "                if len(selected_segments) + len(song_segments) <= max_count:\n",
        "                    selected_segments.extend(song_segments)\n",
        "                elif len(selected_segments) < max_count:\n",
        "                    # Partial song - take contiguous segments from the beginning\n",
        "                    needed = max_count - len(selected_segments)\n",
        "                    selected_segments.extend(song_segments[:needed])\n",
        "                    break\n",
        "            \n",
        "            balanced_segments[composer] = selected_segments\n",
        "            print(f\"  {composer}: {current_count} → {len(selected_segments)} (downsampled)\")\n",
        "            \n",
        "        else:\n",
        "            # Need to oversample - duplicate entire songs\n",
        "            needed_samples = max_count - current_count\n",
        "            \n",
        "            # Group segments by song\n",
        "            songs_dict = defaultdict(list)\n",
        "            for segment in segments:\n",
        "                songs_dict[segment['song_id']].append(segment)\n",
        "            \n",
        "            song_ids = list(songs_dict.keys())\n",
        "            selected_segments = segments.copy()  # Start with all original segments\n",
        "            \n",
        "            # Add complete songs until we reach target\n",
        "            while len(selected_segments) < max_count:\n",
        "                # Randomly select a song to duplicate\n",
        "                song_id = np.random.choice(song_ids)\n",
        "                song_segments = songs_dict[song_id]\n",
        "                \n",
        "                if len(selected_segments) + len(song_segments) <= max_count:\n",
        "                    # Add entire song\n",
        "                    for segment in song_segments:\n",
        "                        # Create a copy with new metadata to avoid conflicts\n",
        "                        new_segment = segment.copy()\n",
        "                        new_segment['song_id'] = f\"{segment['song_id']}_dup_{len(selected_segments)}\"\n",
        "                        selected_segments.append(new_segment)\n",
        "                else:\n",
        "                    # Add partial song if needed\n",
        "                    remaining = max_count - len(selected_segments)\n",
        "                    for i, segment in enumerate(song_segments[:remaining]):\n",
        "                        new_segment = segment.copy()\n",
        "                        new_segment['song_id'] = f\"{segment['song_id']}_dup_{len(selected_segments)}\"\n",
        "                        selected_segments.append(new_segment)\n",
        "                    break\n",
        "            \n",
        "            balanced_segments[composer] = selected_segments\n",
        "            print(f\"  {composer}: {current_count} → {len(selected_segments)} (+{len(selected_segments) - current_count} from song duplication)\")\n",
        "    \n",
        "    return balanced_segments\n",
        "\n",
        "def get_piano_roll_segments_no_overlap(midi_path, fs=100, segment_duration=20.0):\n",
        "    \"\"\"\n",
        "    Extract NON-OVERLAPPING segments from a single MIDI file\n",
        "    This preserves temporal relationships without data leakage\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pm = pretty_midi.PrettyMIDI(midi_path)\n",
        "        actual_duration = pm.get_end_time()\n",
        "        \n",
        "        # Skip very short pieces\n",
        "        if actual_duration < segment_duration:\n",
        "            return None\n",
        "            \n",
        "        segments = []\n",
        "        segment_length = int(segment_duration * fs)\n",
        "        \n",
        "        # Extract non-overlapping segments\n",
        "        current_time = 0.0\n",
        "        segment_idx = 0\n",
        "        \n",
        "        while current_time + segment_duration <= actual_duration:\n",
        "            end_time = current_time + segment_duration\n",
        "            \n",
        "            # Create segment\n",
        "            pm_segment = pretty_midi.PrettyMIDI()\n",
        "            for instrument in pm.instruments:\n",
        "                new_instrument = pretty_midi.Instrument(\n",
        "                    program=instrument.program,\n",
        "                    is_drum=instrument.is_drum,\n",
        "                    name=instrument.name\n",
        "                )\n",
        "                \n",
        "                for note in instrument.notes:\n",
        "                    if current_time <= note.start < end_time:\n",
        "                        new_note = pretty_midi.Note(\n",
        "                            velocity=note.velocity,\n",
        "                            pitch=note.pitch,\n",
        "                            start=note.start - current_time,\n",
        "                            end=min(note.end - current_time, segment_duration)\n",
        "                        )\n",
        "                        new_instrument.notes.append(new_note)\n",
        "                \n",
        "                if new_instrument.notes:\n",
        "                    pm_segment.instruments.append(new_instrument)\n",
        "            \n",
        "            # Convert to piano roll\n",
        "            if pm_segment.instruments:\n",
        "                piano_roll = pm_segment.get_piano_roll(fs=fs)\n",
        "                \n",
        "                # Ensure exact length\n",
        "                if piano_roll.shape[1] > segment_length:\n",
        "                    piano_roll = piano_roll[:, :segment_length]\n",
        "                elif piano_roll.shape[1] < segment_length:\n",
        "                    pad_width = segment_length - piano_roll.shape[1]\n",
        "                    piano_roll = np.pad(piano_roll, ((0,0),(0,pad_width)), mode='constant')\n",
        "                \n",
        "                # Store segment with metadata\n",
        "                segment_info = {\n",
        "                    'piano_roll': piano_roll,\n",
        "                    'song_id': midi_path,\n",
        "                    'segment_idx': segment_idx,\n",
        "                    'start_time': current_time\n",
        "                }\n",
        "                segments.append(segment_info)\n",
        "                segment_idx += 1\n",
        "            \n",
        "            # Move to next segment (NO OVERLAP)\n",
        "            current_time += segment_duration  # Changed from (segment_duration - overlap)\n",
        "        \n",
        "        return segments if segments else None\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {midi_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_segmented_dataset_no_overlap(extract_path, target_composers, segment_duration=20.0, \n",
        "                                     max_files_per_composer=None, balance_classes=True):\n",
        "    \"\"\"\n",
        "    Load dataset with NON-OVERLAPPING segmentation and balancing\n",
        "    \"\"\"\n",
        "    print(\"🎵 LOADING DATASET WITH NON-OVERLAPPING SEGMENTATION...\")\n",
        "    print(f\"Segment duration: {segment_duration}s with NO OVERLAP\")\n",
        "    print(f\"Balance classes: {balance_classes}\")\n",
        "    \n",
        "    composer_to_idx = {c: i for i, c in enumerate(target_composers)}\n",
        "    segments_by_composer = {composer: [] for composer in target_composers}\n",
        "    \n",
        "    for composer in target_composers:\n",
        "        print(f\"\\n--- Processing {composer} ---\")\n",
        "        composer_dir = os.path.join(extract_path, composer)\n",
        "        \n",
        "        if not os.path.isdir(composer_dir):\n",
        "            print(f\"Directory not found: {composer_dir}\")\n",
        "            continue\n",
        "        \n",
        "        files_processed = 0\n",
        "        total_segments = 0\n",
        "        \n",
        "        midi_files = [f for f in os.listdir(composer_dir)\n",
        "                     if f.lower().endswith(('.mid', '.midi'))]\n",
        "        \n",
        "        if max_files_per_composer:\n",
        "            midi_files = midi_files[:max_files_per_composer]\n",
        "        \n",
        "        for file in midi_files:\n",
        "            midi_path = os.path.join(composer_dir, file)\n",
        "            \n",
        "            try:\n",
        "                # Extract NON-OVERLAPPING segments from this file\n",
        "                segments = get_piano_roll_segments_no_overlap(\n",
        "                    midi_path, \n",
        "                    segment_duration=segment_duration\n",
        "                )\n",
        "                \n",
        "                if segments is None:\n",
        "                    continue\n",
        "                \n",
        "                # Process each segment\n",
        "                valid_segments = []\n",
        "                for segment_info in segments:\n",
        "                    piano_roll = segment_info['piano_roll']\n",
        "                    \n",
        "                    # Normalize the segment\n",
        "                    normalized_segment = normalize_piano_roll(piano_roll)\n",
        "                    \n",
        "                    # Extract musical features\n",
        "                    features = extract_musical_features(normalized_segment)\n",
        "                    \n",
        "                    # Quality check: skip if too sparse\n",
        "                    note_density = features['avg_notes_per_time']\n",
        "                    if note_density < 0.05:  # Very sparse, likely poor quality\n",
        "                        continue\n",
        "                    \n",
        "                    # Update segment info\n",
        "                    segment_info['piano_roll'] = normalized_segment\n",
        "                    segment_info['features'] = features\n",
        "                    segment_info['label'] = composer_to_idx[composer]\n",
        "                    \n",
        "                    valid_segments.append(segment_info)\n",
        "                \n",
        "                # Add valid segments to composer collection\n",
        "                segments_by_composer[composer].extend(valid_segments)\n",
        "                total_segments += len(valid_segments)\n",
        "                files_processed += 1\n",
        "                \n",
        "                if files_processed % 10 == 0:\n",
        "                    print(f\"  Processed {files_processed} files, created {total_segments} segments...\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"  Error processing {file}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        print(f\"✅ {composer}: {files_processed} files → {total_segments} segments\")\n",
        "    \n",
        "    # Balance classes if requested\n",
        "    if balance_classes:\n",
        "        print(f\"\\n⚖️ BALANCING CLASSES (SONG-AWARE, NO OVERLAP)...\")\n",
        "        segments_by_composer = balance_classes_song_aware(segments_by_composer)\n",
        "    \n",
        "    # Convert to arrays\n",
        "    all_data = []\n",
        "    all_labels = []\n",
        "    all_features = []\n",
        "    \n",
        "    for composer, segments in segments_by_composer.items():\n",
        "        for segment_info in segments:\n",
        "            all_data.append(segment_info['piano_roll'])\n",
        "            all_labels.append(segment_info['label'])\n",
        "            all_features.append(segment_info['features'])\n",
        "    \n",
        "    data = np.array(all_data)\n",
        "    labels = np.array(all_labels)\n",
        "    \n",
        "    print(f\"\\n📊 FINAL BALANCED DATASET (NO OVERLAP):\")\n",
        "    print(f\"Total samples: {len(data)}\")\n",
        "    print(f\"Data shape: {data.shape}\")\n",
        "    final_counts = np.bincount(labels)\n",
        "    for i, composer in enumerate(target_composers):\n",
        "        if i < len(final_counts):\n",
        "            print(f\"  {composer}: {final_counts[i]} samples\")\n",
        "    \n",
        "    return data, labels, all_features\n",
        "\n",
        "# Load the NON-OVERLAPPING segmented and balanced dataset\n",
        "print(\"🚀 Starting NON-OVERLAPPING segmented data loading...\")\n",
        "segmented_data, segmented_labels, segmented_features = load_segmented_dataset_no_overlap(\n",
        "    extract_path,\n",
        "    TARGET_COMPOSERS,\n",
        "    segment_duration=20.0,      # 20-second segments\n",
        "    max_files_per_composer=150, # More files since we get fewer segments per song\n",
        "    balance_classes=True        # Enable song-aware class balancing\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a21b8b00",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# ENHANCED MUSICAL FEATURE EXTRACTION FOR MULTIMODAL\n",
        "# =====================================================\n",
        "\n",
        "def extract_comprehensive_musical_features(piano_roll):\n",
        "    \"\"\"\n",
        "    Extract comprehensive musical features for the MLP stream\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "    \n",
        "    # Get basic timeline and pitch data\n",
        "    note_density_timeline = np.sum(piano_roll > 0, axis=0)\n",
        "    pitch_activity = np.sum(piano_roll > 0, axis=1)\n",
        "    active_pitches = pitch_activity > 0\n",
        "    \n",
        "    # ==========================================\n",
        "    # TEMPORAL/RHYTHMIC FEATURES\n",
        "    # ==========================================\n",
        "    \n",
        "    # Basic temporal statistics\n",
        "    features['avg_notes_per_time'] = np.mean(note_density_timeline)\n",
        "    features['note_density_variance'] = np.var(note_density_timeline)\n",
        "    features['note_density_std'] = np.std(note_density_timeline)\n",
        "    features['max_simultaneous_notes'] = np.max(note_density_timeline)\n",
        "    \n",
        "    # Rhythmic complexity\n",
        "    onset_pattern = np.diff(note_density_timeline > 0).astype(int)\n",
        "    features['onset_density'] = np.sum(onset_pattern == 1) / len(onset_pattern) if len(onset_pattern) > 0 else 0\n",
        "    features['silence_ratio'] = np.sum(note_density_timeline == 0) / len(note_density_timeline)\n",
        "    \n",
        "    # Temporal distribution analysis\n",
        "    active_frames = note_density_timeline > 0\n",
        "    if np.any(active_frames):\n",
        "        features['temporal_sparsity'] = 1 - (np.sum(active_frames) / len(active_frames))\n",
        "        \n",
        "        # Find note clusters (bursts of activity)\n",
        "        cluster_changes = np.diff(active_frames.astype(int))\n",
        "        features['activity_bursts'] = np.sum(cluster_changes == 1) / len(cluster_changes) if len(cluster_changes) > 0 else 0\n",
        "    else:\n",
        "        features['temporal_sparsity'] = 1.0\n",
        "        features['activity_bursts'] = 0.0\n",
        "    \n",
        "    # ==========================================\n",
        "    # PITCH/HARMONIC FEATURES\n",
        "    # ==========================================\n",
        "    \n",
        "    if np.any(active_pitches):\n",
        "        # Basic pitch statistics\n",
        "        features['pitch_range'] = np.sum(active_pitches)\n",
        "        features['lowest_pitch'] = np.argmax(active_pitches)\n",
        "        features['highest_pitch'] = 127 - np.argmax(active_pitches[::-1])\n",
        "        features['pitch_span'] = features['highest_pitch'] - features['lowest_pitch']\n",
        "        \n",
        "        # Pitch distribution\n",
        "        weighted_pitches = np.arange(128) * pitch_activity\n",
        "        total_weight = np.sum(pitch_activity)\n",
        "        if total_weight > 0:\n",
        "            features['pitch_centroid'] = np.sum(weighted_pitches) / total_weight\n",
        "            features['pitch_variance'] = np.var(pitch_activity[active_pitches])\n",
        "        else:\n",
        "            features['pitch_centroid'] = 60  # Middle C\n",
        "            features['pitch_variance'] = 0\n",
        "            \n",
        "        # Register analysis (musical ranges)\n",
        "        bass_range = pitch_activity[21:48]  # A0 to B2\n",
        "        mid_range = pitch_activity[48:72]   # C3 to B4  \n",
        "        treble_range = pitch_activity[72:108] # C5 to B7\n",
        "        \n",
        "        total_activity = np.sum(pitch_activity)\n",
        "        features['bass_activity'] = np.sum(bass_range) / total_activity if total_activity > 0 else 0\n",
        "        features['mid_activity'] = np.sum(mid_range) / total_activity if total_activity > 0 else 0\n",
        "        features['treble_activity'] = np.sum(treble_range) / total_activity if total_activity > 0 else 0\n",
        "        \n",
        "    else:\n",
        "        # Default values for empty piano rolls\n",
        "        features.update({\n",
        "            'pitch_range': 0, 'lowest_pitch': 60, 'highest_pitch': 60,\n",
        "            'pitch_span': 0, 'pitch_centroid': 60, 'pitch_variance': 0,\n",
        "            'bass_activity': 0, 'mid_activity': 0, 'treble_activity': 0\n",
        "        })\n",
        "    \n",
        "    # ==========================================\n",
        "    # HARMONIC COMPLEXITY FEATURES\n",
        "    # ==========================================\n",
        "    \n",
        "    # Analyze simultaneous note patterns (chords vs single notes)\n",
        "    chord_frames = note_density_timeline >= 3  # 3+ simultaneous notes = chord\n",
        "    single_note_frames = note_density_timeline == 1\n",
        "    \n",
        "    features['chord_ratio'] = np.sum(chord_frames) / len(note_density_timeline)\n",
        "    features['single_note_ratio'] = np.sum(single_note_frames) / len(note_density_timeline)\n",
        "    features['polyphony_complexity'] = np.mean(note_density_timeline[note_density_timeline > 0]) if np.any(note_density_timeline > 0) else 0\n",
        "    \n",
        "    # Chord complexity analysis\n",
        "    if np.sum(chord_frames) > 0:\n",
        "        chord_complexities = note_density_timeline[chord_frames]\n",
        "        features['avg_chord_size'] = np.mean(chord_complexities)\n",
        "        features['chord_variance'] = np.var(chord_complexities)\n",
        "    else:\n",
        "        features['avg_chord_size'] = 0\n",
        "        features['chord_variance'] = 0\n",
        "    \n",
        "    # ==========================================\n",
        "    # VELOCITY/DYNAMICS FEATURES  \n",
        "    # ==========================================\n",
        "    \n",
        "    if np.any(piano_roll > 0):\n",
        "        velocities = piano_roll[piano_roll > 0]\n",
        "        features['avg_velocity'] = np.mean(velocities)\n",
        "        features['velocity_variance'] = np.var(velocities)\n",
        "        features['velocity_range'] = np.max(velocities) - np.min(velocities)\n",
        "        features['dynamic_complexity'] = len(np.unique(velocities)) / len(velocities)\n",
        "    else:\n",
        "        features.update({\n",
        "            'avg_velocity': 0, 'velocity_variance': 0,\n",
        "            'velocity_range': 0, 'dynamic_complexity': 0\n",
        "        })\n",
        "    \n",
        "    # ==========================================\n",
        "    # STYLE-SPECIFIC FEATURES\n",
        "    # ==========================================\n",
        "    \n",
        "    # Measure musical \"busyness\" vs \"spaciousness\"\n",
        "    features['overall_density'] = np.sum(piano_roll > 0) / piano_roll.size\n",
        "    \n",
        "    # Temporal consistency (how regular/irregular the rhythm is)\n",
        "    if len(note_density_timeline) > 1:\n",
        "        features['rhythmic_regularity'] = 1 / (1 + np.var(note_density_timeline))\n",
        "    else:\n",
        "        features['rhythmic_regularity'] = 1.0\n",
        "        \n",
        "    # Pitch movement patterns\n",
        "    if np.sum(active_pitches) > 1:\n",
        "        pitch_centers = []\n",
        "        for t in range(piano_roll.shape[1]):\n",
        "            frame = piano_roll[:, t]\n",
        "            if np.any(frame > 0):\n",
        "                weighted_pitch = np.sum(np.arange(128) * frame) / np.sum(frame)\n",
        "                pitch_centers.append(weighted_pitch)\n",
        "        \n",
        "        if len(pitch_centers) > 1:\n",
        "            pitch_movement = np.diff(pitch_centers)\n",
        "            features['pitch_movement_variance'] = np.var(pitch_movement)\n",
        "            features['melodic_direction_changes'] = np.sum(np.diff(np.sign(pitch_movement)) != 0) / len(pitch_movement) if len(pitch_movement) > 0 else 0\n",
        "        else:\n",
        "            features['pitch_movement_variance'] = 0\n",
        "            features['melodic_direction_changes'] = 0\n",
        "    else:\n",
        "        features['pitch_movement_variance'] = 0\n",
        "        features['melodic_direction_changes'] = 0\n",
        "    \n",
        "    return features\n",
        "\n",
        "# Test the enhanced feature extraction\n",
        "print(\"🧪 Testing enhanced musical feature extraction...\")\n",
        "if 'improved_data' in globals() and improved_data is not None:\n",
        "    sample_piano_roll = improved_data[0]\n",
        "    enhanced_features = extract_comprehensive_musical_features(sample_piano_roll)\n",
        "    \n",
        "    print(f\"✅ Enhanced features extracted!\")\n",
        "    print(f\"Number of features: {len(enhanced_features)}\")\n",
        "    print(f\"Feature names: {list(enhanced_features.keys())}\")\n",
        "    print(f\"Sample values: {list(enhanced_features.values())[:5]}...\")\n",
        "else:\n",
        "    print(\"⏳ Run data loading first to test feature extraction\")\n",
        "\n",
        "print(\"\\n🎯 ENHANCED FEATURES READY FOR MULTIMODAL!\")\n",
        "print(\"Features include:\")\n",
        "print(\"• Temporal/Rhythmic: density, sparsity, bursts\")  \n",
        "print(\"• Pitch/Harmonic: range, centroid, register distribution\")\n",
        "print(\"• Harmonic Complexity: chord ratios, polyphony\")\n",
        "print(\"• Dynamics: velocity patterns, expression\")\n",
        "print(\"• Style: movement patterns, regularity\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "920a6925",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# MULTIMODAL DATASET CLASS\n",
        "# =====================================================\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class that handles both piano rolls (for CNN) and musical features (for MLP)\n",
        "    \"\"\"\n",
        "    def __init__(self, piano_rolls, features_list, labels):\n",
        "        # Convert piano rolls to tensor\n",
        "        self.piano_rolls = torch.tensor(piano_rolls, dtype=torch.float32)\n",
        "        \n",
        "        # Convert feature dictionaries to feature vectors\n",
        "        self.features = self._process_features(features_list)\n",
        "        \n",
        "        # Convert labels to tensor\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "        \n",
        "        print(f\"✅ Multimodal Dataset Created:\")\n",
        "        print(f\"• Piano rolls: {self.piano_rolls.shape}\")\n",
        "        print(f\"• Features: {self.features.shape}\")\n",
        "        print(f\"• Labels: {self.labels.shape}\")\n",
        "        print(f\"• Total samples: {len(self.labels)}\")\n",
        "    \n",
        "    def _process_features(self, features_list):\n",
        "        \"\"\"Convert list of feature dictionaries to tensor\"\"\"\n",
        "        # Get feature names from first sample\n",
        "        feature_names = list(features_list[0].keys())\n",
        "        \n",
        "        # Extract feature values for all samples\n",
        "        feature_matrix = []\n",
        "        for feature_dict in features_list:\n",
        "            feature_vector = [feature_dict[name] for name in feature_names]\n",
        "            feature_matrix.append(feature_vector)\n",
        "        \n",
        "        # Convert to tensor and normalize\n",
        "        features_tensor = torch.tensor(feature_matrix, dtype=torch.float32)\n",
        "        \n",
        "        # Normalize features (important for MLP training)\n",
        "        features_mean = features_tensor.mean(dim=0)\n",
        "        features_std = features_tensor.std(dim=0)\n",
        "        features_std[features_std == 0] = 1  # Avoid division by zero\n",
        "        features_normalized = (features_tensor - features_mean) / features_std\n",
        "        \n",
        "        print(f\"📊 Feature Processing:\")\n",
        "        print(f\"• Feature names: {feature_names[:5]}...\")\n",
        "        print(f\"• Features normalized: mean≈0, std≈1\")\n",
        "        \n",
        "        return features_normalized\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Piano roll with channel dimension for CNN: (1, 128, T)\n",
        "        piano_roll = self.piano_rolls[idx].unsqueeze(0)\n",
        "        \n",
        "        # Feature vector for MLP: (num_features,)\n",
        "        features = self.features[idx]\n",
        "        \n",
        "        # Label\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        return piano_roll, features, label\n",
        "\n",
        "# Create enhanced multimodal dataset with comprehensive features\n",
        "print(\"🎵 CREATING MULTIMODAL DATASET...\")\n",
        "print(\"Extracting comprehensive features for all samples...\")\n",
        "\n",
        "# Extract comprehensive features for all loaded data\n",
        "if improved_data is not None:\n",
        "    print(\"Extracting comprehensive features for multimodal training...\")\n",
        "    comprehensive_features = []\n",
        "    \n",
        "    for i, piano_roll in enumerate(improved_data):\n",
        "        features = extract_comprehensive_musical_features(piano_roll)\n",
        "        comprehensive_features.append(features)\n",
        "        \n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"  Processed {i + 1}/{len(improved_data)} samples...\")\n",
        "    \n",
        "    # Create multimodal dataset\n",
        "    multimodal_dataset = MultimodalDataset(\n",
        "        piano_rolls=improved_data,\n",
        "        features_list=comprehensive_features,\n",
        "        labels=improved_labels\n",
        "    )\n",
        "    \n",
        "    print(\"\\n🎯 MULTIMODAL DATASET READY!\")\n",
        "    print(\"Ready for dual-stream architecture training!\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ No data loaded - run data loading first!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afcff6cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# CREATE MULTIMODAL DATASET FROM SEGMENTED DATA\n",
        "# =====================================================\n",
        "\n",
        "# Create enhanced multimodal dataset with comprehensive features from segmented data\n",
        "print(\"🎵 CREATING MULTIMODAL DATASET FROM SEGMENTED DATA...\")\n",
        "\n",
        "if segmented_data is not None:\n",
        "    print(\"Extracting comprehensive features for multimodal training...\")\n",
        "    comprehensive_features = []\n",
        "    \n",
        "    for i, piano_roll in enumerate(segmented_data):\n",
        "        features = extract_comprehensive_musical_features(piano_roll)\n",
        "        comprehensive_features.append(features)\n",
        "        \n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"  Processed {i + 1}/{len(segmented_data)} samples...\")\n",
        "    \n",
        "    # Create multimodal dataset with segmented, balanced data\n",
        "    multimodal_dataset = MultimodalDataset(\n",
        "        piano_rolls=segmented_data,\n",
        "        features_list=comprehensive_features,\n",
        "        labels=segmented_labels\n",
        "    )\n",
        "    \n",
        "    print(\"\\n🎯 BALANCED MULTIMODAL DATASET READY!\")\n",
        "    print(\"Using segmented data with temporal consistency preserved!\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ No segmented data loaded - run segmentation first!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "755f884a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# MULTIMODAL CNN-MLP FUSION ARCHITECTURE\n",
        "# =====================================================\n",
        "\n",
        "class MultimodalComposerClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Multimodal architecture combining CNN (piano rolls) + MLP (musical features)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=4, num_features=None):\n",
        "        super(MultimodalComposerClassifier, self).__init__()\n",
        "        \n",
        "        # CNN Stream for Piano Rolls (visual patterns)\n",
        "        self.cnn_stream = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.2),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "            \n",
        "            # Block 2\n",
        "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.3),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "            \n",
        "            # Block 3\n",
        "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.4),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "            \n",
        "            # Global pooling instead of LSTM\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        \n",
        "        # MLP Stream for Musical Features (hand-crafted features)\n",
        "        self.mlp_stream = nn.Sequential(\n",
        "            nn.Linear(num_features, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        \n",
        "        # Fusion Layer (combine both streams)\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(128 + 32, 256),  # 128 from CNN + 32 from MLP\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "        \n",
        "    def forward(self, piano_roll, features):\n",
        "        # CNN stream processes piano rolls\n",
        "        cnn_features = self.cnn_stream(piano_roll)\n",
        "        \n",
        "        # MLP stream processes musical features\n",
        "        mlp_features = self.mlp_stream(features)\n",
        "        \n",
        "        # Concatenate both feature streams\n",
        "        combined = torch.cat([cnn_features, mlp_features], dim=1)\n",
        "        \n",
        "        # Final classification\n",
        "        output = self.fusion(combined)\n",
        "        \n",
        "        return output\n",
        "\n",
        "print(\"🏗️ Multimodal Architecture Defined!\")\n",
        "print(\"• CNN Stream: Processes piano roll visual patterns\")\n",
        "print(\"• MLP Stream: Processes hand-crafted musical features\") \n",
        "print(\"• Fusion Layer: Combines both streams for classification\")\n",
        "print(\"• No LSTM: Uses global pooling for better efficiency\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "275cc0f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# TRAINING SETUP & EXECUTION\n",
        "# =====================================================\n",
        "\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create model\n",
        "num_features = len(comprehensive_features[0])  # Number of musical features\n",
        "model = MultimodalComposerClassifier(\n",
        "    num_classes=4, \n",
        "    num_features=num_features\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model created with {num_features} musical features\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Train/validation split\n",
        "train_size = int(0.8 * len(multimodal_dataset))\n",
        "val_size = len(multimodal_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(multimodal_dataset, [train_size, val_size])\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Training setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)\n",
        "\n",
        "# Training function\n",
        "def train_multimodal_model(model, train_loader, val_loader, epochs=25):\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        \n",
        "        for piano_rolls, features, labels in train_loader:\n",
        "            piano_rolls, features, labels = piano_rolls.to(device), features.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(piano_rolls, features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for piano_rolls, features, labels in val_loader:\n",
        "                piano_rolls, features, labels = piano_rolls.to(device), features.to(device), labels.to(device)\n",
        "                outputs = model(piano_rolls, features)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        val_accuracy = 100 * correct / total\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        \n",
        "        train_losses.append(avg_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        \n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
        "        \n",
        "        scheduler.step()\n",
        "    \n",
        "    return train_losses, val_accuracies\n",
        "\n",
        "# Start training\n",
        "print(\"🚀 Starting multimodal training...\")\n",
        "train_losses, val_accuracies = train_multimodal_model(model, train_loader, val_loader, epochs=25)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
