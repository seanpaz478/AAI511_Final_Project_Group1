{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install pretty_midi kagglehub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caJC4Lf_md2R",
        "outputId": "ca8cc17c-6ebb-4d10-934d-8e0796ae5b98"
      },
      "id": "caJC4Lf_md2R",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m178.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (2.0.2)\n",
            "Collecting mido>=1.1.16 (from pretty_midi)\n",
            "  Downloading mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.7.14)\n",
            "Downloading mido-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pretty_midi\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592286 sha256=6b7d68d4fa0805eb47fc8952dc81a689bab8826193208904e3ba2c3a5a908348\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/95/ac/15ceaeb2823b04d8e638fd1495357adb8d26c00ccac9d7782e\n",
            "Successfully built pretty_midi\n",
            "Installing collected packages: mido, pretty_midi\n",
            "Successfully installed mido-1.3.3 pretty_midi-0.2.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "998891eb",
      "metadata": {
        "id": "998891eb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import zipfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import numpy as np\n",
        "import pretty_midi\n",
        "import torch\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6406d2f8",
      "metadata": {
        "id": "6406d2f8"
      },
      "outputs": [],
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c06d7fe",
      "metadata": {
        "id": "2c06d7fe"
      },
      "outputs": [],
      "source": [
        "if not hasattr(np, 'int'):\n",
        "    np.int = int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd63e1d1",
      "metadata": {
        "id": "dd63e1d1"
      },
      "outputs": [],
      "source": [
        "# Define your model class (must match the architecture used in final-project1)\n",
        "class CNN_LSTM_Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=4, lstm_hidden=256):\n",
        "        super(CNN_LSTM_Classifier, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.2),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.3),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.4),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "        self.feature_size = 128 * 16\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.feature_size,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=0.3,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=lstm_hidden * 2,\n",
        "            num_heads=8,\n",
        "            dropout=0.3,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(lstm_hidden * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = x.contiguous().view(batch_size, x.size(1), -1)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
        "        pooled = torch.mean(attn_out, dim=1)\n",
        "        output = self.classifier(pooled)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89e8b887",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89e8b887",
        "outputId": "4f57181d-aee5-414e-d78f-387317635aa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab.\n",
            "✅ Loaded: /content/saved_models/original_cnn_lstm.pth\n",
            "✅ Loaded: /content/saved_models/rhythm_augmented_cnn_lstm.pth\n"
          ]
        }
      ],
      "source": [
        "# Load the models\n",
        "from IPython import get_ipython\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    print(\"Running in Google Colab.\")\n",
        "    original_path = os.path.join('/content/saved_models', 'original_cnn_lstm.pth')\n",
        "    rhythm_path = os.path.join('/content/saved_models', 'rhythm_augmented_cnn_lstm.pth')\n",
        "else:\n",
        "    print(\"Not running in Google Colab.\")\n",
        "    original_path = os.path.join('saved_models', 'original_cnn_lstm.pth')\n",
        "    rhythm_path = os.path.join('saved_models', 'rhythm_augmented_cnn_lstm.pth')\n",
        "\n",
        "model = CNN_LSTM_Classifier(num_classes=4, lstm_hidden=256).to(device)\n",
        "model.load_state_dict(torch.load(original_path, map_location=device))\n",
        "print(f\"✅ Loaded: {original_path}\")\n",
        "\n",
        "rhythm_model = CNN_LSTM_Classifier(num_classes=4, lstm_hidden=256).to(device)\n",
        "rhythm_model.load_state_dict(torch.load(rhythm_path, map_location=device))\n",
        "print(f\"✅ Loaded: {rhythm_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "afa70024",
      "metadata": {
        "id": "afa70024"
      },
      "outputs": [],
      "source": [
        "TARGET_COMPOSERS = [\n",
        "    'Bach',\n",
        "    'Beethoven',\n",
        "    'Chopin',\n",
        "    'Mozart',\n",
        "]\n",
        "\n",
        "path = kagglehub.dataset_download(\"blanderbuss/midi-classic-music\")\n",
        "\n",
        "zip_path = os.path.join(path, 'midiclassics.zip')\n",
        "extract_path = os.path.join('data', 'kaggle', 'midiclassics')\n",
        "\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# list files in extract_path that contain the target composers in name\n",
        "for composer in TARGET_COMPOSERS:\n",
        "    composer_files = [f for f in os.listdir(extract_path) if composer.lower() in f.lower()]\n",
        "\n",
        "# Only keep directories that contain a target composer's name\n",
        "for item in os.listdir(extract_path):\n",
        "    item_path = os.path.join(extract_path, item)\n",
        "    if not any(composer.lower() in item.lower() for composer in TARGET_COMPOSERS):\n",
        "        if os.path.isfile(item_path):\n",
        "            os.remove(item_path)\n",
        "        elif os.path.isdir(item_path):\n",
        "            shutil.rmtree(item_path)\n",
        "\n",
        "# also delete \"C.P.E.Bach\" files. This was the son of J.S. Bach, and we want to keep only the main composers\n",
        "for item in os.listdir(extract_path):\n",
        "    if 'C.P.E.Bach' in item:\n",
        "        item_path = os.path.join(extract_path, item)\n",
        "        if os.path.isfile(item_path):\n",
        "            os.remove(item_path)\n",
        "        elif os.path.isdir(item_path):\n",
        "            shutil.rmtree(item_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c7eb056a",
      "metadata": {
        "id": "c7eb056a"
      },
      "outputs": [],
      "source": [
        "class PianoRollDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        # Add channel dimension for CNN: (1, 128, T)\n",
        "        return self.data[idx].unsqueeze(0), self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ad781104",
      "metadata": {
        "id": "ad781104"
      },
      "outputs": [],
      "source": [
        "def get_piano_roll_improved(midi_path, fs=100, target_duration=45.0):\n",
        "    \"\"\"\n",
        "    Improved MIDI to piano roll conversion with musical awareness\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pm = pretty_midi.PrettyMIDI(midi_path)\n",
        "\n",
        "        # Get the actual duration of the piece\n",
        "        actual_duration = pm.get_end_time()\n",
        "\n",
        "        # If piece is very short, skip it\n",
        "        if actual_duration < 10.0:  # Less than 10 seconds\n",
        "            return None\n",
        "\n",
        "        # For long pieces, extract multiple segments\n",
        "        if actual_duration > target_duration * 1.5:\n",
        "            # Extract from different parts of the piece\n",
        "            segments = []\n",
        "            num_segments = min(3, int(actual_duration // target_duration))\n",
        "\n",
        "            for i in range(num_segments):\n",
        "                start_time = i * (actual_duration / num_segments)\n",
        "                end_time = start_time + target_duration\n",
        "\n",
        "                # Create a copy and trim\n",
        "                pm_segment = pretty_midi.PrettyMIDI()\n",
        "                for instrument in pm.instruments:\n",
        "                    new_instrument = pretty_midi.Instrument(\n",
        "                        program=instrument.program,\n",
        "                        is_drum=instrument.is_drum,\n",
        "                        name=instrument.name\n",
        "                    )\n",
        "\n",
        "                    for note in instrument.notes:\n",
        "                        if start_time <= note.start < end_time:\n",
        "                            new_note = pretty_midi.Note(\n",
        "                                velocity=note.velocity,\n",
        "                                pitch=note.pitch,\n",
        "                                start=note.start - start_time,\n",
        "                                end=min(note.end - start_time, target_duration)\n",
        "                            )\n",
        "                            new_instrument.notes.append(new_note)\n",
        "\n",
        "                    if new_instrument.notes:\n",
        "                        pm_segment.instruments.append(new_instrument)\n",
        "\n",
        "                if pm_segment.instruments:\n",
        "                    piano_roll = pm_segment.get_piano_roll(fs=fs)\n",
        "                    target_length = int(target_duration * fs)\n",
        "\n",
        "                    if piano_roll.shape[1] > target_length:\n",
        "                        piano_roll = piano_roll[:, :target_length]\n",
        "                    else:\n",
        "                        pad_width = target_length - piano_roll.shape[1]\n",
        "                        piano_roll = np.pad(piano_roll, ((0,0),(0,pad_width)), mode='constant')\n",
        "\n",
        "                    segments.append(piano_roll)\n",
        "\n",
        "            return segments\n",
        "\n",
        "        else:\n",
        "            # For normal length pieces, use the whole piece\n",
        "            piano_roll = pm.get_piano_roll(fs=fs)\n",
        "            target_length = int(target_duration * fs)\n",
        "\n",
        "            if piano_roll.shape[1] > target_length:\n",
        "                # Take from the middle rather than truncating end\n",
        "                start_idx = (piano_roll.shape[1] - target_length) // 2\n",
        "                piano_roll = piano_roll[:, start_idx:start_idx + target_length]\n",
        "            else:\n",
        "                pad_width = target_length - piano_roll.shape[1]\n",
        "                piano_roll = np.pad(piano_roll, ((0,0),(0,pad_width)), mode='constant')\n",
        "\n",
        "            return [piano_roll]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {midi_path}: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "230bdd12",
      "metadata": {
        "id": "230bdd12"
      },
      "outputs": [],
      "source": [
        "def normalize_piano_roll(piano_roll):\n",
        "    \"\"\"\n",
        "    Apply musical normalization to piano roll\n",
        "    \"\"\"\n",
        "    # 1. Velocity normalization (already 0-1 from pretty_midi)\n",
        "    normalized = piano_roll.copy()\n",
        "\n",
        "    # 2. Optional: Focus on active pitch range\n",
        "    active_pitches = np.any(normalized > 0, axis=1)\n",
        "    if np.any(active_pitches):\n",
        "        first_active = np.argmax(active_pitches)\n",
        "        last_active = len(active_pitches) - 1 - np.argmax(active_pitches[::-1])\n",
        "\n",
        "        # Ensure we keep a reasonable range (at least 60 semitones = 5 octaves)\n",
        "        min_range = 60\n",
        "        current_range = last_active - first_active + 1\n",
        "\n",
        "        if current_range < min_range:\n",
        "            expand = (min_range - current_range) // 2\n",
        "            first_active = max(0, first_active - expand)\n",
        "            last_active = min(127, last_active + expand)\n",
        "\n",
        "    return normalized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b1b1f0c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1b1f0c8",
        "outputId": "9b42d5aa-6afe-41b2-d758-6d4047f65bba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Improved data processing functions defined!\n",
            "Key improvements:\n",
            "• Intelligent segment extraction for long pieces\n",
            "• Musical boundary awareness\n",
            "• Better normalization\n",
            "• Feature extraction for analysis\n"
          ]
        }
      ],
      "source": [
        "def extract_musical_features(piano_roll):\n",
        "    \"\"\"\n",
        "    Extract features that capture musical style\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "\n",
        "    # Temporal features\n",
        "    note_density_timeline = np.sum(piano_roll > 0, axis=0)\n",
        "    features['avg_notes_per_time'] = np.mean(note_density_timeline)\n",
        "    features['note_density_variance'] = np.var(note_density_timeline)\n",
        "\n",
        "    # Pitch features\n",
        "    pitch_activity = np.sum(piano_roll > 0, axis=1)\n",
        "    active_pitches = pitch_activity > 0\n",
        "    if np.any(active_pitches):\n",
        "        features['pitch_range'] = np.sum(active_pitches)\n",
        "        features['lowest_pitch'] = np.argmax(active_pitches)\n",
        "        features['highest_pitch'] = 127 - np.argmax(active_pitches[::-1])\n",
        "    else:\n",
        "        features['pitch_range'] = 0\n",
        "        features['lowest_pitch'] = 60  # Middle C\n",
        "        features['highest_pitch'] = 60\n",
        "\n",
        "    # Rhythmic features\n",
        "    onset_pattern = np.diff(note_density_timeline > 0).astype(int)\n",
        "    features['onset_density'] = np.sum(onset_pattern == 1) / len(onset_pattern)\n",
        "\n",
        "    return features\n",
        "\n",
        "print(\"✅ Improved data processing functions defined!\")\n",
        "print(\"Key improvements:\")\n",
        "print(\"• Intelligent segment extraction for long pieces\")\n",
        "print(\"• Musical boundary awareness\")\n",
        "print(\"• Better normalization\")\n",
        "print(\"• Feature extraction for analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9771e8b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9771e8b1",
        "outputId": "fbdafcbb-dc52-4c15-c31b-8ec7e1c080fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting improved data loading...\n",
            "🎵 LOADING DATASET WITH IMPROVED PROCESSING...\n",
            "Improvements over original:\n",
            "• Intelligent segment extraction for long pieces\n",
            "• Better handling of piece lengths\n",
            "• Musical feature extraction\n",
            "• Quality filtering\n",
            "\n",
            "--- Processing Bach ---\n",
            "  Processed 10 files, created 25 segments...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processed 20 files, created 51 segments...\n",
            "  Processed 30 files, created 77 segments...\n",
            "  Processed 40 files, created 104 segments...\n",
            "  Processed 50 files, created 130 segments...\n",
            "  Processed 60 files, created 154 segments...\n",
            "  Processed 70 files, created 182 segments...\n",
            "  Processed 80 files, created 211 segments...\n",
            "  Processed 90 files, created 233 segments...\n",
            "  Processed 100 files, created 261 segments...\n",
            "  Processed 110 files, created 286 segments...\n",
            "  Processed 120 files, created 312 segments...\n",
            "✅ Bach: 120 files → 312 segments\n",
            "  Final data shape: (312, 128, 4500)\n",
            "\n",
            "--- Processing Beethoven ---\n",
            "  Processed 10 files, created 26 segments...\n",
            "  Processed 20 files, created 52 segments...\n",
            "Error processing data/kaggle/midiclassics/Beethoven/Anhang 14-3.mid: Could not decode key with 3 flats and mode 255\n",
            "  Processed 30 files, created 78 segments...\n",
            "  Processed 40 files, created 105 segments...\n",
            "  Processed 50 files, created 126 segments...\n",
            "  Processed 60 files, created 153 segments...\n",
            "  Processed 70 files, created 176 segments...\n",
            "  Processed 80 files, created 201 segments...\n",
            "  Processed 90 files, created 226 segments...\n",
            "  Processed 100 files, created 251 segments...\n",
            "  Processed 110 files, created 279 segments...\n",
            "✅ Beethoven: 119 files → 304 segments\n",
            "  Final data shape: (304, 128, 4500)\n",
            "\n",
            "--- Processing Chopin ---\n",
            "  Processed 10 files, created 23 segments...\n",
            "  Processed 20 files, created 45 segments...\n",
            "  Processed 30 files, created 66 segments...\n",
            "  Processed 40 files, created 92 segments...\n",
            "  Processed 50 files, created 117 segments...\n",
            "  Processed 60 files, created 135 segments...\n",
            "  Processed 70 files, created 157 segments...\n",
            "  Processed 80 files, created 178 segments...\n",
            "  Processed 90 files, created 203 segments...\n",
            "  Processed 100 files, created 232 segments...\n",
            "  Processed 110 files, created 258 segments...\n",
            "  Processed 120 files, created 286 segments...\n",
            "✅ Chopin: 120 files → 286 segments\n",
            "  Final data shape: (286, 128, 4500)\n",
            "\n",
            "--- Processing Mozart ---\n",
            "  Processed 10 files, created 28 segments...\n",
            "  Processed 20 files, created 53 segments...\n",
            "  Processed 30 files, created 78 segments...\n",
            "  Processed 40 files, created 106 segments...\n",
            "  Processed 50 files, created 131 segments...\n",
            "  Processed 60 files, created 157 segments...\n",
            "  Processed 70 files, created 187 segments...\n",
            "  Processed 80 files, created 216 segments...\n",
            "  Processed 90 files, created 244 segments...\n",
            "✅ Mozart: 90 files → 244 segments\n",
            "  Final data shape: (244, 128, 4500)\n",
            "\n",
            "🎯 FINAL IMPROVED DATASET:\n",
            "Total samples: 1146\n",
            "Data shape: (1146, 128, 4500)\n",
            "Label distribution: [312 304 286 244]\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# IMPROVED DATA LOADING WITH BETTER PROCESSING\n",
        "# =====================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def load_improved_dataset(extract_path, target_composers, target_duration=45.0, max_files_per_composer=None):\n",
        "    \"\"\"\n",
        "    Load dataset with improved processing that addresses previous shortcomings\n",
        "    \"\"\"\n",
        "    print(\"🎵 LOADING DATASET WITH IMPROVED PROCESSING...\")\n",
        "    print(\"Improvements over original:\")\n",
        "    print(\"• Intelligent segment extraction for long pieces\")\n",
        "    print(\"• Better handling of piece lengths\")\n",
        "    print(\"• Musical feature extraction\")\n",
        "    print(\"• Quality filtering\")\n",
        "\n",
        "    composer_to_idx = {c: i for i, c in enumerate(target_composers)}\n",
        "    all_data = []\n",
        "    all_labels = []\n",
        "    all_features = []\n",
        "\n",
        "    for composer in target_composers:\n",
        "        print(f\"\\n--- Processing {composer} ---\")\n",
        "        composer_dir = os.path.join(extract_path, composer)\n",
        "\n",
        "        if not os.path.isdir(composer_dir):\n",
        "            print(f\"Directory not found: {composer_dir}\")\n",
        "            continue\n",
        "\n",
        "        composer_data = []\n",
        "        composer_labels = []\n",
        "        composer_features = []\n",
        "        files_processed = 0\n",
        "        segments_created = 0\n",
        "\n",
        "        midi_files = [f for f in os.listdir(composer_dir)\n",
        "                     if f.lower().endswith(('.mid', '.midi'))]\n",
        "\n",
        "        if max_files_per_composer:\n",
        "            midi_files = midi_files[:max_files_per_composer]\n",
        "\n",
        "        for file in midi_files:\n",
        "            midi_path = os.path.join(composer_dir, file)\n",
        "\n",
        "            try:\n",
        "                # Use improved processing\n",
        "                segments = get_piano_roll_improved(midi_path, target_duration=target_duration)\n",
        "\n",
        "                if segments is None:\n",
        "                    continue\n",
        "\n",
        "                for segment in segments:\n",
        "                    # Normalize the segment\n",
        "                    normalized_segment = normalize_piano_roll(segment)\n",
        "\n",
        "                    # Extract musical features\n",
        "                    features = extract_musical_features(normalized_segment)\n",
        "\n",
        "                    # Quality check: skip if too sparse\n",
        "                    note_density = features['avg_notes_per_time']\n",
        "                    if note_density < 0.1:  # Very sparse, likely poor quality\n",
        "                        continue\n",
        "\n",
        "                    composer_data.append(normalized_segment)\n",
        "                    composer_labels.append(composer_to_idx[composer])\n",
        "                    composer_features.append(features)\n",
        "                    segments_created += 1\n",
        "\n",
        "                files_processed += 1\n",
        "\n",
        "                if files_processed % 10 == 0:\n",
        "                    print(f\"  Processed {files_processed} files, created {segments_created} segments...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error processing {file}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"✅ {composer}: {files_processed} files → {segments_created} segments\")\n",
        "\n",
        "        if composer_data:\n",
        "            composer_data = np.array(composer_data)\n",
        "            composer_labels = np.array(composer_labels)\n",
        "\n",
        "            all_data.append(composer_data)\n",
        "            all_labels.append(composer_labels)\n",
        "            all_features.extend(composer_features)\n",
        "\n",
        "            print(f\"  Final data shape: {composer_data.shape}\")\n",
        "\n",
        "    # Combine all data\n",
        "    if all_data:\n",
        "        data = np.concatenate(all_data, axis=0)\n",
        "        labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "        print(f\"\\n🎯 FINAL IMPROVED DATASET:\")\n",
        "        print(f\"Total samples: {len(data)}\")\n",
        "        print(f\"Data shape: {data.shape}\")\n",
        "        print(f\"Label distribution: {np.bincount(labels)}\")\n",
        "\n",
        "        return data, labels, all_features\n",
        "    else:\n",
        "        print(\"❌ No data loaded!\")\n",
        "        return None, None, None\n",
        "\n",
        "# Load the improved dataset\n",
        "print(\"🚀 Starting improved data loading...\")\n",
        "improved_data, improved_labels, features = load_improved_dataset(\n",
        "    extract_path,\n",
        "    TARGET_COMPOSERS,\n",
        "    target_duration=45.0,  # 45 seconds per segment\n",
        "    max_files_per_composer=120  # Limit for testing - remove for full dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80f24ee1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80f24ee1",
        "outputId": "7bed835d-4b3d-4ef3-f82a-063995dd83cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 TESTING MODELS ON IMPROVED DATASET\n",
            "==================================================\n",
            "\n",
            "📊 Original Model Results:\n",
            "Accuracy: 68.32%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Bach      0.717     0.875     0.788       312\n",
            "   Beethoven      0.682     0.480     0.564       304\n",
            "      Chopin      0.775     0.724     0.749       286\n",
            "      Mozart      0.553     0.643     0.595       244\n",
            "\n",
            "    accuracy                          0.683      1146\n",
            "   macro avg      0.682     0.681     0.674      1146\n",
            "weighted avg      0.687     0.683     0.677      1146\n",
            "\n",
            "\n",
            "📊 Rhythm Model Results:\n",
            "Accuracy: 72.08%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Bach      0.839     0.817     0.828       312\n",
            "   Beethoven      0.623     0.724     0.670       304\n",
            "      Chopin      0.835     0.689     0.755       286\n",
            "      Mozart      0.609     0.631     0.620       244\n",
            "\n",
            "    accuracy                          0.721      1146\n",
            "   macro avg      0.726     0.715     0.718      1146\n",
            "weighted avg      0.732     0.721     0.723      1146\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# TEST TRAINED MODELS ON IMPROVED DATASET\n",
        "# =====================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create dataset and dataloader for improved data\n",
        "improved_dataset = PianoRollDataset(improved_data, improved_labels)\n",
        "test_loader = DataLoader(improved_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "def evaluate_model(model, dataloader, model_name):\n",
        "    \"\"\"Evaluate a single model\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * np.mean(np.array(all_preds) == np.array(all_targets))\n",
        "\n",
        "    print(f\"\\n📊 {model_name} Results:\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_targets, all_preds,\n",
        "                              target_names=TARGET_COMPOSERS,\n",
        "                              digits=3))\n",
        "\n",
        "    return all_preds, all_targets, all_probs, accuracy\n",
        "\n",
        "# Test both models\n",
        "print(\"🧪 TESTING MODELS ON IMPROVED DATASET\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Test original model\n",
        "orig_preds, targets, orig_probs, orig_acc = evaluate_model(model, test_loader, \"Original Model\")\n",
        "\n",
        "# Test rhythm model\n",
        "rhythm_preds, _, rhythm_probs, rhythm_acc = evaluate_model(rhythm_model, test_loader, \"Rhythm Model\")\n",
        "\n",
        "# Convert to numpy arrays\n",
        "orig_probs = np.array(orig_probs)\n",
        "rhythm_probs = np.array(rhythm_probs)\n",
        "targets = np.array(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d0aefe6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0aefe6d",
        "outputId": "9ef3f32c-b7f5-460a-8a50-c4bc45a946a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Non-overlapping segmentation function defined!\n"
          ]
        }
      ],
      "source": [
        "def get_piano_roll_non_overlapping_segments(midi_path, fs=100, segment_duration=45.0):\n",
        "    \"\"\"\n",
        "    Extract non-overlapping segments to avoid data leakage with LSTM\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pm = pretty_midi.PrettyMIDI(midi_path)\n",
        "\n",
        "        # Get the actual duration of the piece\n",
        "        actual_duration = pm.get_end_time()\n",
        "\n",
        "        # If piece is very short, skip it\n",
        "        if actual_duration < 15.0:  # Less than 15 seconds\n",
        "            return None\n",
        "\n",
        "        segments = []\n",
        "        segment_size = segment_duration\n",
        "\n",
        "        # Calculate number of non-overlapping segments\n",
        "        num_segments = int(actual_duration // segment_size)\n",
        "\n",
        "        # If piece is shorter than one segment, use the whole piece (padded)\n",
        "        if num_segments == 0:\n",
        "            piano_roll = pm.get_piano_roll(fs=fs)\n",
        "            target_length = int(segment_duration * fs)\n",
        "\n",
        "            if piano_roll.shape[1] < target_length:\n",
        "                pad_width = target_length - piano_roll.shape[1]\n",
        "                piano_roll = np.pad(piano_roll, ((0,0),(0,pad_width)), mode='constant')\n",
        "            else:\n",
        "                piano_roll = piano_roll[:, :target_length]\n",
        "\n",
        "            segments.append(piano_roll)\n",
        "            return segments\n",
        "\n",
        "        # Extract non-overlapping segments\n",
        "        for i in range(num_segments):\n",
        "            start_time = i * segment_size\n",
        "            end_time = start_time + segment_size\n",
        "\n",
        "            # Create a copy and trim\n",
        "            pm_segment = pretty_midi.PrettyMIDI()\n",
        "            for instrument in pm.instruments:\n",
        "                new_instrument = pretty_midi.Instrument(\n",
        "                    program=instrument.program,\n",
        "                    is_drum=instrument.is_drum,\n",
        "                    name=instrument.name\n",
        "                )\n",
        "\n",
        "                for note in instrument.notes:\n",
        "                    if start_time <= note.start < end_time:\n",
        "                        new_note = pretty_midi.Note(\n",
        "                            velocity=note.velocity,\n",
        "                            pitch=note.pitch,\n",
        "                            start=note.start - start_time,\n",
        "                            end=min(note.end - start_time, segment_duration)\n",
        "                        )\n",
        "                        new_instrument.notes.append(new_note)\n",
        "\n",
        "                if new_instrument.notes:\n",
        "                    pm_segment.instruments.append(new_instrument)\n",
        "\n",
        "            if pm_segment.instruments:\n",
        "                piano_roll = pm_segment.get_piano_roll(fs=fs)\n",
        "                target_length = int(segment_duration * fs)\n",
        "\n",
        "                if piano_roll.shape[1] > target_length:\n",
        "                    piano_roll = piano_roll[:, :target_length]\n",
        "                else:\n",
        "                    pad_width = target_length - piano_roll.shape[1]\n",
        "                    piano_roll = np.pad(piano_roll, ((0,0),(0,pad_width)), mode='constant')\n",
        "\n",
        "                segments.append(piano_roll)\n",
        "\n",
        "        return segments if segments else None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {midi_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"✅ Non-overlapping segmentation function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "43715e17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43715e17",
        "outputId": "e347e176-4aae-4075-9db9-0ff41242e076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting FULL non-overlapping segment data loading...\n",
            "🎵 LOADING FULL DATASET WITH NON-OVERLAPPING SEGMENTS...\n",
            "Benefits for LSTM training:\n",
            "• Segment duration: 45.0s (no overlap)\n",
            "• Using ALL available files (no limits)\n",
            "• No data leakage between train/test\n",
            "• Cleaner temporal boundaries\n",
            "• Will address class imbalance later during training\n",
            "\n",
            "--- Processing Bach ---\n",
            "  Found 131 MIDI files for Bach\n",
            "  Processed 50/131 files, created 394 segments...\n",
            "  Processed 100/131 files, created 715 segments...\n",
            "✅ Bach: 131/131 files → 977 segments\n",
            "  Final data shape: (977, 128, 4500)\n",
            "\n",
            "--- Processing Beethoven ---\n",
            "  Found 134 MIDI files for Beethoven\n",
            "Error processing data/kaggle/midiclassics/Beethoven/Anhang 14-3.mid: Could not decode key with 3 flats and mode 255\n",
            "  Processed 50/134 files, created 357 segments...\n",
            "  Processed 100/134 files, created 765 segments...\n",
            "✅ Beethoven: 133/134 files → 987 segments\n",
            "  Final data shape: (987, 128, 4500)\n",
            "\n",
            "--- Processing Chopin ---\n",
            "  Found 136 MIDI files for Chopin\n",
            "  Processed 50/136 files, created 218 segments...\n",
            "  Processed 100/136 files, created 449 segments...\n",
            "✅ Chopin: 136/136 files → 610 segments\n",
            "  Final data shape: (610, 128, 4500)\n",
            "\n",
            "--- Processing Mozart ---\n",
            "  Found 90 MIDI files for Mozart\n",
            "  Processed 50/90 files, created 271 segments...\n",
            "✅ Mozart: 90/90 files → 524 segments\n",
            "  Final data shape: (524, 128, 4500)\n",
            "\n",
            "🎯 FINAL FULL NON-OVERLAPPING DATASET:\n",
            "Total files processed: 490\n",
            "Total samples: 3098\n",
            "Data shape: (3098, 128, 4500)\n",
            "Label distribution: [977 987 610 524]\n",
            "  Bach: 977 samples (31.5%)\n",
            "  Beethoven: 987 samples (31.9%)\n",
            "  Chopin: 610 samples (19.7%)\n",
            "  Mozart: 524 samples (16.9%)\n"
          ]
        }
      ],
      "source": [
        "def load_dataset_non_overlapping_full(extract_path, target_composers, segment_duration=45.0):\n",
        "    \"\"\"\n",
        "    Load FULL dataset with non-overlapping segments - using ALL available files\n",
        "    \"\"\"\n",
        "    print(\"🎵 LOADING FULL DATASET WITH NON-OVERLAPPING SEGMENTS...\")\n",
        "    print(\"Benefits for LSTM training:\")\n",
        "    print(f\"• Segment duration: {segment_duration}s (no overlap)\")\n",
        "    print(\"• Using ALL available files (no limits)\")\n",
        "    print(\"• No data leakage between train/test\")\n",
        "    print(\"• Cleaner temporal boundaries\")\n",
        "    print(\"• Will address class imbalance later during training\")\n",
        "\n",
        "    composer_to_idx = {c: i for i, c in enumerate(target_composers)}\n",
        "    all_data = []\n",
        "    all_labels = []\n",
        "    all_features = []\n",
        "\n",
        "    total_files_processed = 0\n",
        "    total_segments_created = 0\n",
        "\n",
        "    for composer in target_composers:\n",
        "        print(f\"\\n--- Processing {composer} ---\")\n",
        "        composer_dir = os.path.join(extract_path, composer)\n",
        "\n",
        "        if not os.path.isdir(composer_dir):\n",
        "            print(f\"Directory not found: {composer_dir}\")\n",
        "            continue\n",
        "\n",
        "        composer_data = []\n",
        "        composer_labels = []\n",
        "        composer_features = []\n",
        "        files_processed = 0\n",
        "        segments_created = 0\n",
        "\n",
        "        # Get ALL MIDI files - no limit\n",
        "        midi_files = [f for f in os.listdir(composer_dir)\n",
        "                     if f.lower().endswith(('.mid', '.midi'))]\n",
        "\n",
        "        print(f\"  Found {len(midi_files)} MIDI files for {composer}\")\n",
        "\n",
        "        for file in midi_files:\n",
        "            midi_path = os.path.join(composer_dir, file)\n",
        "\n",
        "            try:\n",
        "                # Use non-overlapping segmentation\n",
        "                segments = get_piano_roll_non_overlapping_segments(\n",
        "                    midi_path,\n",
        "                    segment_duration=segment_duration\n",
        "                )\n",
        "\n",
        "                if segments is None:\n",
        "                    continue\n",
        "\n",
        "                for segment in segments:\n",
        "                    # Normalize the segment\n",
        "                    normalized_segment = normalize_piano_roll(segment)\n",
        "\n",
        "                    # Extract musical features\n",
        "                    features = extract_musical_features(normalized_segment)\n",
        "\n",
        "                    # Quality check: skip if too sparse\n",
        "                    note_density = features['avg_notes_per_time']\n",
        "                    if note_density < 0.1:  # Very sparse, likely poor quality\n",
        "                        continue\n",
        "\n",
        "                    composer_data.append(normalized_segment)\n",
        "                    composer_labels.append(composer_to_idx[composer])\n",
        "                    composer_features.append(features)\n",
        "                    segments_created += 1\n",
        "\n",
        "                files_processed += 1\n",
        "\n",
        "                # Progress update every 50 files for full dataset\n",
        "                if files_processed % 50 == 0:\n",
        "                    print(f\"  Processed {files_processed}/{len(midi_files)} files, created {segments_created} segments...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error processing {file}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"✅ {composer}: {files_processed}/{len(midi_files)} files → {segments_created} segments\")\n",
        "\n",
        "        if composer_data:\n",
        "            composer_data = np.array(composer_data)\n",
        "            composer_labels = np.array(composer_labels)\n",
        "\n",
        "            all_data.append(composer_data)\n",
        "            all_labels.append(composer_labels)\n",
        "            all_features.extend(composer_features)\n",
        "\n",
        "            print(f\"  Final data shape: {composer_data.shape}\")\n",
        "\n",
        "        total_files_processed += files_processed\n",
        "        total_segments_created += segments_created\n",
        "\n",
        "    # Combine all data\n",
        "    if all_data:\n",
        "        data = np.concatenate(all_data, axis=0)\n",
        "        labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "        print(f\"\\n🎯 FINAL FULL NON-OVERLAPPING DATASET:\")\n",
        "        print(f\"Total files processed: {total_files_processed}\")\n",
        "        print(f\"Total samples: {len(data)}\")\n",
        "        print(f\"Data shape: {data.shape}\")\n",
        "        print(f\"Label distribution: {np.bincount(labels)}\")\n",
        "\n",
        "        # Show class distribution percentages\n",
        "        for i, composer in enumerate(target_composers):\n",
        "            count = np.sum(labels == i)\n",
        "            percentage = (count / len(labels)) * 100\n",
        "            print(f\"  {composer}: {count} samples ({percentage:.1f}%)\")\n",
        "\n",
        "        return data, labels, all_features\n",
        "    else:\n",
        "        print(\"❌ No data loaded!\")\n",
        "        return None, None, None\n",
        "\n",
        "# Load the FULL dataset with non-overlapping segments\n",
        "print(\"🚀 Starting FULL non-overlapping segment data loading...\")\n",
        "full_data, full_labels, full_features = load_dataset_non_overlapping_full(\n",
        "    extract_path,\n",
        "    TARGET_COMPOSERS,\n",
        "    segment_duration=45.0  # 45-second segments, no overlap, ALL files\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1ecadfdd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ecadfdd",
        "outputId": "76a2cdf0-9b14-40b5-97c8-0deead9a8406"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Creating AGGRESSIVE model for A100 40GB...\n",
            "🚀 Building AGGRESSIVE CNN-LSTM-Transformer for A100 40GB...\n",
            "• Deep CNN feature extraction (6 blocks)\n",
            "• Large LSTM temporal modeling (hidden: 512)\n",
            "• Deep Transformer self-attention (dim: 1024, heads: 16, layers: 8)\n",
            "• Multi-scale feature fusion\n",
            "• Advanced attention mechanisms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ AGGRESSIVE CNN-LSTM-Transformer architecture built!\n",
            "Input shape: torch.Size([4, 1, 128, 4500])\n",
            "Main output shape: torch.Size([4, 4])\n",
            "Auxiliary output shape: torch.Size([4, 4])\n",
            "✅ Aggressive model forward pass successful!\n",
            "\n",
            "📊 AGGRESSIVE Model Statistics:\n",
            "Total parameters: 185,688,776\n",
            "Trainable parameters: 185,688,776\n",
            "Model size: ~708.3 MB\n",
            "Estimated GPU memory (training): ~2833.4 MB\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# AGGRESSIVE CNN-LSTM-TRANSFORMER FOR A100 40GB\n",
        "# =====================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Enhanced positional encoding for transformer\"\"\"\n",
        "    def __init__(self, d_model, max_len=10000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "class AggressiveCNN_LSTM_Transformer(nn.Module):\n",
        "    def __init__(self, num_classes=4, lstm_hidden=512, transformer_dim=1024, num_heads=16, num_layers=8):\n",
        "        super(AggressiveCNN_LSTM_Transformer, self).__init__()\n",
        "\n",
        "        print(\"🚀 Building AGGRESSIVE CNN-LSTM-Transformer for A100 40GB...\")\n",
        "        print(f\"• Deep CNN feature extraction (6 blocks)\")\n",
        "        print(f\"• Large LSTM temporal modeling (hidden: {lstm_hidden})\")\n",
        "        print(f\"• Deep Transformer self-attention (dim: {transformer_dim}, heads: {num_heads}, layers: {num_layers})\")\n",
        "        print(f\"• Multi-scale feature fusion\")\n",
        "        print(f\"• Advanced attention mechanisms\")\n",
        "\n",
        "        # ==========================================\n",
        "        # DEEP CNN BACKBONE - 6 BLOCKS\n",
        "        # ==========================================\n",
        "\n",
        "        # Block 1: Initial feature extraction\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.1),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))  # 128x64 -> 64x32\n",
        "        )\n",
        "\n",
        "        # Block 2: Deeper features\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.15),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))  # 64x32 -> 32x16\n",
        "        )\n",
        "\n",
        "        # Block 3: More complex patterns\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.2),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))  # 32x16 -> 16x8\n",
        "        )\n",
        "\n",
        "        # Block 4: High-level features\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.25),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))  # 16x8 -> 8x4\n",
        "        )\n",
        "\n",
        "        # Block 5: Abstract features\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(512, 768, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(768),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(768, 768, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(768),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.3),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))  # 8x4 -> 4x2\n",
        "        )\n",
        "\n",
        "        # Block 6: Final feature extraction\n",
        "        self.conv6 = nn.Sequential(\n",
        "            nn.Conv2d(768, 1024, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(1024, 1024, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.35),\n",
        "            nn.AdaptiveAvgPool2d((2, 1))  # Ensure consistent output: 2x1\n",
        "        )\n",
        "\n",
        "        # ==========================================\n",
        "        # LARGE BIDIRECTIONAL LSTM\n",
        "        # ==========================================\n",
        "        self.feature_size = 1024 * 2  # 1024 channels * 2x1 spatial\n",
        "        self.lstm_hidden = lstm_hidden\n",
        "\n",
        "        # Multi-layer LSTM with larger capacity\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.feature_size,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=4,  # Deeper LSTM\n",
        "            batch_first=True,\n",
        "            dropout=0.3,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Additional LSTM for temporal refinement\n",
        "        self.lstm_refine = nn.LSTM(\n",
        "            input_size=lstm_hidden * 2,\n",
        "            hidden_size=lstm_hidden // 2,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=0.2,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # ==========================================\n",
        "        # DEEP TRANSFORMER ENCODER\n",
        "        # ==========================================\n",
        "        self.transformer_dim = transformer_dim\n",
        "\n",
        "        # Project LSTM output to transformer dimension\n",
        "        self.lstm_to_transformer = nn.Sequential(\n",
        "            nn.Linear(lstm_hidden, transformer_dim),\n",
        "            nn.LayerNorm(transformer_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(transformer_dim, max_len=10000)\n",
        "\n",
        "        # Deep transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=transformer_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=transformer_dim * 4,  # Large feedforward\n",
        "            dropout=0.1,\n",
        "            activation='gelu',  # GELU activation for better performance\n",
        "            batch_first=True,\n",
        "            norm_first=True  # Pre-norm for better training stability\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers,\n",
        "            norm=nn.LayerNorm(transformer_dim)\n",
        "        )\n",
        "\n",
        "        # ==========================================\n",
        "        # MULTI-SCALE ATTENTION & FUSION\n",
        "        # ==========================================\n",
        "\n",
        "        # Multi-scale attention heads\n",
        "        self.global_attention = nn.MultiheadAttention(\n",
        "            embed_dim=transformer_dim,\n",
        "            num_heads=num_heads,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.local_attention = nn.MultiheadAttention(\n",
        "            embed_dim=transformer_dim,\n",
        "            num_heads=num_heads // 2,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Cross-attention between global and local features\n",
        "        self.cross_attention = nn.MultiheadAttention(\n",
        "            embed_dim=transformer_dim,\n",
        "            num_heads=num_heads // 2,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Feature fusion\n",
        "        self.feature_fusion = nn.Sequential(\n",
        "            nn.Linear(transformer_dim * 3, transformer_dim),\n",
        "            nn.LayerNorm(transformer_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        # ==========================================\n",
        "        # ADVANCED CLASSIFICATION HEAD\n",
        "        # ==========================================\n",
        "\n",
        "        # Hierarchical classification with multiple paths\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(transformer_dim),\n",
        "            nn.Linear(transformer_dim, 2048),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        # Auxiliary classifier for regularization\n",
        "        self.aux_classifier = nn.Sequential(\n",
        "            nn.Linear(lstm_hidden, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        print(\"✅ AGGRESSIVE CNN-LSTM-Transformer architecture built!\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # ==========================================\n",
        "        # DEEP CNN FEATURE EXTRACTION\n",
        "        # ==========================================\n",
        "        x = self.conv1(x)      # (batch, 64, 64, T/2)\n",
        "        x = self.conv2(x)      # (batch, 128, 32, T/4)\n",
        "        x = self.conv3(x)      # (batch, 256, 16, T/8)\n",
        "        x = self.conv4(x)      # (batch, 512, 8, T/16)\n",
        "        x = self.conv5(x)      # (batch, 768, 4, T/32)\n",
        "        x = self.conv6(x)      # (batch, 1024, 2, T/64)\n",
        "\n",
        "        # Reshape for LSTM: (batch, time_steps, features)\n",
        "        x = x.permute(0, 3, 1, 2)  # (batch, T/64, 1024, 2)\n",
        "        x = x.contiguous().view(batch_size, x.size(1), -1)  # (batch, T/64, 1024*2)\n",
        "\n",
        "        # ==========================================\n",
        "        # LARGE LSTM PROCESSING\n",
        "        # ==========================================\n",
        "        lstm_out, _ = self.lstm(x)  # (batch, T/64, lstm_hidden*2)\n",
        "        lstm_refined, _ = self.lstm_refine(lstm_out)  # (batch, T/64, lstm_hidden)\n",
        "\n",
        "        # Auxiliary classification from LSTM features (for regularization)\n",
        "        lstm_pooled = torch.mean(lstm_refined, dim=1)\n",
        "        aux_output = self.aux_classifier(lstm_pooled)\n",
        "\n",
        "        # ==========================================\n",
        "        # DEEP TRANSFORMER PROCESSING\n",
        "        # ==========================================\n",
        "\n",
        "        # Project to transformer dimension\n",
        "        transformer_input = self.lstm_to_transformer(lstm_refined)  # (batch, T/64, transformer_dim)\n",
        "\n",
        "        # Add positional encoding\n",
        "        transformer_input = transformer_input.transpose(0, 1)  # (T/64, batch, transformer_dim)\n",
        "        transformer_input = self.pos_encoding(transformer_input)\n",
        "        transformer_input = transformer_input.transpose(0, 1)  # (batch, T/64, transformer_dim)\n",
        "\n",
        "        # Deep transformer encoding\n",
        "        transformer_out = self.transformer_encoder(transformer_input)  # (batch, T/64, transformer_dim)\n",
        "\n",
        "        # ==========================================\n",
        "        # MULTI-SCALE ATTENTION & FUSION\n",
        "        # ==========================================\n",
        "\n",
        "        # Global attention (full sequence)\n",
        "        global_attended, _ = self.global_attention(\n",
        "            transformer_out, transformer_out, transformer_out\n",
        "        )\n",
        "\n",
        "        # Local attention (sliding window - simulate by chunking)\n",
        "        seq_len = transformer_out.size(1)\n",
        "        if seq_len > 16:\n",
        "            # Use overlapping windows\n",
        "            local_features = []\n",
        "            window_size = min(16, seq_len)\n",
        "            for i in range(0, max(1, seq_len - window_size + 1), window_size // 2):\n",
        "                end_idx = min(i + window_size, seq_len)\n",
        "                window = transformer_out[:, i:end_idx, :]\n",
        "                local_att, _ = self.local_attention(window, window, window)\n",
        "                local_features.append(torch.mean(local_att, dim=1, keepdim=True))\n",
        "            local_attended = torch.cat(local_features, dim=1)\n",
        "        else:\n",
        "            local_attended, _ = self.local_attention(\n",
        "                transformer_out, transformer_out, transformer_out\n",
        "            )\n",
        "\n",
        "        # Cross attention between global and local\n",
        "        cross_attended, _ = self.cross_attention(\n",
        "            global_attended, local_attended, local_attended\n",
        "        )\n",
        "\n",
        "        # Fusion of multi-scale features\n",
        "        # Pool to same size for concatenation\n",
        "        global_pooled = torch.mean(global_attended, dim=1)\n",
        "        local_pooled = torch.mean(local_attended, dim=1)\n",
        "        cross_pooled = torch.mean(cross_attended, dim=1)\n",
        "\n",
        "        fused_features = torch.cat([global_pooled, local_pooled, cross_pooled], dim=1)\n",
        "        final_features = self.feature_fusion(fused_features)\n",
        "\n",
        "        # ==========================================\n",
        "        # CLASSIFICATION\n",
        "        # ==========================================\n",
        "        main_output = self.classifier(final_features)\n",
        "\n",
        "        return main_output, aux_output\n",
        "\n",
        "# Create the aggressive model\n",
        "print(\"🚀 Creating AGGRESSIVE model for A100 40GB...\")\n",
        "aggressive_model = AggressiveCNN_LSTM_Transformer(\n",
        "    num_classes=4,\n",
        "    lstm_hidden=512,        # Doubled from 256\n",
        "    transformer_dim=1024,   # Doubled from 512\n",
        "    num_heads=16,          # Doubled from 8\n",
        "    num_layers=8           # Doubled from 4\n",
        ").to(device)\n",
        "\n",
        "# Test with dummy input\n",
        "test_input = torch.randn(4, 1, 128, 4500).to(device)  # Larger batch\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    main_out, aux_out = aggressive_model(test_input)\n",
        "    print(f\"Main output shape: {main_out.shape}\")\n",
        "    print(f\"Auxiliary output shape: {aux_out.shape}\")\n",
        "    print(f\"✅ Aggressive model forward pass successful!\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in aggressive_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in aggressive_model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n📊 AGGRESSIVE Model Statistics:\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n",
        "print(f\"Estimated GPU memory (training): ~{total_params * 16 / 1024 / 1024:.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "145287c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "145287c4",
        "outputId": "62f6a80a-cbca-4008-bbb8-5be3974d65dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting piece-tracked data loading...\n",
            "🎵 LOADING DATASET WITH PIECE TRACKING...\n",
            "Key improvements:\n",
            "• Track piece identity for each segment\n",
            "• Enable consecutive segment modeling\n",
            "• Support contrastive learning approaches\n",
            "• Segment duration: 45.0s\n",
            "\n",
            "--- Processing Bach ---\n",
            "  Found 131 MIDI files for Bach\n",
            "  Processed 50/131 files, created 394 segments...\n",
            "  Processed 100/131 files, created 715 segments...\n",
            "✅ Bach: 131/131 files → 977 segments\n",
            "\n",
            "--- Processing Beethoven ---\n",
            "  Found 134 MIDI files for Beethoven\n",
            "Error processing data/kaggle/midiclassics/Beethoven/Anhang 14-3.mid: Could not decode key with 3 flats and mode 255\n",
            "  Processed 50/134 files, created 357 segments...\n",
            "  Processed 100/134 files, created 765 segments...\n",
            "✅ Beethoven: 133/134 files → 987 segments\n",
            "\n",
            "--- Processing Chopin ---\n",
            "  Found 136 MIDI files for Chopin\n",
            "  Processed 50/136 files, created 218 segments...\n",
            "  Processed 100/136 files, created 449 segments...\n",
            "✅ Chopin: 136/136 files → 610 segments\n",
            "\n",
            "--- Processing Mozart ---\n",
            "  Found 90 MIDI files for Mozart\n",
            "  Processed 50/90 files, created 271 segments...\n",
            "✅ Mozart: 90/90 files → 524 segments\n",
            "\n",
            "🎯 FINAL PIECE-TRACKED DATASET:\n",
            "Total files processed: 490\n",
            "Total segments: 3098\n",
            "Total unique pieces: 490\n",
            "Data shape: (3098, 128, 4500)\n",
            "Label distribution: [977 987 610 524]\n",
            "Segments per piece - Mean: 6.3, Min: 1, Max: 30\n",
            "  Bach: 977 segments (31.5%)\n",
            "  Beethoven: 987 segments (31.9%)\n",
            "  Chopin: 610 segments (19.7%)\n",
            "  Mozart: 524 segments (16.9%)\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# SEQUENCE-AWARE DATA LOADING WITH PIECE TRACKING\n",
        "# =====================================================\n",
        "\n",
        "def load_dataset_with_piece_tracking(extract_path, target_composers, segment_duration=45.0):\n",
        "    \"\"\"\n",
        "    Load dataset while tracking which segments belong to the same piece\n",
        "    This enables sequence-aware training that distinguishes:\n",
        "    - Consecutive segments from SAME piece vs DIFFERENT pieces\n",
        "    - Same composer vs different composer relationships\n",
        "    \"\"\"\n",
        "    print(\"🎵 LOADING DATASET WITH PIECE TRACKING...\")\n",
        "    print(\"Key improvements:\")\n",
        "    print(f\"• Track piece identity for each segment\")\n",
        "    print(f\"• Enable consecutive segment modeling\")\n",
        "    print(f\"• Support contrastive learning approaches\")\n",
        "    print(f\"• Segment duration: {segment_duration}s\")\n",
        "\n",
        "    composer_to_idx = {c: i for i, c in enumerate(target_composers)}\n",
        "    all_data = []\n",
        "    all_labels = []\n",
        "    all_piece_ids = []  # NEW: Track which piece each segment comes from\n",
        "    all_piece_names = []  # NEW: Track piece names for analysis\n",
        "\n",
        "    piece_id_counter = 0\n",
        "    total_files_processed = 0\n",
        "    total_segments_created = 0\n",
        "\n",
        "    for composer in target_composers:\n",
        "        print(f\"\\n--- Processing {composer} ---\")\n",
        "        composer_dir = os.path.join(extract_path, composer)\n",
        "\n",
        "        if not os.path.isdir(composer_dir):\n",
        "            print(f\"Directory not found: {composer_dir}\")\n",
        "            continue\n",
        "\n",
        "        midi_files = [f for f in os.listdir(composer_dir)\n",
        "                     if f.lower().endswith(('.mid', '.midi'))]\n",
        "\n",
        "        print(f\"  Found {len(midi_files)} MIDI files for {composer}\")\n",
        "        files_processed = 0\n",
        "        segments_created = 0\n",
        "\n",
        "        for file in midi_files:\n",
        "            midi_path = os.path.join(composer_dir, file)\n",
        "\n",
        "            try:\n",
        "                # Use non-overlapping segmentation\n",
        "                segments = get_piano_roll_non_overlapping_segments(\n",
        "                    midi_path, segment_duration=segment_duration\n",
        "                )\n",
        "\n",
        "                if segments is None or len(segments) == 0:\n",
        "                    continue\n",
        "\n",
        "                # All segments from this file get the same piece_id\n",
        "                current_piece_id = piece_id_counter\n",
        "                piece_name = f\"{composer}_{file}\"\n",
        "                piece_id_counter += 1\n",
        "\n",
        "                valid_segments_from_piece = 0\n",
        "\n",
        "                for segment_idx, segment in enumerate(segments):\n",
        "                    # Normalize the segment\n",
        "                    normalized_segment = normalize_piano_roll(segment)\n",
        "\n",
        "                    # Extract musical features\n",
        "                    features = extract_musical_features(normalized_segment)\n",
        "\n",
        "                    # Quality check: skip if too sparse\n",
        "                    if features['avg_notes_per_time'] < 0.1:\n",
        "                        continue\n",
        "\n",
        "                    all_data.append(normalized_segment)\n",
        "                    all_labels.append(composer_to_idx[composer])\n",
        "                    all_piece_ids.append(current_piece_id)\n",
        "                    all_piece_names.append(f\"{piece_name}_seg{segment_idx}\")\n",
        "\n",
        "                    valid_segments_from_piece += 1\n",
        "                    segments_created += 1\n",
        "\n",
        "                if valid_segments_from_piece > 0:\n",
        "                    files_processed += 1\n",
        "\n",
        "                # Progress update\n",
        "                if files_processed % 50 == 0:\n",
        "                    print(f\"  Processed {files_processed}/{len(midi_files)} files, created {segments_created} segments...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error processing {file}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"✅ {composer}: {files_processed}/{len(midi_files)} files → {segments_created} segments\")\n",
        "        total_files_processed += files_processed\n",
        "        total_segments_created += segments_created\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    data = np.array(all_data)\n",
        "    labels = np.array(all_labels)\n",
        "    piece_ids = np.array(all_piece_ids)\n",
        "\n",
        "    print(f\"\\n🎯 FINAL PIECE-TRACKED DATASET:\")\n",
        "    print(f\"Total files processed: {total_files_processed}\")\n",
        "    print(f\"Total segments: {len(data)}\")\n",
        "    print(f\"Total unique pieces: {len(np.unique(piece_ids))}\")\n",
        "    print(f\"Data shape: {data.shape}\")\n",
        "    print(f\"Label distribution: {np.bincount(labels)}\")\n",
        "\n",
        "    # Analyze segments per piece\n",
        "    segments_per_piece = []\n",
        "    for piece_id in np.unique(piece_ids):\n",
        "        count = np.sum(piece_ids == piece_id)\n",
        "        segments_per_piece.append(count)\n",
        "\n",
        "    print(f\"Segments per piece - Mean: {np.mean(segments_per_piece):.1f}, \"\n",
        "          f\"Min: {np.min(segments_per_piece)}, Max: {np.max(segments_per_piece)}\")\n",
        "\n",
        "    # Show class distribution percentages\n",
        "    for i, composer in enumerate(target_composers):\n",
        "        count = np.sum(labels == i)\n",
        "        percentage = (count / len(labels)) * 100\n",
        "        print(f\"  {composer}: {count} segments ({percentage:.1f}%)\")\n",
        "\n",
        "    return data, labels, piece_ids, all_piece_names\n",
        "\n",
        "# Load the dataset with piece tracking\n",
        "print(\"🚀 Starting piece-tracked data loading...\")\n",
        "tracked_data, tracked_labels, tracked_piece_ids, piece_names = load_dataset_with_piece_tracking(\n",
        "    extract_path,\n",
        "    TARGET_COMPOSERS,\n",
        "    segment_duration=45.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3a85def4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a85def4",
        "outputId": "06735a95-4f81-41fb-9905-9df919109c5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚖️ COMPUTING CLASS WEIGHTS...\n",
            "Original distribution:\n",
            "  Bach: 977 samples (31.5%)\n",
            "  Beethoven: 987 samples (31.9%)\n",
            "  Chopin: 610 samples (19.7%)\n",
            "  Mozart: 524 samples (16.9%)\n",
            "\n",
            "Computed class weights:\n",
            "  Bach: 0.793\n",
            "  Beethoven: 0.785\n",
            "  Chopin: 1.270\n",
            "  Mozart: 1.478\n",
            "\n",
            "✅ Class weights ready for CrossEntropyLoss\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# CLASS WEIGHTS IMPLEMENTATION\n",
        "# =====================================================\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch.nn as nn\n",
        "\n",
        "# Compute class weights for our imbalanced dataset\n",
        "def compute_class_weights(labels, target_composers):\n",
        "    \"\"\"\n",
        "    Compute class weights to handle imbalance\n",
        "    \"\"\"\n",
        "    print(\"⚖️ COMPUTING CLASS WEIGHTS...\")\n",
        "\n",
        "    # Get unique labels and compute balanced weights\n",
        "    unique_labels = np.unique(labels)\n",
        "    class_weights = compute_class_weight(\n",
        "        'balanced',\n",
        "        classes=unique_labels,\n",
        "        y=labels\n",
        "    )\n",
        "\n",
        "    print(f\"Original distribution:\")\n",
        "    for i, composer in enumerate(target_composers):\n",
        "        count = np.sum(labels == i)\n",
        "        percentage = (count / len(labels)) * 100\n",
        "        print(f\"  {composer}: {count} samples ({percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nComputed class weights:\")\n",
        "    for i, (composer, weight) in enumerate(zip(target_composers, class_weights)):\n",
        "        print(f\"  {composer}: {weight:.3f}\")\n",
        "\n",
        "    # Convert to PyTorch tensor\n",
        "    class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
        "\n",
        "    print(f\"\\n✅ Class weights ready for CrossEntropyLoss\")\n",
        "    return class_weights_tensor\n",
        "\n",
        "# Compute class weights for our tracked dataset\n",
        "class_weights = compute_class_weights(tracked_labels, TARGET_COMPOSERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5673124f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5673124f",
        "outputId": "69d45c37-13e0-410a-8d78-f69282963981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔗 Creating sequence-aware dataset...\n",
            "📊 SEQUENCE DATASET CREATED:\n",
            "• Sequence length: 2\n",
            "• Total pieces: 490\n",
            "• Total sequences: 5706\n",
            "• Include single segments: True\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# SEQUENCE-AWARE DATASET FOR CONSECUTIVE SEGMENTS\n",
        "# =====================================================\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "class SequenceAwareDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset that creates sequences of consecutive segments from same pieces\n",
        "    This enables the model to learn temporal relationships within compositions\n",
        "    \"\"\"\n",
        "    def __init__(self, data, labels, piece_ids, sequence_length=2, include_singles=True):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "        self.piece_ids = torch.tensor(piece_ids, dtype=torch.long)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.include_singles = include_singles\n",
        "\n",
        "        # Group segments by piece\n",
        "        self.piece_segments = {}\n",
        "        for idx, piece_id in enumerate(piece_ids):\n",
        "            piece_id = int(piece_id)\n",
        "            if piece_id not in self.piece_segments:\n",
        "                self.piece_segments[piece_id] = []\n",
        "            self.piece_segments[piece_id].append(idx)\n",
        "\n",
        "        # Create sequence indices\n",
        "        self.sequence_indices = self._create_sequence_indices()\n",
        "\n",
        "        print(f\"📊 SEQUENCE DATASET CREATED:\")\n",
        "        print(f\"• Sequence length: {sequence_length}\")\n",
        "        print(f\"• Total pieces: {len(self.piece_segments)}\")\n",
        "        print(f\"• Total sequences: {len(self.sequence_indices)}\")\n",
        "        print(f\"• Include single segments: {include_singles}\")\n",
        "\n",
        "    def _create_sequence_indices(self):\n",
        "        \"\"\"Create indices for all possible sequences\"\"\"\n",
        "        sequences = []\n",
        "\n",
        "        # Add consecutive sequences from same pieces\n",
        "        for piece_id, segment_indices in self.piece_segments.items():\n",
        "            if len(segment_indices) >= self.sequence_length:\n",
        "                # Create all possible consecutive sequences from this piece\n",
        "                for start_idx in range(len(segment_indices) - self.sequence_length + 1):\n",
        "                    seq_indices = segment_indices[start_idx:start_idx + self.sequence_length]\n",
        "                    sequences.append({\n",
        "                        'type': 'sequence',\n",
        "                        'indices': seq_indices,\n",
        "                        'piece_id': piece_id,\n",
        "                        'label': int(self.labels[seq_indices[0]])  # All should have same label\n",
        "                    })\n",
        "\n",
        "        # Optionally add single segments\n",
        "        if self.include_singles:\n",
        "            for piece_id, segment_indices in self.piece_segments.items():\n",
        "                for idx in segment_indices:\n",
        "                    sequences.append({\n",
        "                        'type': 'single',\n",
        "                        'indices': [idx],\n",
        "                        'piece_id': piece_id,\n",
        "                        'label': int(self.labels[idx])\n",
        "                    })\n",
        "\n",
        "        return sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequence_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence_info = self.sequence_indices[idx]\n",
        "        indices = sequence_info['indices']\n",
        "\n",
        "        if len(indices) == 1:\n",
        "            # Single segment\n",
        "            segment = self.data[indices[0]].unsqueeze(0)  # Add channel dim\n",
        "            return segment, sequence_info['label'], sequence_info['piece_id'], 'single'\n",
        "        else:\n",
        "            # Multiple segments - stack them\n",
        "            segments = []\n",
        "            for seg_idx in indices:\n",
        "                segments.append(self.data[seg_idx].unsqueeze(0))  # Add channel dim\n",
        "\n",
        "            # Stack segments: (sequence_length, 1, 128, T)\n",
        "            sequence = torch.stack(segments, dim=0)\n",
        "\n",
        "            return sequence, sequence_info['label'], sequence_info['piece_id'], 'sequence'\n",
        "\n",
        "# Create sequence-aware dataset\n",
        "print(\"🔗 Creating sequence-aware dataset...\")\n",
        "sequence_dataset = SequenceAwareDataset(\n",
        "    tracked_data,\n",
        "    tracked_labels,\n",
        "    tracked_piece_ids,\n",
        "    sequence_length=2,  # Start with pairs\n",
        "    include_singles=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b7683e92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7683e92",
        "outputId": "7f4ee006-24d6-43f3-b39f-67eac5739fff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 CREATING BALANCED TRAINING SETUP...\n",
            "Using class weights: tensor([0.7927, 0.7847, 1.2697, 1.4781], device='cuda:0')\n",
            "📊 DATA SPLIT:\n",
            "Train pieces: 318 | segments: 1986\n",
            "Val pieces:   74 | segments: 438\n",
            "Test pieces:  98 | segments: 674\n",
            "\n",
            "Train distribution:\n",
            "  Bach: 606 (30.5%)\n",
            "  Beethoven: 600 (30.2%)\n",
            "  Chopin: 443 (22.3%)\n",
            "  Mozart: 337 (17.0%)\n",
            "\n",
            "Val distribution:\n",
            "  Bach: 154 (35.2%)\n",
            "  Beethoven: 160 (36.5%)\n",
            "  Chopin: 53 (12.1%)\n",
            "  Mozart: 71 (16.2%)\n",
            "\n",
            "Test distribution:\n",
            "  Bach: 217 (32.2%)\n",
            "  Beethoven: 227 (33.7%)\n",
            "  Chopin: 114 (16.9%)\n",
            "  Mozart: 116 (17.2%)\n",
            "\n",
            "✅ Weighted loss function created with class weights\n",
            "\n",
            "📚 DATA LOADERS CREATED:\n",
            "• Batch size: 16\n",
            "• Train batches: 125\n",
            "• Val batches: 28\n",
            "• Test batches: 43\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# TRAINING SETUP WITH CLASS WEIGHTS\n",
        "# =====================================================\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def create_balanced_training_setup(data, labels, piece_ids, class_weights, test_size=0.2, val_size=0.15):\n",
        "    \"\"\"\n",
        "    Create training setup with class weights for imbalance handling\n",
        "    \"\"\"\n",
        "    print(\"🎯 CREATING BALANCED TRAINING SETUP...\")\n",
        "    print(f\"Using class weights: {class_weights}\")\n",
        "\n",
        "    # Split data at the piece level to avoid data leakage\n",
        "    unique_pieces = np.unique(piece_ids)\n",
        "    piece_labels = np.array([labels[piece_ids == pid][0] for pid in unique_pieces])\n",
        "\n",
        "    # Split pieces into train/val/test\n",
        "    train_pieces, test_pieces, _, _ = train_test_split(\n",
        "        unique_pieces, piece_labels,\n",
        "        test_size=test_size,\n",
        "        stratify=piece_labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    train_pieces, val_pieces, _, _ = train_test_split(\n",
        "        train_pieces, piece_labels[np.isin(unique_pieces, train_pieces)],\n",
        "        test_size=val_size/(1-test_size),\n",
        "        stratify=piece_labels[np.isin(unique_pieces, train_pieces)],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Create masks for train/val/test\n",
        "    train_mask = np.isin(piece_ids, train_pieces)\n",
        "    val_mask = np.isin(piece_ids, val_pieces)\n",
        "    test_mask = np.isin(piece_ids, test_pieces)\n",
        "\n",
        "    print(f\"📊 DATA SPLIT:\")\n",
        "    print(f\"Train pieces: {len(train_pieces)} | segments: {np.sum(train_mask)}\")\n",
        "    print(f\"Val pieces:   {len(val_pieces)} | segments: {np.sum(val_mask)}\")\n",
        "    print(f\"Test pieces:  {len(test_pieces)} | segments: {np.sum(test_mask)}\")\n",
        "\n",
        "    # Show class distribution per split\n",
        "    for split_name, mask in [(\"Train\", train_mask), (\"Val\", val_mask), (\"Test\", test_mask)]:\n",
        "        split_labels = labels[mask]\n",
        "        print(f\"\\n{split_name} distribution:\")\n",
        "        for i, composer in enumerate(TARGET_COMPOSERS):\n",
        "            count = np.sum(split_labels == i)\n",
        "            percentage = (count / len(split_labels)) * 100 if len(split_labels) > 0 else 0\n",
        "            print(f\"  {composer}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = PianoRollDataset(data[train_mask], labels[train_mask])\n",
        "    val_dataset = PianoRollDataset(data[val_mask], labels[val_mask])\n",
        "    test_dataset = PianoRollDataset(data[test_mask], labels[test_mask])\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset, train_mask, val_mask, test_mask\n",
        "\n",
        "# Create the balanced training setup\n",
        "train_dataset, val_dataset, test_dataset, train_mask, val_mask, test_mask = create_balanced_training_setup(\n",
        "    tracked_data, tracked_labels, tracked_piece_ids, class_weights\n",
        ")\n",
        "\n",
        "# Create weighted loss function\n",
        "weighted_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "print(f\"\\n✅ Weighted loss function created with class weights\")\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 16  # Smaller batch size for stability\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"\\n📚 DATA LOADERS CREATED:\")\n",
        "print(f\"• Batch size: {batch_size}\")\n",
        "print(f\"• Train batches: {len(train_loader)}\")\n",
        "print(f\"• Val batches: {len(val_loader)}\")\n",
        "print(f\"• Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "73cd4b0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73cd4b0a",
        "outputId": "d9957402-908c-4729-f7f4-f895952a48f6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting training of AGGRESSIVE CNN-LSTM-Transformer...\n",
            "🚀 STARTING AGGRESSIVE MODEL TRAINING...\n",
            "• Model parameters: 185,688,776\n",
            "• Training samples: 1986\n",
            "• Validation samples: 438\n",
            "• Epochs: 50\n",
            "• Initial learning rate: 0.001\n",
            "• Using mixed precision: True\n",
            "• Class weights: tensor([0.7927, 0.7847, 1.2697, 1.4781], device='cuda:0')\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-57433144.py:48: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
            "/tmp/ipython-input-57433144.py:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🎯 Training Configuration:\n",
            "• Optimizer: AdamW (lr=0.001, weight_decay=1e-4)\n",
            "• Scheduler: CosineAnnealingWarmRestarts (T_0=10, T_mult=2)\n",
            "• Main loss weight: 0.7, Auxiliary loss weight: 0.3\n",
            "• Early stopping patience: 15\n",
            "• Mixed precision: CUDA\n",
            "\n",
            "📈 Epoch 1/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3886 | Acc: 18.75% | LR: 1.00e-03\n",
            "  Batch  20/125 | Loss: 1.3827 | Acc: 27.08% | LR: 1.00e-03\n",
            "  Batch  40/125 | Loss: 1.5159 | Acc: 26.83% | LR: 1.00e-03\n",
            "  Batch  60/125 | Loss: 1.3915 | Acc: 24.90% | LR: 1.00e-03\n",
            "  Batch  80/125 | Loss: 1.4364 | Acc: 24.46% | LR: 1.00e-03\n",
            "  Batch 100/125 | Loss: 1.4357 | Acc: 23.76% | LR: 1.00e-03\n",
            "  Batch 120/125 | Loss: 1.3804 | Acc: 24.23% | LR: 1.00e-03\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-57433144.py:146: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 1 Summary:\n",
            "  Train: Loss=1.3986, Acc=24.42%\n",
            "  Val:   Loss=1.4438, Acc=12.10%\n",
            "  LR: 9.76e-04, Time: 39.0s\n",
            "  💾 New best model saved! Val Acc: 12.10%\n",
            "\n",
            "📈 Epoch 2/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.4461 | Acc: 12.50% | LR: 9.76e-04\n",
            "  Batch  20/125 | Loss: 1.3940 | Acc: 17.86% | LR: 9.76e-04\n",
            "  Batch  40/125 | Loss: 1.4059 | Acc: 21.34% | LR: 9.76e-04\n",
            "  Batch  60/125 | Loss: 1.4010 | Acc: 23.36% | LR: 9.76e-04\n",
            "  Batch  80/125 | Loss: 1.4133 | Acc: 24.77% | LR: 9.76e-04\n",
            "  Batch 100/125 | Loss: 1.4068 | Acc: 23.45% | LR: 9.76e-04\n",
            "  Batch 120/125 | Loss: 1.3862 | Acc: 24.43% | LR: 9.76e-04\n",
            "\n",
            "📊 Epoch 2 Summary:\n",
            "  Train: Loss=1.3948, Acc=24.42%\n",
            "  Val:   Loss=1.4042, Acc=12.10%\n",
            "  LR: 9.05e-04, Time: 38.1s\n",
            "  ⏳ Patience: 1/15\n",
            "\n",
            "📈 Epoch 3/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3915 | Acc: 18.75% | LR: 9.05e-04\n",
            "  Batch  20/125 | Loss: 1.3860 | Acc: 21.13% | LR: 9.05e-04\n",
            "  Batch  40/125 | Loss: 1.4380 | Acc: 19.51% | LR: 9.05e-04\n",
            "  Batch  60/125 | Loss: 1.4294 | Acc: 19.36% | LR: 9.05e-04\n",
            "  Batch  80/125 | Loss: 1.3867 | Acc: 19.83% | LR: 9.05e-04\n",
            "  Batch 100/125 | Loss: 1.3613 | Acc: 20.98% | LR: 9.05e-04\n",
            "  Batch 120/125 | Loss: 1.3984 | Acc: 21.54% | LR: 9.05e-04\n",
            "\n",
            "📊 Epoch 3 Summary:\n",
            "  Train: Loss=1.3832, Acc=21.55%\n",
            "  Val:   Loss=1.4346, Acc=12.10%\n",
            "  LR: 7.94e-04, Time: 38.0s\n",
            "  ⏳ Patience: 2/15\n",
            "\n",
            "📈 Epoch 4/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3453 | Acc: 18.75% | LR: 7.94e-04\n",
            "  Batch  20/125 | Loss: 1.3322 | Acc: 24.40% | LR: 7.94e-04\n",
            "  Batch  40/125 | Loss: 1.5325 | Acc: 25.15% | LR: 7.94e-04\n",
            "  Batch  60/125 | Loss: 1.4615 | Acc: 24.49% | LR: 7.94e-04\n",
            "  Batch  80/125 | Loss: 1.3683 | Acc: 23.38% | LR: 7.94e-04\n",
            "  Batch 100/125 | Loss: 1.3766 | Acc: 23.33% | LR: 7.94e-04\n",
            "  Batch 120/125 | Loss: 1.3336 | Acc: 22.73% | LR: 7.94e-04\n",
            "\n",
            "📊 Epoch 4 Summary:\n",
            "  Train: Loss=1.3484, Acc=22.71%\n",
            "  Val:   Loss=1.3268, Acc=12.10%\n",
            "  LR: 6.55e-04, Time: 38.1s\n",
            "  ⏳ Patience: 3/15\n",
            "\n",
            "📈 Epoch 5/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.2910 | Acc: 18.75% | LR: 6.55e-04\n",
            "  Batch  20/125 | Loss: 1.3038 | Acc: 23.21% | LR: 6.55e-04\n",
            "  Batch  40/125 | Loss: 1.2651 | Acc: 23.17% | LR: 6.55e-04\n",
            "  Batch  60/125 | Loss: 1.3096 | Acc: 23.05% | LR: 6.55e-04\n",
            "  Batch  80/125 | Loss: 1.3004 | Acc: 23.23% | LR: 6.55e-04\n",
            "  Batch 100/125 | Loss: 1.3228 | Acc: 23.02% | LR: 6.55e-04\n",
            "  Batch 120/125 | Loss: 1.2730 | Acc: 22.93% | LR: 6.55e-04\n",
            "\n",
            "📊 Epoch 5 Summary:\n",
            "  Train: Loss=1.3221, Acc=23.01%\n",
            "  Val:   Loss=1.3322, Acc=12.10%\n",
            "  LR: 5.01e-04, Time: 38.1s\n",
            "  ⏳ Patience: 4/15\n",
            "\n",
            "📈 Epoch 6/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.2743 | Acc: 37.50% | LR: 5.01e-04\n",
            "  Batch  20/125 | Loss: 1.2434 | Acc: 25.00% | LR: 5.01e-04\n",
            "  Batch  40/125 | Loss: 1.2771 | Acc: 22.87% | LR: 5.01e-04\n",
            "  Batch  60/125 | Loss: 1.3203 | Acc: 22.85% | LR: 5.01e-04\n",
            "  Batch  80/125 | Loss: 1.3514 | Acc: 22.61% | LR: 5.01e-04\n",
            "  Batch 100/125 | Loss: 1.3852 | Acc: 22.90% | LR: 5.01e-04\n",
            "  Batch 120/125 | Loss: 1.3759 | Acc: 22.68% | LR: 5.01e-04\n",
            "\n",
            "📊 Epoch 6 Summary:\n",
            "  Train: Loss=1.3081, Acc=22.81%\n",
            "  Val:   Loss=1.3103, Acc=16.21%\n",
            "  LR: 3.46e-04, Time: 38.1s\n",
            "  💾 New best model saved! Val Acc: 16.21%\n",
            "\n",
            "📈 Epoch 7/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3292 | Acc: 18.75% | LR: 3.46e-04\n",
            "  Batch  20/125 | Loss: 1.4187 | Acc: 22.02% | LR: 3.46e-04\n",
            "  Batch  40/125 | Loss: 1.3172 | Acc: 20.58% | LR: 3.46e-04\n",
            "  Batch  60/125 | Loss: 1.2988 | Acc: 21.31% | LR: 3.46e-04\n",
            "  Batch  80/125 | Loss: 1.1974 | Acc: 21.84% | LR: 3.46e-04\n",
            "  Batch 100/125 | Loss: 1.3932 | Acc: 22.59% | LR: 3.46e-04\n",
            "  Batch 120/125 | Loss: 1.2790 | Acc: 22.37% | LR: 3.46e-04\n",
            "\n",
            "📊 Epoch 7 Summary:\n",
            "  Train: Loss=1.2945, Acc=22.26%\n",
            "  Val:   Loss=1.3339, Acc=12.10%\n",
            "  LR: 2.07e-04, Time: 38.2s\n",
            "  ⏳ Patience: 1/15\n",
            "\n",
            "📈 Epoch 8/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3298 | Acc: 25.00% | LR: 2.07e-04\n",
            "  Batch  20/125 | Loss: 1.3278 | Acc: 26.79% | LR: 2.07e-04\n",
            "  Batch  40/125 | Loss: 1.2529 | Acc: 25.76% | LR: 2.07e-04\n",
            "  Batch  60/125 | Loss: 1.3816 | Acc: 24.08% | LR: 2.07e-04\n",
            "  Batch  80/125 | Loss: 1.2039 | Acc: 22.92% | LR: 2.07e-04\n",
            "  Batch 100/125 | Loss: 1.3266 | Acc: 23.08% | LR: 2.07e-04\n",
            "  Batch 120/125 | Loss: 1.2431 | Acc: 22.68% | LR: 2.07e-04\n",
            "\n",
            "📊 Epoch 8 Summary:\n",
            "  Train: Loss=1.2854, Acc=22.66%\n",
            "  Val:   Loss=1.2994, Acc=12.10%\n",
            "  LR: 9.64e-05, Time: 38.2s\n",
            "  ⏳ Patience: 2/15\n",
            "\n",
            "📈 Epoch 9/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.2234 | Acc: 31.25% | LR: 9.64e-05\n",
            "  Batch  20/125 | Loss: 1.2597 | Acc: 22.02% | LR: 9.64e-05\n",
            "  Batch  40/125 | Loss: 1.2327 | Acc: 23.02% | LR: 9.64e-05\n",
            "  Batch  60/125 | Loss: 1.2432 | Acc: 22.13% | LR: 9.64e-05\n",
            "  Batch  80/125 | Loss: 1.2711 | Acc: 20.91% | LR: 9.64e-05\n",
            "  Batch 100/125 | Loss: 1.2754 | Acc: 21.84% | LR: 9.64e-05\n",
            "  Batch 120/125 | Loss: 1.2354 | Acc: 21.80% | LR: 9.64e-05\n",
            "\n",
            "📊 Epoch 9 Summary:\n",
            "  Train: Loss=1.2702, Acc=22.00%\n",
            "  Val:   Loss=1.2946, Acc=12.10%\n",
            "  LR: 2.54e-05, Time: 38.0s\n",
            "  ⏳ Patience: 3/15\n",
            "\n",
            "📈 Epoch 10/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.2819 | Acc: 31.25% | LR: 2.54e-05\n",
            "  Batch  20/125 | Loss: 1.2224 | Acc: 19.94% | LR: 2.54e-05\n",
            "  Batch  40/125 | Loss: 1.3545 | Acc: 20.73% | LR: 2.54e-05\n",
            "  Batch  60/125 | Loss: 1.2227 | Acc: 21.72% | LR: 2.54e-05\n",
            "  Batch  80/125 | Loss: 1.3013 | Acc: 23.15% | LR: 2.54e-05\n",
            "  Batch 100/125 | Loss: 1.3474 | Acc: 22.83% | LR: 2.54e-05\n",
            "  Batch 120/125 | Loss: 1.2982 | Acc: 22.78% | LR: 2.54e-05\n",
            "\n",
            "📊 Epoch 10 Summary:\n",
            "  Train: Loss=1.2665, Acc=22.71%\n",
            "  Val:   Loss=1.3001, Acc=12.10%\n",
            "  LR: 1.00e-03, Time: 38.1s\n",
            "  ⏳ Patience: 4/15\n",
            "\n",
            "📈 Epoch 11/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.1454 | Acc: 37.50% | LR: 1.00e-03\n",
            "  Batch  20/125 | Loss: 1.3167 | Acc: 25.60% | LR: 1.00e-03\n",
            "  Batch  40/125 | Loss: 1.4004 | Acc: 23.48% | LR: 1.00e-03\n",
            "  Batch  60/125 | Loss: 1.3154 | Acc: 22.75% | LR: 1.00e-03\n",
            "  Batch  80/125 | Loss: 1.2436 | Acc: 24.23% | LR: 1.00e-03\n",
            "  Batch 100/125 | Loss: 1.3864 | Acc: 23.70% | LR: 1.00e-03\n",
            "  Batch 120/125 | Loss: 1.3230 | Acc: 23.71% | LR: 1.00e-03\n",
            "\n",
            "📊 Epoch 11 Summary:\n",
            "  Train: Loss=1.3085, Acc=23.67%\n",
            "  Val:   Loss=1.3237, Acc=12.10%\n",
            "  LR: 9.94e-04, Time: 38.0s\n",
            "  ⏳ Patience: 5/15\n",
            "\n",
            "📈 Epoch 12/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3342 | Acc: 37.50% | LR: 9.94e-04\n",
            "  Batch  20/125 | Loss: 1.4060 | Acc: 21.43% | LR: 9.94e-04\n",
            "  Batch  40/125 | Loss: 1.2438 | Acc: 19.82% | LR: 9.94e-04\n",
            "  Batch  60/125 | Loss: 1.3037 | Acc: 20.59% | LR: 9.94e-04\n",
            "  Batch  80/125 | Loss: 1.2885 | Acc: 21.22% | LR: 9.94e-04\n",
            "  Batch 100/125 | Loss: 1.2291 | Acc: 21.53% | LR: 9.94e-04\n",
            "  Batch 120/125 | Loss: nan | Acc: 21.95% | LR: 9.94e-04\n",
            "\n",
            "📊 Epoch 12 Summary:\n",
            "  Train: Loss=nan, Acc=21.90%\n",
            "  Val:   Loss=1.3239, Acc=12.10%\n",
            "  LR: 9.76e-04, Time: 38.0s\n",
            "  ⏳ Patience: 6/15\n",
            "\n",
            "📈 Epoch 13/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: nan | Acc: 25.00% | LR: 9.76e-04\n",
            "  Batch  20/125 | Loss: 1.2269 | Acc: 22.02% | LR: 9.76e-04\n",
            "  Batch  40/125 | Loss: 1.2875 | Acc: 21.34% | LR: 9.76e-04\n",
            "  Batch  60/125 | Loss: 1.4087 | Acc: 22.85% | LR: 9.76e-04\n",
            "  Batch  80/125 | Loss: 1.2009 | Acc: 23.38% | LR: 9.76e-04\n",
            "  Batch 100/125 | Loss: 1.2494 | Acc: 23.70% | LR: 9.76e-04\n",
            "  Batch 120/125 | Loss: 1.1954 | Acc: 23.19% | LR: 9.76e-04\n",
            "\n",
            "📊 Epoch 13 Summary:\n",
            "  Train: Loss=nan, Acc=23.01%\n",
            "  Val:   Loss=1.3223, Acc=12.10%\n",
            "  LR: 9.46e-04, Time: 37.9s\n",
            "  ⏳ Patience: 7/15\n",
            "\n",
            "📈 Epoch 14/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.2513 | Acc: 25.00% | LR: 9.46e-04\n",
            "  Batch  20/125 | Loss: 1.3290 | Acc: 22.32% | LR: 9.46e-04\n",
            "  Batch  40/125 | Loss: 1.3160 | Acc: 20.43% | LR: 9.46e-04\n",
            "  Batch  60/125 | Loss: 1.2114 | Acc: 19.77% | LR: 9.46e-04\n",
            "  Batch  80/125 | Loss: 1.3494 | Acc: 19.44% | LR: 9.46e-04\n",
            "  Batch 100/125 | Loss: 1.2605 | Acc: 19.99% | LR: 9.46e-04\n",
            "  Batch 120/125 | Loss: 1.2267 | Acc: 19.78% | LR: 9.46e-04\n",
            "\n",
            "📊 Epoch 14 Summary:\n",
            "  Train: Loss=1.2766, Acc=19.69%\n",
            "  Val:   Loss=1.2958, Acc=36.53%\n",
            "  LR: 9.05e-04, Time: 38.1s\n",
            "  💾 New best model saved! Val Acc: 36.53%\n",
            "\n",
            "📈 Epoch 15/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3033 | Acc: 18.75% | LR: 9.05e-04\n",
            "  Batch  20/125 | Loss: 1.3576 | Acc: 18.75% | LR: 9.05e-04\n",
            "  Batch  40/125 | Loss: 1.2352 | Acc: 19.97% | LR: 9.05e-04\n",
            "  Batch  60/125 | Loss: 1.1912 | Acc: 20.18% | LR: 9.05e-04\n",
            "  Batch  80/125 | Loss: 1.3019 | Acc: 20.14% | LR: 9.05e-04\n",
            "  Batch 100/125 | Loss: 1.2939 | Acc: 20.92% | LR: 9.05e-04\n",
            "  Batch 120/125 | Loss: 1.3225 | Acc: 21.59% | LR: 9.05e-04\n",
            "\n",
            "📊 Epoch 15 Summary:\n",
            "  Train: Loss=1.2779, Acc=21.45%\n",
            "  Val:   Loss=1.2981, Acc=12.10%\n",
            "  LR: 8.54e-04, Time: 38.1s\n",
            "  ⏳ Patience: 1/15\n",
            "\n",
            "📈 Epoch 16/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.1995 | Acc: 12.50% | LR: 8.54e-04\n",
            "  Batch  20/125 | Loss: 1.2873 | Acc: 17.56% | LR: 8.54e-04\n",
            "  Batch  40/125 | Loss: 1.2823 | Acc: 21.95% | LR: 8.54e-04\n",
            "  Batch  60/125 | Loss: 1.2678 | Acc: 22.44% | LR: 8.54e-04\n",
            "  Batch  80/125 | Loss: 1.2370 | Acc: 22.69% | LR: 8.54e-04\n",
            "  Batch 100/125 | Loss: 1.3633 | Acc: 22.34% | LR: 8.54e-04\n",
            "  Batch 120/125 | Loss: 1.2949 | Acc: 22.21% | LR: 8.54e-04\n",
            "\n",
            "📊 Epoch 16 Summary:\n",
            "  Train: Loss=1.2805, Acc=22.26%\n",
            "  Val:   Loss=1.3003, Acc=12.10%\n",
            "  LR: 7.94e-04, Time: 38.1s\n",
            "  ⏳ Patience: 2/15\n",
            "\n",
            "📈 Epoch 17/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3940 | Acc: 31.25% | LR: 7.94e-04\n",
            "  Batch  20/125 | Loss: 1.3201 | Acc: 24.70% | LR: 7.94e-04\n",
            "  Batch  40/125 | Loss: 1.2563 | Acc: 23.78% | LR: 7.94e-04\n",
            "  Batch  60/125 | Loss: 1.3209 | Acc: 25.92% | LR: 7.94e-04\n",
            "  Batch  80/125 | Loss: 1.2059 | Acc: 24.77% | LR: 7.94e-04\n",
            "  Batch 100/125 | Loss: 1.3667 | Acc: 24.32% | LR: 7.94e-04\n",
            "  Batch 120/125 | Loss: 1.3404 | Acc: 24.02% | LR: 7.94e-04\n",
            "\n",
            "📊 Epoch 17 Summary:\n",
            "  Train: Loss=1.2782, Acc=24.02%\n",
            "  Val:   Loss=1.3053, Acc=12.10%\n",
            "  LR: 7.27e-04, Time: 38.1s\n",
            "  ⏳ Patience: 3/15\n",
            "\n",
            "📈 Epoch 18/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.1979 | Acc: 18.75% | LR: 7.27e-04\n",
            "  Batch  20/125 | Loss: 1.3134 | Acc: 21.43% | LR: 7.27e-04\n",
            "  Batch  40/125 | Loss: 1.3704 | Acc: 20.73% | LR: 7.27e-04\n",
            "  Batch  60/125 | Loss: 1.2945 | Acc: 21.82% | LR: 7.27e-04\n",
            "  Batch  80/125 | Loss: 1.3025 | Acc: 21.76% | LR: 7.27e-04\n",
            "  Batch 100/125 | Loss: 1.2310 | Acc: 22.22% | LR: 7.27e-04\n",
            "  Batch 120/125 | Loss: 1.2488 | Acc: 22.57% | LR: 7.27e-04\n",
            "\n",
            "📊 Epoch 18 Summary:\n",
            "  Train: Loss=1.2695, Acc=22.36%\n",
            "  Val:   Loss=1.3033, Acc=12.10%\n",
            "  LR: 6.55e-04, Time: 38.1s\n",
            "  ⏳ Patience: 4/15\n",
            "\n",
            "📈 Epoch 19/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.2328 | Acc: 25.00% | LR: 6.55e-04\n",
            "  Batch  20/125 | Loss: 1.2270 | Acc: 18.45% | LR: 6.55e-04\n",
            "  Batch  40/125 | Loss: 1.2101 | Acc: 20.27% | LR: 6.55e-04\n",
            "  Batch  60/125 | Loss: 1.2442 | Acc: 20.90% | LR: 6.55e-04\n",
            "  Batch  80/125 | Loss: 1.2317 | Acc: 22.07% | LR: 6.55e-04\n",
            "  Batch 100/125 | Loss: 1.2103 | Acc: 22.09% | LR: 6.55e-04\n",
            "  Batch 120/125 | Loss: 1.3197 | Acc: 22.21% | LR: 6.55e-04\n",
            "\n",
            "📊 Epoch 19 Summary:\n",
            "  Train: Loss=1.2694, Acc=22.31%\n",
            "  Val:   Loss=1.2962, Acc=12.10%\n",
            "  LR: 5.79e-04, Time: 38.1s\n",
            "  ⏳ Patience: 5/15\n",
            "\n",
            "📈 Epoch 20/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.2733 | Acc: 25.00% | LR: 5.79e-04\n",
            "  Batch  20/125 | Loss: 1.2562 | Acc: 21.13% | LR: 5.79e-04\n",
            "  Batch  40/125 | Loss: 1.2400 | Acc: 21.65% | LR: 5.79e-04\n",
            "  Batch  60/125 | Loss: 1.3006 | Acc: 22.34% | LR: 5.79e-04\n",
            "  Batch  80/125 | Loss: 1.2758 | Acc: 21.68% | LR: 5.79e-04\n",
            "  Batch 100/125 | Loss: 1.2733 | Acc: 21.23% | LR: 5.79e-04\n",
            "  Batch 120/125 | Loss: 1.3176 | Acc: 21.90% | LR: 5.79e-04\n",
            "\n",
            "📊 Epoch 20 Summary:\n",
            "  Train: Loss=1.2667, Acc=22.00%\n",
            "  Val:   Loss=1.3112, Acc=12.10%\n",
            "  LR: 5.01e-04, Time: 38.2s\n",
            "  ⏳ Patience: 6/15\n",
            "\n",
            "📈 Epoch 21/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.2518 | Acc: 6.25% | LR: 5.01e-04\n",
            "  Batch  20/125 | Loss: 1.1829 | Acc: 25.00% | LR: 5.01e-04\n",
            "  Batch  40/125 | Loss: 1.2669 | Acc: 22.71% | LR: 5.01e-04\n",
            "  Batch  60/125 | Loss: 1.3006 | Acc: 22.23% | LR: 5.01e-04\n",
            "  Batch  80/125 | Loss: 1.2331 | Acc: 22.99% | LR: 5.01e-04\n",
            "  Batch 100/125 | Loss: 1.3736 | Acc: 22.34% | LR: 5.01e-04\n",
            "  Batch 120/125 | Loss: 1.3872 | Acc: 22.16% | LR: 5.01e-04\n",
            "\n",
            "📊 Epoch 21 Summary:\n",
            "  Train: Loss=1.2664, Acc=22.26%\n",
            "  Val:   Loss=1.4806, Acc=12.10%\n",
            "  LR: 4.22e-04, Time: 38.2s\n",
            "  ⏳ Patience: 7/15\n",
            "\n",
            "📈 Epoch 22/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.2420 | Acc: 18.75% | LR: 4.22e-04\n",
            "  Batch  20/125 | Loss: 1.3416 | Acc: 24.70% | LR: 4.22e-04\n",
            "  Batch  40/125 | Loss: 1.3075 | Acc: 23.48% | LR: 4.22e-04\n",
            "  Batch  60/125 | Loss: 1.3513 | Acc: 23.16% | LR: 4.22e-04\n",
            "  Batch  80/125 | Loss: 1.2741 | Acc: 22.92% | LR: 4.22e-04\n",
            "  Batch 100/125 | Loss: 1.2960 | Acc: 22.52% | LR: 4.22e-04\n",
            "  Batch 120/125 | Loss: 1.3133 | Acc: 22.31% | LR: 4.22e-04\n",
            "\n",
            "📊 Epoch 22 Summary:\n",
            "  Train: Loss=1.3122, Acc=22.31%\n",
            "  Val:   Loss=1.3039, Acc=12.10%\n",
            "  LR: 3.46e-04, Time: 38.1s\n",
            "  ⏳ Patience: 8/15\n",
            "\n",
            "📈 Epoch 23/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3797 | Acc: 6.25% | LR: 3.46e-04\n",
            "  Batch  20/125 | Loss: 1.3168 | Acc: 19.94% | LR: 3.46e-04\n",
            "  Batch  40/125 | Loss: 1.3680 | Acc: 21.65% | LR: 3.46e-04\n",
            "  Batch  60/125 | Loss: 1.2782 | Acc: 22.13% | LR: 3.46e-04\n",
            "  Batch  80/125 | Loss: 1.2981 | Acc: 22.45% | LR: 3.46e-04\n",
            "  Batch 100/125 | Loss: 1.3184 | Acc: 22.09% | LR: 3.46e-04\n",
            "  Batch 120/125 | Loss: 1.3263 | Acc: 22.42% | LR: 3.46e-04\n",
            "\n",
            "📊 Epoch 23 Summary:\n",
            "  Train: Loss=1.2957, Acc=22.31%\n",
            "  Val:   Loss=1.2896, Acc=12.10%\n",
            "  LR: 2.74e-04, Time: 38.2s\n",
            "  ⏳ Patience: 9/15\n",
            "\n",
            "📈 Epoch 24/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.2703 | Acc: 18.75% | LR: 2.74e-04\n",
            "  Batch  20/125 | Loss: 1.1905 | Acc: 22.02% | LR: 2.74e-04\n",
            "  Batch  40/125 | Loss: 1.3839 | Acc: 22.56% | LR: 2.74e-04\n",
            "  Batch  60/125 | Loss: 1.3299 | Acc: 22.75% | LR: 2.74e-04\n",
            "  Batch  80/125 | Loss: 1.3883 | Acc: 22.07% | LR: 2.74e-04\n",
            "  Batch 100/125 | Loss: 1.3082 | Acc: 21.47% | LR: 2.74e-04\n",
            "  Batch 120/125 | Loss: 1.2301 | Acc: 22.37% | LR: 2.74e-04\n",
            "\n",
            "📊 Epoch 24 Summary:\n",
            "  Train: Loss=1.2898, Acc=22.31%\n",
            "  Val:   Loss=1.2897, Acc=12.10%\n",
            "  LR: 2.07e-04, Time: 38.1s\n",
            "  ⏳ Patience: 10/15\n",
            "\n",
            "📈 Epoch 25/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.2680 | Acc: 31.25% | LR: 2.07e-04\n",
            "  Batch  20/125 | Loss: 1.2754 | Acc: 22.92% | LR: 2.07e-04\n",
            "  Batch  40/125 | Loss: 1.3091 | Acc: 20.88% | LR: 2.07e-04\n",
            "  Batch  60/125 | Loss: 1.2114 | Acc: 21.00% | LR: 2.07e-04\n",
            "  Batch  80/125 | Loss: 1.2531 | Acc: 21.14% | LR: 2.07e-04\n",
            "  Batch 100/125 | Loss: 1.2459 | Acc: 21.84% | LR: 2.07e-04\n",
            "  Batch 120/125 | Loss: 1.3125 | Acc: 22.37% | LR: 2.07e-04\n",
            "\n",
            "📊 Epoch 25 Summary:\n",
            "  Train: Loss=1.2832, Acc=22.31%\n",
            "  Val:   Loss=1.3090, Acc=12.10%\n",
            "  LR: 1.47e-04, Time: 38.1s\n",
            "  ⏳ Patience: 11/15\n",
            "\n",
            "📈 Epoch 26/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3381 | Acc: 37.50% | LR: 1.47e-04\n",
            "  Batch  20/125 | Loss: 1.3203 | Acc: 23.51% | LR: 1.47e-04\n",
            "  Batch  40/125 | Loss: 1.2932 | Acc: 22.71% | LR: 1.47e-04\n",
            "  Batch  60/125 | Loss: 1.3304 | Acc: 23.87% | LR: 1.47e-04\n",
            "  Batch  80/125 | Loss: 1.4102 | Acc: 23.23% | LR: 1.47e-04\n",
            "  Batch 100/125 | Loss: 1.2654 | Acc: 22.71% | LR: 1.47e-04\n",
            "  Batch 120/125 | Loss: 1.2733 | Acc: 22.06% | LR: 1.47e-04\n",
            "\n",
            "📊 Epoch 26 Summary:\n",
            "  Train: Loss=1.2822, Acc=22.31%\n",
            "  Val:   Loss=1.2819, Acc=12.10%\n",
            "  LR: 9.64e-05, Time: 38.1s\n",
            "  ⏳ Patience: 12/15\n",
            "\n",
            "📈 Epoch 27/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3104 | Acc: 25.00% | LR: 9.64e-05\n",
            "  Batch  20/125 | Loss: 1.2357 | Acc: 17.56% | LR: 9.64e-05\n",
            "  Batch  40/125 | Loss: 1.2433 | Acc: 17.68% | LR: 9.64e-05\n",
            "  Batch  60/125 | Loss: 1.3223 | Acc: 20.39% | LR: 9.64e-05\n",
            "  Batch  80/125 | Loss: 1.2588 | Acc: 22.38% | LR: 9.64e-05\n",
            "  Batch 100/125 | Loss: 1.2422 | Acc: 22.22% | LR: 9.64e-05\n",
            "  Batch 120/125 | Loss: 1.2498 | Acc: 22.47% | LR: 9.64e-05\n",
            "\n",
            "📊 Epoch 27 Summary:\n",
            "  Train: Loss=1.2796, Acc=22.31%\n",
            "  Val:   Loss=1.2928, Acc=12.10%\n",
            "  LR: 5.54e-05, Time: 38.1s\n",
            "  ⏳ Patience: 13/15\n",
            "\n",
            "📈 Epoch 28/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.2974 | Acc: 18.75% | LR: 5.54e-05\n",
            "  Batch  20/125 | Loss: 1.3739 | Acc: 25.30% | LR: 5.54e-05\n",
            "  Batch  40/125 | Loss: 1.3074 | Acc: 24.09% | LR: 5.54e-05\n",
            "  Batch  60/125 | Loss: 1.2807 | Acc: 23.77% | LR: 5.54e-05\n",
            "  Batch  80/125 | Loss: 1.3180 | Acc: 22.99% | LR: 5.54e-05\n",
            "  Batch 100/125 | Loss: 1.2681 | Acc: 22.40% | LR: 5.54e-05\n",
            "  Batch 120/125 | Loss: 1.2293 | Acc: 22.47% | LR: 5.54e-05\n",
            "\n",
            "📊 Epoch 28 Summary:\n",
            "  Train: Loss=1.2813, Acc=22.31%\n",
            "  Val:   Loss=1.2848, Acc=12.10%\n",
            "  LR: 2.54e-05, Time: 38.1s\n",
            "  ⏳ Patience: 14/15\n",
            "\n",
            "📈 Epoch 29/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.2149 | Acc: 37.50% | LR: 2.54e-05\n",
            "  Batch  20/125 | Loss: 1.2358 | Acc: 24.11% | LR: 2.54e-05\n",
            "  Batch  40/125 | Loss: 1.3500 | Acc: 21.95% | LR: 2.54e-05\n",
            "  Batch  60/125 | Loss: 1.2271 | Acc: 21.41% | LR: 2.54e-05\n",
            "  Batch  80/125 | Loss: 1.1831 | Acc: 20.83% | LR: 2.54e-05\n",
            "  Batch 100/125 | Loss: 1.3312 | Acc: 21.78% | LR: 2.54e-05\n",
            "  Batch 120/125 | Loss: 1.2595 | Acc: 22.47% | LR: 2.54e-05\n",
            "\n",
            "📊 Epoch 29 Summary:\n",
            "  Train: Loss=1.2754, Acc=22.31%\n",
            "  Val:   Loss=1.2942, Acc=12.10%\n",
            "  LR: 7.15e-06, Time: 38.2s\n",
            "  ⏳ Patience: 15/15\n",
            "\n",
            "🛑 Early stopping triggered! Best Val Acc: 36.53%\n",
            "\n",
            "✅ Training completed!\n",
            "Best validation accuracy: 36.53%\n",
            "Model saved to: saved_models/aggressive_cnn_lstm_transformer.pth\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# ADVANCED TRAINING LOOP FOR AGGRESSIVE MODEL\n",
        "# =====================================================\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_aggressive_model(model, train_loader, val_loader, class_weights,\n",
        "                          epochs=50, initial_lr=1e-3, save_path='aggressive_model.pth'):\n",
        "    \"\"\"\n",
        "    Advanced training with:\n",
        "    - Auxiliary loss for regularization\n",
        "    - Cosine annealing with warm restarts\n",
        "    - Mixed precision training\n",
        "    - Advanced metrics tracking\n",
        "    - Early stopping with patience\n",
        "    \"\"\"\n",
        "    print(\"🚀 STARTING AGGRESSIVE MODEL TRAINING...\")\n",
        "    print(f\"• Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"• Training samples: {len(train_loader.dataset)}\")\n",
        "    print(f\"• Validation samples: {len(val_loader.dataset)}\")\n",
        "    print(f\"• Epochs: {epochs}\")\n",
        "    print(f\"• Initial learning rate: {initial_lr}\")\n",
        "    print(f\"• Using mixed precision: True\")\n",
        "    print(f\"• Class weights: {class_weights}\")\n",
        "\n",
        "    # Setup optimizers and schedulers\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=initial_lr,\n",
        "        weight_decay=1e-4,\n",
        "        betas=(0.9, 0.999),\n",
        "        eps=1e-8\n",
        "    )\n",
        "\n",
        "    # Cosine annealing with warm restarts\n",
        "    scheduler = CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=10,  # Restart every 10 epochs\n",
        "        T_mult=2,  # Double the period after each restart\n",
        "        eta_min=1e-6\n",
        "    )\n",
        "\n",
        "    # Mixed precision scaler\n",
        "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
        "\n",
        "    # Loss functions\n",
        "    main_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    aux_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    # Training tracking\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [], 'train_main_loss': [], 'train_aux_loss': [],\n",
        "        'val_loss': [], 'val_acc': [], 'val_main_loss': [], 'val_aux_loss': [],\n",
        "        'lr': [], 'epoch_time': []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    patience = 15  # Early stopping patience\n",
        "\n",
        "    print(\"\\n🎯 Training Configuration:\")\n",
        "    print(f\"• Optimizer: AdamW (lr={initial_lr}, weight_decay=1e-4)\")\n",
        "    print(f\"• Scheduler: CosineAnnealingWarmRestarts (T_0=10, T_mult=2)\")\n",
        "    print(f\"• Main loss weight: 0.7, Auxiliary loss weight: 0.3\")\n",
        "    print(f\"• Early stopping patience: {patience}\")\n",
        "    print(f\"• Mixed precision: {'CUDA' if scaler else 'Disabled'}\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        # ==========================================\n",
        "        # TRAINING PHASE\n",
        "        # ==========================================\n",
        "        model.train()\n",
        "        train_metrics = defaultdict(float)\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        print(f\"\\n📈 Epoch {epoch+1}/{epochs}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Mixed precision forward pass\n",
        "            if scaler:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    main_output, aux_output = model(data)\n",
        "                    main_loss = main_criterion(main_output, target)\n",
        "                    aux_loss = aux_criterion(aux_output, target)\n",
        "                    # Combine losses with weights\n",
        "                    total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
        "\n",
        "                # Mixed precision backward pass\n",
        "                scaler.scale(total_loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                main_output, aux_output = model(data)\n",
        "                main_loss = main_criterion(main_output, target)\n",
        "                aux_loss = aux_criterion(aux_output, target)\n",
        "                total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
        "\n",
        "                total_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Track metrics\n",
        "            train_metrics['total_loss'] += total_loss.item()\n",
        "            train_metrics['main_loss'] += main_loss.item()\n",
        "            train_metrics['aux_loss'] += aux_loss.item()\n",
        "\n",
        "            # Accuracy from main output\n",
        "            _, predicted = torch.max(main_output.data, 1)\n",
        "            train_total += target.size(0)\n",
        "            train_correct += (predicted == target).sum().item()\n",
        "\n",
        "            # Progress update\n",
        "            if batch_idx % 20 == 0:\n",
        "                current_lr = optimizer.param_groups[0]['lr']\n",
        "                print(f\"  Batch {batch_idx:3d}/{len(train_loader)} | \"\n",
        "                      f\"Loss: {total_loss.item():.4f} | \"\n",
        "                      f\"Acc: {100.*train_correct/train_total:.2f}% | \"\n",
        "                      f\"LR: {current_lr:.2e}\")\n",
        "\n",
        "        # ==========================================\n",
        "        # VALIDATION PHASE\n",
        "        # ==========================================\n",
        "        model.eval()\n",
        "        val_metrics = defaultdict(float)\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "\n",
        "                if scaler:\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        main_output, aux_output = model(data)\n",
        "                        main_loss = main_criterion(main_output, target)\n",
        "                        aux_loss = aux_criterion(aux_output, target)\n",
        "                        total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
        "                else:\n",
        "                    main_output, aux_output = model(data)\n",
        "                    main_loss = main_criterion(main_output, target)\n",
        "                    aux_loss = aux_criterion(aux_output, target)\n",
        "                    total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
        "\n",
        "                val_metrics['total_loss'] += total_loss.item()\n",
        "                val_metrics['main_loss'] += main_loss.item()\n",
        "                val_metrics['aux_loss'] += aux_loss.item()\n",
        "\n",
        "                _, predicted = torch.max(main_output.data, 1)\n",
        "                val_total += target.size(0)\n",
        "                val_correct += (predicted == target).sum().item()\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        train_loss = train_metrics['total_loss'] / len(train_loader)\n",
        "        train_acc = 100. * train_correct / train_total\n",
        "        val_loss = val_metrics['total_loss'] / len(val_loader)\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['train_main_loss'].append(train_metrics['main_loss'] / len(train_loader))\n",
        "        history['train_aux_loss'].append(train_metrics['aux_loss'] / len(train_loader))\n",
        "\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['val_main_loss'].append(val_metrics['main_loss'] / len(val_loader))\n",
        "        history['val_aux_loss'].append(val_metrics['aux_loss'] / len(val_loader))\n",
        "\n",
        "        history['lr'].append(current_lr)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        history['epoch_time'].append(epoch_time)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"\\n📊 Epoch {epoch+1} Summary:\")\n",
        "        print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
        "        print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
        "        print(f\"  LR: {current_lr:.2e}, Time: {epoch_time:.1f}s\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_val_acc': best_val_acc,\n",
        "                'history': history\n",
        "            }, save_path)\n",
        "            print(f\"  💾 New best model saved! Val Acc: {val_acc:.2f}%\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"  ⏳ Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\n🛑 Early stopping triggered! Best Val Acc: {best_val_acc:.2f}%\")\n",
        "            break\n",
        "\n",
        "        # Memory cleanup\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"\\n✅ Training completed!\")\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "    print(f\"Model saved to: {save_path}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "# Create save directory\n",
        "os.makedirs('saved_models', exist_ok=True)\n",
        "\n",
        "# Start training the aggressive model\n",
        "print(\"🚀 Starting training of AGGRESSIVE CNN-LSTM-Transformer...\")\n",
        "history = train_aggressive_model(\n",
        "    aggressive_model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    class_weights,\n",
        "    epochs=50,\n",
        "    initial_lr=1e-3,\n",
        "    save_path='saved_models/aggressive_cnn_lstm_transformer.pth'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# TRAINING STABILITY FIXES FOR AGGRESSIVE MODEL\n",
        "# =====================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "def train_stable_aggressive_model(model, train_loader, val_loader, class_weights,\n",
        "                                 epochs=50, initial_lr=5e-4, save_path='stable_aggressive_model.pth'):\n",
        "    \"\"\"\n",
        "    STABLE training with fixes for NaN loss:\n",
        "    - Gradient clipping\n",
        "    - Lower learning rate\n",
        "    - Gradient accumulation\n",
        "    - Better initialization\n",
        "    - Loss scaling protection\n",
        "    - Model parameter monitoring\n",
        "    \"\"\"\n",
        "    print(\"🔧 STARTING STABLE AGGRESSIVE MODEL TRAINING...\")\n",
        "    print(\"🛡️ STABILITY FIXES APPLIED:\")\n",
        "    print(\"• Gradient clipping (max_norm=1.0)\")\n",
        "    print(\"• Lower learning rate (5e-4 → 1e-4)\")\n",
        "    print(\"• Gradient accumulation (effective batch size x2)\")\n",
        "    print(\"• Weight initialization check\")\n",
        "    print(\"• NaN detection and recovery\")\n",
        "    print(\"• Mixed precision with loss scaling\")\n",
        "\n",
        "    # ==========================================\n",
        "    # MODEL STABILITY CHECKS\n",
        "    # ==========================================\n",
        "\n",
        "    # Check for NaN in initial weights\n",
        "    def check_model_weights(model, name=\"\"):\n",
        "        nan_count = 0\n",
        "        inf_count = 0\n",
        "        total_params = 0\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if torch.isnan(param).any():\n",
        "                print(f\"⚠️ NaN detected in {name}\")\n",
        "                nan_count += 1\n",
        "            if torch.isinf(param).any():\n",
        "                print(f\"⚠️ Inf detected in {name}\")\n",
        "                inf_count += 1\n",
        "            total_params += param.numel()\n",
        "\n",
        "        print(f\"Model check: {nan_count} NaN params, {inf_count} Inf params, {total_params:,} total\")\n",
        "        return nan_count == 0 and inf_count == 0\n",
        "\n",
        "    # Initialize model weights properly\n",
        "    def init_weights(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_normal_(m.weight, gain=0.1)  # Smaller gain\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu', a=0.1)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LSTM):\n",
        "            for name, param in m.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    nn.init.orthogonal_(param, gain=0.1)\n",
        "                elif 'bias' in name:\n",
        "                    nn.init.constant_(param, 0)\n",
        "\n",
        "    print(\"🔄 Reinitializing model weights with smaller scale...\")\n",
        "    model.apply(init_weights)\n",
        "\n",
        "    if not check_model_weights(model, \"After reinitialization\"):\n",
        "        print(\"❌ Model has NaN/Inf after initialization!\")\n",
        "        return None\n",
        "\n",
        "    print(\"✅ Model weights are stable\")\n",
        "\n",
        "    # ==========================================\n",
        "    # OPTIMIZER & SCHEDULER SETUP\n",
        "    # ==========================================\n",
        "\n",
        "    # Much more conservative optimizer settings\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=initial_lr,  # Reduced from 1e-3 to 5e-4\n",
        "        weight_decay=1e-5,  # Reduced weight decay\n",
        "        betas=(0.9, 0.98),  # More stable betas\n",
        "        eps=1e-6  # Larger epsilon\n",
        "    )\n",
        "\n",
        "    # More conservative scheduler\n",
        "    scheduler = CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=20,  # Longer restart period\n",
        "        T_mult=1,  # No multiplication\n",
        "        eta_min=1e-7\n",
        "    )\n",
        "\n",
        "    # Mixed precision with careful scaling\n",
        "    scaler = torch.cuda.amp.GradScaler(\n",
        "        init_scale=2**10,  # Smaller initial scale\n",
        "        growth_factor=1.1,  # Slower growth\n",
        "        backoff_factor=0.8,  # More aggressive backoff\n",
        "        growth_interval=100  # Less frequent growth\n",
        "    ) if device.type == 'cuda' else None\n",
        "\n",
        "    # Loss functions with label smoothing for stability\n",
        "    main_criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
        "    aux_criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
        "\n",
        "    # Training tracking\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [], 'train_main_loss': [], 'train_aux_loss': [],\n",
        "        'val_loss': [], 'val_acc': [], 'val_main_loss': [], 'val_aux_loss': [],\n",
        "        'lr': [], 'epoch_time': [], 'grad_norm': []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    patience = 20  # Increased patience\n",
        "    accumulation_steps = 2  # Gradient accumulation\n",
        "\n",
        "    print(f\"\\n🎯 STABLE Training Configuration:\")\n",
        "    print(f\"• Learning rate: {initial_lr} (reduced)\")\n",
        "    print(f\"• Weight decay: 1e-5 (reduced)\")\n",
        "    print(f\"• Gradient clipping: max_norm=1.0\")\n",
        "    print(f\"• Gradient accumulation: {accumulation_steps} steps\")\n",
        "    print(f\"• Label smoothing: 0.1\")\n",
        "    print(f\"• Patience: {patience}\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        # ==========================================\n",
        "        # TRAINING PHASE WITH STABILITY\n",
        "        # ==========================================\n",
        "        model.train()\n",
        "        train_metrics = defaultdict(float)\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        grad_norms = []\n",
        "\n",
        "        print(f\"\\n📈 Epoch {epoch+1}/{epochs}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Mixed precision forward pass\n",
        "            if scaler:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    main_output, aux_output = model(data)\n",
        "                    main_loss = main_criterion(main_output, target)\n",
        "                    aux_loss = aux_criterion(aux_output, target)\n",
        "                    total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
        "                    # Scale loss for gradient accumulation\n",
        "                    total_loss = total_loss / accumulation_steps\n",
        "\n",
        "                # Backward pass\n",
        "                scaler.scale(total_loss).backward()\n",
        "            else:\n",
        "                main_output, aux_output = model(data)\n",
        "                main_loss = main_criterion(main_output, target)\n",
        "                aux_loss = aux_criterion(aux_output, target)\n",
        "                total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
        "                total_loss = total_loss / accumulation_steps\n",
        "                total_loss.backward()\n",
        "\n",
        "            # Gradient accumulation\n",
        "            if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                if scaler:\n",
        "                    # Gradient clipping before optimizer step\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    grad_norms.append(grad_norm.item())\n",
        "\n",
        "                    # Check for NaN gradients\n",
        "                    if torch.isnan(grad_norm) or torch.isinf(grad_norm):\n",
        "                        print(f\"⚠️ NaN/Inf gradient detected at batch {batch_idx}, skipping...\")\n",
        "                        scaler.update()\n",
        "                        optimizer.zero_grad()\n",
        "                        continue\n",
        "\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    grad_norms.append(grad_norm.item())\n",
        "\n",
        "                    if torch.isnan(grad_norm) or torch.isinf(grad_norm):\n",
        "                        print(f\"⚠️ NaN/Inf gradient detected at batch {batch_idx}, skipping...\")\n",
        "                        optimizer.zero_grad()\n",
        "                        continue\n",
        "\n",
        "                    optimizer.step()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Track metrics (scale back the loss)\n",
        "            actual_loss = total_loss.item() * accumulation_steps\n",
        "            if not (torch.isnan(torch.tensor(actual_loss)) or torch.isinf(torch.tensor(actual_loss))):\n",
        "                train_metrics['total_loss'] += actual_loss\n",
        "                train_metrics['main_loss'] += main_loss.item()\n",
        "                train_metrics['aux_loss'] += aux_loss.item()\n",
        "\n",
        "                # Accuracy from main output\n",
        "                _, predicted = torch.max(main_output.data, 1)\n",
        "                train_total += target.size(0)\n",
        "                train_correct += (predicted == target).sum().item()\n",
        "\n",
        "            # Progress update\n",
        "            if batch_idx % 20 == 0:\n",
        "                current_lr = optimizer.param_groups[0]['lr']\n",
        "                avg_grad_norm = np.mean(grad_norms[-10:]) if grad_norms else 0.0\n",
        "                print(f\"  Batch {batch_idx:3d}/{len(train_loader)} | \"\n",
        "                      f\"Loss: {actual_loss:.4f} | \"\n",
        "                      f\"Acc: {100.*train_correct/train_total:.2f}% | \"\n",
        "                      f\"LR: {current_lr:.2e} | \"\n",
        "                      f\"GradNorm: {avg_grad_norm:.3f}\")\n",
        "\n",
        "        # ==========================================\n",
        "        # VALIDATION PHASE\n",
        "        # ==========================================\n",
        "        model.eval()\n",
        "        val_metrics = defaultdict(float)\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "\n",
        "                if scaler:\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        main_output, aux_output = model(data)\n",
        "                        main_loss = main_criterion(main_output, target)\n",
        "                        aux_loss = aux_criterion(aux_output, target)\n",
        "                        total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
        "                else:\n",
        "                    main_output, aux_output = model(data)\n",
        "                    main_loss = main_criterion(main_output, target)\n",
        "                    aux_loss = aux_criterion(aux_output, target)\n",
        "                    total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
        "\n",
        "                # Only add if not NaN\n",
        "                if not torch.isnan(total_loss):\n",
        "                    val_metrics['total_loss'] += total_loss.item()\n",
        "                    val_metrics['main_loss'] += main_loss.item()\n",
        "                    val_metrics['aux_loss'] += aux_loss.item()\n",
        "\n",
        "                    _, predicted = torch.max(main_output.data, 1)\n",
        "                    val_total += target.size(0)\n",
        "                    val_correct += (predicted == target).sum().item()\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        if len(train_loader) > 0 and train_total > 0:\n",
        "            train_loss = train_metrics['total_loss'] / len(train_loader)\n",
        "            train_acc = 100. * train_correct / train_total\n",
        "        else:\n",
        "            train_loss = float('inf')\n",
        "            train_acc = 0.0\n",
        "\n",
        "        if len(val_loader) > 0 and val_total > 0:\n",
        "            val_loss = val_metrics['total_loss'] / len(val_loader)\n",
        "            val_acc = 100. * val_correct / val_total\n",
        "        else:\n",
        "            val_loss = float('inf')\n",
        "            val_acc = 0.0\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Record history\n",
        "        if not (torch.isnan(torch.tensor(train_loss)) or torch.isnan(torch.tensor(val_loss))):\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['train_acc'].append(train_acc)\n",
        "            history['train_main_loss'].append(train_metrics['main_loss'] / len(train_loader))\n",
        "            history['train_aux_loss'].append(train_metrics['aux_loss'] / len(train_loader))\n",
        "\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['val_acc'].append(val_acc)\n",
        "            history['val_main_loss'].append(val_metrics['main_loss'] / len(val_loader))\n",
        "            history['val_aux_loss'].append(val_metrics['aux_loss'] / len(val_loader))\n",
        "\n",
        "            history['lr'].append(current_lr)\n",
        "            history['grad_norm'].append(np.mean(grad_norms) if grad_norms else 0.0)\n",
        "\n",
        "            epoch_time = time.time() - epoch_start_time\n",
        "            history['epoch_time'].append(epoch_time)\n",
        "\n",
        "            # Print epoch summary\n",
        "            print(f\"\\n📊 Epoch {epoch+1} Summary:\")\n",
        "            print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
        "            print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
        "            print(f\"  LR: {current_lr:.2e}, Time: {epoch_time:.1f}s\")\n",
        "            print(f\"  Avg Grad Norm: {np.mean(grad_norms):.3f}\")\n",
        "\n",
        "            # Save best model\n",
        "            if val_acc > best_val_acc and not torch.isnan(torch.tensor(val_acc)):\n",
        "                best_val_acc = val_acc\n",
        "                patience_counter = 0\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': scheduler.state_dict(),\n",
        "                    'best_val_acc': best_val_acc,\n",
        "                    'history': history\n",
        "                }, save_path)\n",
        "                print(f\"  💾 New best model saved! Val Acc: {val_acc:.2f}%\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                print(f\"  ⏳ Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "            # Early stopping\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"\\n🛑 Early stopping triggered! Best Val Acc: {best_val_acc:.2f}%\")\n",
        "                break\n",
        "        else:\n",
        "            print(f\"\\n⚠️ NaN loss detected in epoch {epoch+1}, continuing...\")\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Memory cleanup\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"\\n✅ Stable training completed!\")\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "    print(f\"Model saved to: {save_path}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "# Create a fresh model instance with proper initialization\n",
        "print(\"🔄 Creating fresh aggressive model with stability fixes...\")\n",
        "stable_aggressive_model = AggressiveCNN_LSTM_Transformer(\n",
        "    num_classes=4,\n",
        "    lstm_hidden=384,        # Slightly reduced from 512\n",
        "    transformer_dim=768,    # Slightly reduced from 1024\n",
        "    num_heads=12,          # Reduced from 16\n",
        "    num_layers=6           # Reduced from 8\n",
        ").to(device)\n",
        "\n",
        "print(\"🚀 Starting STABLE training of aggressive model...\")\n",
        "stable_history = train_stable_aggressive_model(\n",
        "    stable_aggressive_model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    class_weights,\n",
        "    epochs=50,\n",
        "    initial_lr=5e-4,  # Much lower learning rate\n",
        "    save_path='saved_models/stable_aggressive_cnn_lstm_transformer.pth'\n",
        ")"
      ],
      "metadata": {
        "id": "fNJXxZ0Pvgy9",
        "outputId": "c3077b51-c130-4409-c430-8c0e07d52845",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "fNJXxZ0Pvgy9",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Creating fresh aggressive model with stability fixes...\n",
            "🚀 Building AGGRESSIVE CNN-LSTM-Transformer for A100 40GB...\n",
            "• Deep CNN feature extraction (6 blocks)\n",
            "• Large LSTM temporal modeling (hidden: 384)\n",
            "• Deep Transformer self-attention (dim: 768, heads: 12, layers: 6)\n",
            "• Multi-scale feature fusion\n",
            "• Advanced attention mechanisms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ AGGRESSIVE CNN-LSTM-Transformer architecture built!\n",
            "🚀 Starting STABLE training of aggressive model...\n",
            "🔧 STARTING STABLE AGGRESSIVE MODEL TRAINING...\n",
            "🛡️ STABILITY FIXES APPLIED:\n",
            "• Gradient clipping (max_norm=1.0)\n",
            "• Lower learning rate (5e-4 → 1e-4)\n",
            "• Gradient accumulation (effective batch size x2)\n",
            "• Weight initialization check\n",
            "• NaN detection and recovery\n",
            "• Mixed precision with loss scaling\n",
            "🔄 Reinitializing model weights with smaller scale...\n",
            "Model check: 0 NaN params, 0 Inf params, 106,655,432 total\n",
            "✅ Model weights are stable\n",
            "\n",
            "🎯 STABLE Training Configuration:\n",
            "• Learning rate: 0.0005 (reduced)\n",
            "• Weight decay: 1e-5 (reduced)\n",
            "• Gradient clipping: max_norm=1.0\n",
            "• Gradient accumulation: 2 steps\n",
            "• Label smoothing: 0.1\n",
            "• Patience: 20\n",
            "\n",
            "📈 Epoch 1/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-771516916.py:102: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(\n",
            "/tmp/ipython-input-771516916.py:155: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   0/125 | Loss: 1.3962 | Acc: 25.00% | LR: 5.00e-04 | GradNorm: 0.000\n",
            "  Batch  20/125 | Loss: 1.3916 | Acc: 23.81% | LR: 5.00e-04 | GradNorm: 0.109\n",
            "  Batch  40/125 | Loss: 1.3934 | Acc: 23.32% | LR: 5.00e-04 | GradNorm: 0.084\n",
            "  Batch  60/125 | Loss: 1.4074 | Acc: 21.52% | LR: 5.00e-04 | GradNorm: 0.094\n",
            "  Batch  80/125 | Loss: 1.4174 | Acc: 21.76% | LR: 5.00e-04 | GradNorm: 0.119\n",
            "  Batch 100/125 | Loss: 1.4079 | Acc: 22.34% | LR: 5.00e-04 | GradNorm: 0.120\n",
            "  Batch 120/125 | Loss: 1.3913 | Acc: 21.85% | LR: 5.00e-04 | GradNorm: 0.135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-771516916.py:238: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 1 Summary:\n",
            "  Train: Loss=1.3963, Acc=22.16%\n",
            "  Val:   Loss=1.4146, Acc=12.10%\n",
            "  LR: 4.97e-04, Time: 39.1s\n",
            "  Avg Grad Norm: 0.111\n",
            "  💾 New best model saved! Val Acc: 12.10%\n",
            "\n",
            "📈 Epoch 2/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3910 | Acc: 12.50% | LR: 4.97e-04 | GradNorm: 0.000\n",
            "  Batch  20/125 | Loss: 1.3970 | Acc: 20.83% | LR: 4.97e-04 | GradNorm: 0.135\n",
            "  Batch  40/125 | Loss: 1.3841 | Acc: 21.04% | LR: 4.97e-04 | GradNorm: 0.096\n",
            "  Batch  60/125 | Loss: 1.4186 | Acc: 20.39% | LR: 4.97e-04 | GradNorm: 0.100\n",
            "  Batch  80/125 | Loss: 1.3956 | Acc: 21.30% | LR: 4.97e-04 | GradNorm: 0.109\n",
            "  Batch 100/125 | Loss: 1.3442 | Acc: 21.97% | LR: 4.97e-04 | GradNorm: 0.198\n",
            "  Batch 120/125 | Loss: 1.4253 | Acc: 22.00% | LR: 4.97e-04 | GradNorm: 0.197\n",
            "\n",
            "📊 Epoch 2 Summary:\n",
            "  Train: Loss=1.3956, Acc=22.10%\n",
            "  Val:   Loss=1.4278, Acc=12.10%\n",
            "  LR: 4.88e-04, Time: 39.1s\n",
            "  Avg Grad Norm: 0.140\n",
            "  ⏳ Patience: 1/20\n",
            "\n",
            "📈 Epoch 3/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3569 | Acc: 37.50% | LR: 4.88e-04 | GradNorm: 0.000\n",
            "  Batch  20/125 | Loss: 1.3894 | Acc: 21.43% | LR: 4.88e-04 | GradNorm: 0.116\n",
            "  Batch  40/125 | Loss: 1.4358 | Acc: 22.71% | LR: 4.88e-04 | GradNorm: 0.142\n",
            "  Batch  60/125 | Loss: 1.3904 | Acc: 21.52% | LR: 4.88e-04 | GradNorm: 0.158\n",
            "  Batch  80/125 | Loss: 1.3809 | Acc: 22.99% | LR: 4.88e-04 | GradNorm: 0.132\n",
            "  Batch 100/125 | Loss: 1.3865 | Acc: 22.03% | LR: 4.88e-04 | GradNorm: 0.152\n",
            "  Batch 120/125 | Loss: 1.4001 | Acc: 21.44% | LR: 4.88e-04 | GradNorm: 0.128\n",
            "\n",
            "📊 Epoch 3 Summary:\n",
            "  Train: Loss=1.3962, Acc=21.20%\n",
            "  Val:   Loss=1.4250, Acc=12.10%\n",
            "  LR: 4.73e-04, Time: 39.0s\n",
            "  Avg Grad Norm: 0.139\n",
            "  ⏳ Patience: 2/20\n",
            "\n",
            "📈 Epoch 4/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.4066 | Acc: 18.75% | LR: 4.73e-04 | GradNorm: 0.000\n",
            "  Batch  20/125 | Loss: 1.4280 | Acc: 24.11% | LR: 4.73e-04 | GradNorm: 0.124\n",
            "  Batch  40/125 | Loss: 1.4497 | Acc: 22.10% | LR: 4.73e-04 | GradNorm: 0.114\n",
            "  Batch  60/125 | Loss: 1.4130 | Acc: 22.13% | LR: 4.73e-04 | GradNorm: 0.116\n",
            "  Batch  80/125 | Loss: 1.4140 | Acc: 21.76% | LR: 4.73e-04 | GradNorm: 0.103\n",
            "  Batch 100/125 | Loss: 1.3699 | Acc: 22.22% | LR: 4.73e-04 | GradNorm: 0.131\n",
            "  Batch 120/125 | Loss: 1.4064 | Acc: 22.11% | LR: 4.73e-04 | GradNorm: 0.147\n",
            "\n",
            "📊 Epoch 4 Summary:\n",
            "  Train: Loss=1.3961, Acc=22.31%\n",
            "  Val:   Loss=1.4291, Acc=12.10%\n",
            "  LR: 4.52e-04, Time: 39.1s\n",
            "  Avg Grad Norm: 0.122\n",
            "  ⏳ Patience: 3/20\n",
            "\n",
            "📈 Epoch 5/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3980 | Acc: 18.75% | LR: 4.52e-04 | GradNorm: 0.000\n",
            "  Batch  20/125 | Loss: 1.3806 | Acc: 20.24% | LR: 4.52e-04 | GradNorm: 0.128\n",
            "  Batch  40/125 | Loss: 1.3773 | Acc: 21.19% | LR: 4.52e-04 | GradNorm: 0.113\n",
            "  Batch  60/125 | Loss: 1.3880 | Acc: 22.64% | LR: 4.52e-04 | GradNorm: 0.137\n",
            "  Batch  80/125 | Loss: 1.3760 | Acc: 22.15% | LR: 4.52e-04 | GradNorm: 0.133\n",
            "  Batch 100/125 | Loss: 1.3683 | Acc: 21.84% | LR: 4.52e-04 | GradNorm: 0.138\n",
            "  Batch 120/125 | Loss: 1.4089 | Acc: 22.21% | LR: 4.52e-04 | GradNorm: 0.137\n",
            "\n",
            "📊 Epoch 5 Summary:\n",
            "  Train: Loss=1.3950, Acc=22.31%\n",
            "  Val:   Loss=1.4334, Acc=12.10%\n",
            "  LR: 4.27e-04, Time: 39.1s\n",
            "  Avg Grad Norm: 0.130\n",
            "  ⏳ Patience: 4/20\n",
            "\n",
            "📈 Epoch 6/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3816 | Acc: 6.25% | LR: 4.27e-04 | GradNorm: 0.000\n",
            "  Batch  20/125 | Loss: 1.3986 | Acc: 23.81% | LR: 4.27e-04 | GradNorm: 0.146\n",
            "  Batch  40/125 | Loss: 1.3634 | Acc: 23.02% | LR: 4.27e-04 | GradNorm: 0.186\n",
            "  Batch  60/125 | Loss: 1.4042 | Acc: 22.34% | LR: 4.27e-04 | GradNorm: 0.161\n",
            "  Batch  80/125 | Loss: 1.3995 | Acc: 22.45% | LR: 4.27e-04 | GradNorm: 0.132\n",
            "  Batch 100/125 | Loss: 1.3957 | Acc: 22.90% | LR: 4.27e-04 | GradNorm: 0.116\n",
            "  Batch 120/125 | Loss: 1.3784 | Acc: 22.52% | LR: 4.27e-04 | GradNorm: 0.149\n",
            "\n",
            "📊 Epoch 6 Summary:\n",
            "  Train: Loss=1.3949, Acc=22.31%\n",
            "  Val:   Loss=1.4269, Acc=12.10%\n",
            "  LR: 3.97e-04, Time: 39.1s\n",
            "  Avg Grad Norm: 0.147\n",
            "  ⏳ Patience: 5/20\n",
            "\n",
            "📈 Epoch 7/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.4297 | Acc: 6.25% | LR: 3.97e-04 | GradNorm: 0.000\n",
            "  Batch  20/125 | Loss: 1.3661 | Acc: 21.43% | LR: 3.97e-04 | GradNorm: 0.112\n",
            "  Batch  40/125 | Loss: 1.3738 | Acc: 23.02% | LR: 3.97e-04 | GradNorm: 0.141\n",
            "  Batch  60/125 | Loss: 1.3851 | Acc: 23.57% | LR: 3.97e-04 | GradNorm: 0.148\n",
            "  Batch  80/125 | Loss: 1.4016 | Acc: 22.76% | LR: 3.97e-04 | GradNorm: 0.152\n",
            "  Batch 100/125 | Loss: 1.3963 | Acc: 22.59% | LR: 3.97e-04 | GradNorm: 0.107\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-771516916.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🚀 Starting STABLE training of aggressive model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m stable_history = train_stable_aggressive_model(\n\u001b[0m\u001b[1;32m    350\u001b[0m     \u001b[0mstable_aggressive_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-771516916.py\u001b[0m in \u001b[0;36mtrain_stable_aggressive_model\u001b[0;34m(model, train_loader, val_loader, class_weights, epochs, initial_lr, save_path)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mmain_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# SIMPLE WORKING MODEL WITH LEAKY RELU\n",
        "# =====================================================\n",
        "\n",
        "class SimpleCNN_LSTM_LeakyReLU(nn.Module):\n",
        "    def __init__(self, num_classes=4, lstm_hidden=128):\n",
        "        super(SimpleCNN_LSTM_LeakyReLU, self).__init__()\n",
        "\n",
        "        print(\"🎯 Building SIMPLE CNN-LSTM with LeakyReLU...\")\n",
        "\n",
        "        # Simple CNN - 3 blocks only (like your working model)\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),  # LeakyReLU as suggested\n",
        "            nn.Conv2d(32, 32, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout2d(0.1),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout2d(0.15),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(128, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout2d(0.2),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "\n",
        "        # Calculate feature size: same as your original model\n",
        "        self.feature_size = 128 * 16  # 128 channels * 16 remaining pitch bins\n",
        "\n",
        "        # Simple LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.feature_size,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=0.2,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Simple attention\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=lstm_hidden * 2,\n",
        "            num_heads=4,  # Fewer heads\n",
        "            dropout=0.2,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Simple classifier with LeakyReLU\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(lstm_hidden * 2, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "        print(\"✅ Simple LeakyReLU architecture built!\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # CNN feature extraction\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        # Reshape for LSTM (same as original)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = x.contiguous().view(batch_size, x.size(1), -1)\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Attention\n",
        "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
        "\n",
        "        # Global average pooling\n",
        "        pooled = torch.mean(attn_out, dim=1)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(pooled)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Create simple model\n",
        "simple_model = SimpleCNN_LSTM_LeakyReLU(num_classes=4, lstm_hidden=128).to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in simple_model.parameters())\n",
        "print(f\"📊 Simple Model: {total_params:,} parameters (vs 106M+ in aggressive model)\")\n",
        "\n",
        "# Test forward pass\n",
        "test_input = torch.randn(4, 1, 128, 4500).to(device)\n",
        "with torch.no_grad():\n",
        "    output = simple_model(test_input)\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"✅ Simple model forward pass successful!\")"
      ],
      "metadata": {
        "id": "GIpRHJc19GL3",
        "outputId": "96312d40-8657-45c0-95c4-a04ff1c65482",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "GIpRHJc19GL3",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Building SIMPLE CNN-LSTM with LeakyReLU...\n",
            "✅ Simple LeakyReLU architecture built!\n",
            "📊 Simple Model: 3,275,236 parameters (vs 106M+ in aggressive model)\n",
            "Output shape: torch.Size([4, 4])\n",
            "✅ Simple model forward pass successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_simple_model_working():\n",
        "    \"\"\"Train the simple model with settings that should work\"\"\"\n",
        "\n",
        "    print(\"🚀 TRAINING SIMPLE MODEL WITH PROVEN SETTINGS...\")\n",
        "\n",
        "    # Use higher learning rate and simple loss\n",
        "    optimizer = optim.Adam(simple_model.parameters(), lr=1e-2)  # Higher LR\n",
        "    criterion = nn.CrossEntropyLoss()  # No class weights initially\n",
        "\n",
        "    # Train for just 10 epochs to test\n",
        "    epochs = 10\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        simple_model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = simple_model(data)\n",
        "            loss = criterion(outputs, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "            if batch_idx % 20 == 0:\n",
        "                print(f\"  Batch {batch_idx:3d}/{len(train_loader)} | \"\n",
        "                      f\"Loss: {loss.item():.4f} | \"\n",
        "                      f\"Acc: {100.*correct/total:.2f}%\")\n",
        "\n",
        "        # Validation\n",
        "        simple_model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                outputs = simple_model(data)\n",
        "                loss = criterion(outputs, target)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += target.size(0)\n",
        "                val_correct += (predicted == target).sum().item()\n",
        "\n",
        "        train_acc = 100. * correct / total\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "\n",
        "        print(f\"📊 Epoch {epoch+1} Summary:\")\n",
        "        print(f\"  Train: Loss={total_loss/len(train_loader):.4f}, Acc={train_acc:.2f}%\")\n",
        "        print(f\"  Val:   Loss={val_loss/len(val_loader):.4f}, Acc={val_acc:.2f}%\")\n",
        "\n",
        "        # If validation accuracy > 25% (random chance), model is learning!\n",
        "        if val_acc > 25:\n",
        "            print(f\"  ✅ Model is learning! (Val Acc > random: {val_acc:.2f}%)\")\n",
        "        else:\n",
        "            print(f\"  ⚠️ Still around random chance: {val_acc:.2f}%\")\n",
        "\n",
        "# Run the simple training\n",
        "train_simple_model_working()"
      ],
      "metadata": {
        "id": "AITw-NWnvs_h",
        "outputId": "a884fcc5-2ba8-44d6-d925-38e9ef36ba15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AITw-NWnvs_h",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 TRAINING SIMPLE MODEL WITH PROVEN SETTINGS...\n",
            "\n",
            "Epoch 1/10\n",
            "  Batch   0/125 | Loss: 1.4000 | Acc: 6.25%\n",
            "  Batch  20/125 | Loss: 1.4118 | Acc: 25.00%\n",
            "  Batch  40/125 | Loss: 1.3426 | Acc: 25.46%\n",
            "  Batch  60/125 | Loss: 1.4429 | Acc: 30.43%\n",
            "  Batch  80/125 | Loss: 1.3237 | Acc: 33.56%\n",
            "  Batch 100/125 | Loss: 1.3595 | Acc: 32.43%\n",
            "  Batch 120/125 | Loss: 1.4348 | Acc: 31.87%\n",
            "📊 Epoch 1 Summary:\n",
            "  Train: Loss=1.4955, Acc=31.52%\n",
            "  Val:   Loss=1.3384, Acc=35.16%\n",
            "  ✅ Model is learning! (Val Acc > random: 35.16%)\n",
            "\n",
            "Epoch 2/10\n",
            "  Batch   0/125 | Loss: 1.4588 | Acc: 18.75%\n",
            "  Batch  20/125 | Loss: 1.3310 | Acc: 32.74%\n",
            "  Batch  40/125 | Loss: 1.4169 | Acc: 32.16%\n",
            "  Batch  60/125 | Loss: 1.4940 | Acc: 32.58%\n",
            "  Batch  80/125 | Loss: 1.3641 | Acc: 31.64%\n",
            "  Batch 100/125 | Loss: 1.4821 | Acc: 32.67%\n",
            "  Batch 120/125 | Loss: 1.2441 | Acc: 35.07%\n",
            "📊 Epoch 2 Summary:\n",
            "  Train: Loss=1.3399, Acc=34.94%\n",
            "  Val:   Loss=1.2627, Acc=46.12%\n",
            "  ✅ Model is learning! (Val Acc > random: 46.12%)\n",
            "\n",
            "Epoch 3/10\n",
            "  Batch   0/125 | Loss: 1.2675 | Acc: 31.25%\n",
            "  Batch  20/125 | Loss: 1.1578 | Acc: 46.43%\n",
            "  Batch  40/125 | Loss: 1.6428 | Acc: 48.93%\n",
            "  Batch  60/125 | Loss: 1.2687 | Acc: 46.93%\n",
            "  Batch  80/125 | Loss: 0.8734 | Acc: 48.77%\n",
            "  Batch 100/125 | Loss: 1.3433 | Acc: 49.07%\n",
            "  Batch 120/125 | Loss: 1.0395 | Acc: 48.97%\n",
            "📊 Epoch 3 Summary:\n",
            "  Train: Loss=1.1713, Acc=49.09%\n",
            "  Val:   Loss=1.0694, Acc=59.59%\n",
            "  ✅ Model is learning! (Val Acc > random: 59.59%)\n",
            "\n",
            "Epoch 4/10\n",
            "  Batch   0/125 | Loss: 1.2367 | Acc: 43.75%\n",
            "  Batch  20/125 | Loss: 0.8377 | Acc: 50.60%\n",
            "  Batch  40/125 | Loss: 0.8721 | Acc: 55.18%\n",
            "  Batch  60/125 | Loss: 1.1600 | Acc: 52.77%\n",
            "  Batch  80/125 | Loss: 1.2917 | Acc: 51.00%\n",
            "  Batch 100/125 | Loss: 0.6111 | Acc: 51.55%\n",
            "  Batch 120/125 | Loss: 1.0220 | Acc: 51.86%\n",
            "📊 Epoch 4 Summary:\n",
            "  Train: Loss=1.1016, Acc=52.01%\n",
            "  Val:   Loss=0.9537, Acc=66.44%\n",
            "  ✅ Model is learning! (Val Acc > random: 66.44%)\n",
            "\n",
            "Epoch 5/10\n",
            "  Batch   0/125 | Loss: 0.7312 | Acc: 75.00%\n",
            "  Batch  20/125 | Loss: 1.4802 | Acc: 51.79%\n",
            "  Batch  40/125 | Loss: 1.0440 | Acc: 51.83%\n",
            "  Batch  60/125 | Loss: 1.1067 | Acc: 52.66%\n",
            "  Batch  80/125 | Loss: 0.7533 | Acc: 53.70%\n",
            "  Batch 100/125 | Loss: 1.0011 | Acc: 55.45%\n",
            "  Batch 120/125 | Loss: 1.0325 | Acc: 56.15%\n",
            "📊 Epoch 5 Summary:\n",
            "  Train: Loss=1.0499, Acc=56.14%\n",
            "  Val:   Loss=1.1501, Acc=60.27%\n",
            "  ✅ Model is learning! (Val Acc > random: 60.27%)\n",
            "\n",
            "Epoch 6/10\n",
            "  Batch   0/125 | Loss: 1.0870 | Acc: 81.25%\n",
            "  Batch  20/125 | Loss: 1.0370 | Acc: 56.85%\n",
            "  Batch  40/125 | Loss: 1.2324 | Acc: 57.01%\n",
            "  Batch  60/125 | Loss: 0.9214 | Acc: 57.58%\n",
            "  Batch  80/125 | Loss: 1.0652 | Acc: 56.71%\n",
            "  Batch 100/125 | Loss: 1.0891 | Acc: 56.87%\n",
            "  Batch 120/125 | Loss: 0.8600 | Acc: 57.64%\n",
            "📊 Epoch 6 Summary:\n",
            "  Train: Loss=0.9934, Acc=57.35%\n",
            "  Val:   Loss=1.1472, Acc=49.09%\n",
            "  ✅ Model is learning! (Val Acc > random: 49.09%)\n",
            "\n",
            "Epoch 7/10\n",
            "  Batch   0/125 | Loss: 1.4143 | Acc: 37.50%\n",
            "  Batch  20/125 | Loss: 1.4562 | Acc: 51.19%\n",
            "  Batch  40/125 | Loss: 1.0182 | Acc: 53.96%\n",
            "  Batch  60/125 | Loss: 0.5467 | Acc: 58.61%\n",
            "  Batch  80/125 | Loss: 0.9373 | Acc: 58.72%\n",
            "  Batch 100/125 | Loss: 1.0240 | Acc: 58.04%\n",
            "  Batch 120/125 | Loss: 1.0672 | Acc: 57.90%\n",
            "📊 Epoch 7 Summary:\n",
            "  Train: Loss=0.9848, Acc=58.01%\n",
            "  Val:   Loss=0.8464, Acc=64.61%\n",
            "  ✅ Model is learning! (Val Acc > random: 64.61%)\n",
            "\n",
            "Epoch 8/10\n",
            "  Batch   0/125 | Loss: 1.7556 | Acc: 37.50%\n",
            "  Batch  20/125 | Loss: 0.9532 | Acc: 56.55%\n",
            "  Batch  40/125 | Loss: 0.6034 | Acc: 57.93%\n",
            "  Batch  60/125 | Loss: 0.7568 | Acc: 58.61%\n",
            "  Batch  80/125 | Loss: 0.9720 | Acc: 58.95%\n",
            "  Batch 100/125 | Loss: 0.5175 | Acc: 59.28%\n",
            "  Batch 120/125 | Loss: 1.2094 | Acc: 58.73%\n",
            "📊 Epoch 8 Summary:\n",
            "  Train: Loss=1.0175, Acc=58.56%\n",
            "  Val:   Loss=1.0090, Acc=63.24%\n",
            "  ✅ Model is learning! (Val Acc > random: 63.24%)\n",
            "\n",
            "Epoch 9/10\n",
            "  Batch   0/125 | Loss: 1.0533 | Acc: 50.00%\n",
            "  Batch  20/125 | Loss: 1.2293 | Acc: 48.51%\n",
            "  Batch  40/125 | Loss: 1.0551 | Acc: 48.63%\n",
            "  Batch  60/125 | Loss: 1.4748 | Acc: 49.59%\n",
            "  Batch  80/125 | Loss: 0.9262 | Acc: 51.54%\n",
            "  Batch 100/125 | Loss: 1.2463 | Acc: 51.36%\n",
            "  Batch 120/125 | Loss: 0.8817 | Acc: 51.60%\n",
            "📊 Epoch 9 Summary:\n",
            "  Train: Loss=1.0550, Acc=51.96%\n",
            "  Val:   Loss=1.1828, Acc=64.38%\n",
            "  ✅ Model is learning! (Val Acc > random: 64.38%)\n",
            "\n",
            "Epoch 10/10\n",
            "  Batch   0/125 | Loss: 1.0968 | Acc: 68.75%\n",
            "  Batch  20/125 | Loss: 1.2217 | Acc: 54.76%\n",
            "  Batch  40/125 | Loss: 1.7517 | Acc: 54.12%\n",
            "  Batch  60/125 | Loss: 1.1674 | Acc: 52.46%\n",
            "  Batch  80/125 | Loss: 0.6341 | Acc: 53.94%\n",
            "  Batch 100/125 | Loss: 1.4714 | Acc: 54.46%\n",
            "  Batch 120/125 | Loss: 0.9200 | Acc: 54.60%\n",
            "📊 Epoch 10 Summary:\n",
            "  Train: Loss=1.0679, Acc=54.58%\n",
            "  Val:   Loss=1.0618, Acc=49.77%\n",
            "  ✅ Model is learning! (Val Acc > random: 49.77%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_improved_simple_model():\n",
        "    \"\"\"Train the simple model with improvements for better performance\"\"\"\n",
        "\n",
        "    print(\"🚀 TRAINING IMPROVED SIMPLE MODEL...\")\n",
        "\n",
        "    # Use the working settings but with improvements\n",
        "    optimizer = optim.AdamW(simple_model.parameters(), lr=5e-3, weight_decay=1e-4)  # Slightly lower LR + weight decay\n",
        "\n",
        "    # Add class weights now that we know it can learn\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)  # Now use class weights\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
        "    )\n",
        "\n",
        "    # Train for more epochs with early stopping\n",
        "    epochs = 25\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    patience = 8\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        simple_model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = simple_model(data)\n",
        "            loss = criterion(outputs, target)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(simple_model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "            if batch_idx % 25 == 0:\n",
        "                print(f\"  Batch {batch_idx:3d}/{len(train_loader)} | \"\n",
        "                      f\"Loss: {loss.item():.4f} | \"\n",
        "                      f\"Acc: {100.*correct/total:.2f}% | \"\n",
        "                      f\"LR: {optimizer.param_groups[0]['lr']:.1e}\")\n",
        "\n",
        "        # Validation\n",
        "        simple_model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                outputs = simple_model(data)\n",
        "                loss = criterion(outputs, target)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += target.size(0)\n",
        "                val_correct += (predicted == target).sum().item()\n",
        "\n",
        "        train_acc = 100. * correct / total\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "\n",
        "        print(f\"📊 Epoch {epoch+1} Summary:\")\n",
        "        print(f\"  Train: Loss={total_loss/len(train_loader):.4f}, Acc={train_acc:.2f}%\")\n",
        "        print(f\"  Val:   Loss={val_loss/len(val_loader):.4f}, Acc={val_acc:.2f}%\")\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save({\n",
        "                'model_state_dict': simple_model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'epoch': epoch\n",
        "            }, 'saved_models/simple_leaky_relu_model.pth')\n",
        "            print(f\"  💾 New best model saved! Val Acc: {val_acc:.2f}%\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"  ⏳ Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\n🛑 Early stopping! Best Val Acc: {best_val_acc:.2f}%\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\n✅ Training completed! Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "    return best_val_acc\n",
        "\n",
        "# Create save directory\n",
        "os.makedirs('saved_models', exist_ok=True)\n",
        "\n",
        "# Train the improved model\n",
        "best_acc = train_improved_simple_model()"
      ],
      "metadata": {
        "id": "xCazAyyO8fz1",
        "outputId": "43433d7a-404d-4d77-aade-01b4d0a57d13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "xCazAyyO8fz1",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 TRAINING IMPROVED SIMPLE MODEL...\n",
            "\n",
            "Epoch 1/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   0/125 | Loss: 1.1525 | Acc: 50.00% | LR: 5.0e-03\n",
            "  Batch  25/125 | Loss: 0.8272 | Acc: 51.68% | LR: 5.0e-03\n",
            "  Batch  50/125 | Loss: 1.0390 | Acc: 53.92% | LR: 5.0e-03\n",
            "  Batch  75/125 | Loss: 0.9505 | Acc: 56.66% | LR: 5.0e-03\n",
            "  Batch 100/125 | Loss: 1.0458 | Acc: 57.18% | LR: 5.0e-03\n",
            "📊 Epoch 1 Summary:\n",
            "  Train: Loss=0.9813, Acc=58.61%\n",
            "  Val:   Loss=1.1967, Acc=59.13%\n",
            "  💾 New best model saved! Val Acc: 59.13%\n",
            "\n",
            "Epoch 2/25\n",
            "  Batch   0/125 | Loss: 0.7123 | Acc: 75.00% | LR: 5.0e-03\n",
            "  Batch  25/125 | Loss: 0.8929 | Acc: 63.70% | LR: 5.0e-03\n",
            "  Batch  50/125 | Loss: 1.1012 | Acc: 63.60% | LR: 5.0e-03\n",
            "  Batch  75/125 | Loss: 0.7676 | Acc: 66.12% | LR: 5.0e-03\n",
            "  Batch 100/125 | Loss: 0.9061 | Acc: 64.48% | LR: 5.0e-03\n",
            "📊 Epoch 2 Summary:\n",
            "  Train: Loss=0.9057, Acc=63.60%\n",
            "  Val:   Loss=1.0638, Acc=61.42%\n",
            "  💾 New best model saved! Val Acc: 61.42%\n",
            "\n",
            "Epoch 3/25\n",
            "  Batch   0/125 | Loss: 1.2152 | Acc: 56.25% | LR: 5.0e-03\n",
            "  Batch  25/125 | Loss: 0.6727 | Acc: 66.59% | LR: 5.0e-03\n",
            "  Batch  50/125 | Loss: 1.0883 | Acc: 63.60% | LR: 5.0e-03\n",
            "  Batch  75/125 | Loss: 0.7467 | Acc: 64.97% | LR: 5.0e-03\n",
            "  Batch 100/125 | Loss: 0.9642 | Acc: 65.41% | LR: 5.0e-03\n",
            "📊 Epoch 3 Summary:\n",
            "  Train: Loss=0.8694, Acc=65.56%\n",
            "  Val:   Loss=0.9239, Acc=57.99%\n",
            "  ⏳ Patience: 1/8\n",
            "\n",
            "Epoch 4/25\n",
            "  Batch   0/125 | Loss: 0.9610 | Acc: 50.00% | LR: 5.0e-03\n",
            "  Batch  25/125 | Loss: 0.6374 | Acc: 65.87% | LR: 5.0e-03\n",
            "  Batch  50/125 | Loss: 1.1981 | Acc: 67.28% | LR: 5.0e-03\n",
            "  Batch  75/125 | Loss: 0.7055 | Acc: 67.35% | LR: 5.0e-03\n",
            "  Batch 100/125 | Loss: 0.4629 | Acc: 69.25% | LR: 5.0e-03\n",
            "📊 Epoch 4 Summary:\n",
            "  Train: Loss=0.8234, Acc=69.28%\n",
            "  Val:   Loss=1.1419, Acc=63.70%\n",
            "  💾 New best model saved! Val Acc: 63.70%\n",
            "\n",
            "Epoch 5/25\n",
            "  Batch   0/125 | Loss: 0.8387 | Acc: 56.25% | LR: 5.0e-03\n",
            "  Batch  25/125 | Loss: 0.3737 | Acc: 69.71% | LR: 5.0e-03\n",
            "  Batch  50/125 | Loss: 0.7232 | Acc: 68.26% | LR: 5.0e-03\n",
            "  Batch  75/125 | Loss: 0.5866 | Acc: 69.24% | LR: 5.0e-03\n",
            "  Batch 100/125 | Loss: 0.6354 | Acc: 69.74% | LR: 5.0e-03\n",
            "📊 Epoch 5 Summary:\n",
            "  Train: Loss=0.7604, Acc=70.09%\n",
            "  Val:   Loss=1.1225, Acc=61.87%\n",
            "  ⏳ Patience: 1/8\n",
            "\n",
            "Epoch 6/25\n",
            "  Batch   0/125 | Loss: 0.6234 | Acc: 81.25% | LR: 5.0e-03\n",
            "  Batch  25/125 | Loss: 1.4235 | Acc: 68.75% | LR: 5.0e-03\n",
            "  Batch  50/125 | Loss: 0.6025 | Acc: 69.98% | LR: 5.0e-03\n",
            "  Batch  75/125 | Loss: 0.5120 | Acc: 70.15% | LR: 5.0e-03\n",
            "  Batch 100/125 | Loss: 0.5820 | Acc: 70.85% | LR: 5.0e-03\n",
            "📊 Epoch 6 Summary:\n",
            "  Train: Loss=0.7593, Acc=70.59%\n",
            "  Val:   Loss=1.3756, Acc=64.61%\n",
            "  💾 New best model saved! Val Acc: 64.61%\n",
            "\n",
            "Epoch 7/25\n",
            "  Batch   0/125 | Loss: 0.4138 | Acc: 81.25% | LR: 5.0e-03\n",
            "  Batch  25/125 | Loss: 0.5338 | Acc: 70.19% | LR: 5.0e-03\n",
            "  Batch  50/125 | Loss: 0.7902 | Acc: 72.06% | LR: 5.0e-03\n",
            "  Batch  75/125 | Loss: 0.5960 | Acc: 73.11% | LR: 5.0e-03\n",
            "  Batch 100/125 | Loss: 0.4223 | Acc: 74.07% | LR: 5.0e-03\n",
            "📊 Epoch 7 Summary:\n",
            "  Train: Loss=0.7382, Acc=73.36%\n",
            "  Val:   Loss=0.9808, Acc=59.13%\n",
            "  ⏳ Patience: 1/8\n",
            "\n",
            "Epoch 8/25\n",
            "  Batch   0/125 | Loss: 0.6849 | Acc: 75.00% | LR: 5.0e-03\n",
            "  Batch  25/125 | Loss: 0.7057 | Acc: 75.00% | LR: 5.0e-03\n",
            "  Batch  50/125 | Loss: 0.7878 | Acc: 76.10% | LR: 5.0e-03\n",
            "  Batch  75/125 | Loss: 0.6734 | Acc: 75.00% | LR: 5.0e-03\n",
            "  Batch 100/125 | Loss: 0.2935 | Acc: 75.43% | LR: 5.0e-03\n",
            "📊 Epoch 8 Summary:\n",
            "  Train: Loss=0.6440, Acc=75.03%\n",
            "  Val:   Loss=1.3734, Acc=51.37%\n",
            "  ⏳ Patience: 2/8\n",
            "\n",
            "Epoch 9/25\n",
            "  Batch   0/125 | Loss: 0.5465 | Acc: 75.00% | LR: 5.0e-03\n",
            "  Batch  25/125 | Loss: 0.3557 | Acc: 72.36% | LR: 5.0e-03\n",
            "  Batch  50/125 | Loss: 0.6605 | Acc: 74.88% | LR: 5.0e-03\n",
            "  Batch  75/125 | Loss: 0.4239 | Acc: 76.81% | LR: 5.0e-03\n",
            "  Batch 100/125 | Loss: 0.6944 | Acc: 76.30% | LR: 5.0e-03\n",
            "📊 Epoch 9 Summary:\n",
            "  Train: Loss=0.6843, Acc=74.97%\n",
            "  Val:   Loss=1.0762, Acc=60.27%\n",
            "  ⏳ Patience: 3/8\n",
            "\n",
            "Epoch 10/25\n",
            "  Batch   0/125 | Loss: 0.8099 | Acc: 62.50% | LR: 5.0e-03\n",
            "  Batch  25/125 | Loss: 0.4619 | Acc: 76.68% | LR: 5.0e-03\n",
            "  Batch  50/125 | Loss: 0.7306 | Acc: 76.35% | LR: 5.0e-03\n",
            "  Batch  75/125 | Loss: 1.0753 | Acc: 75.00% | LR: 5.0e-03\n",
            "  Batch 100/125 | Loss: 0.5159 | Acc: 75.74% | LR: 5.0e-03\n",
            "📊 Epoch 10 Summary:\n",
            "  Train: Loss=0.6560, Acc=75.73%\n",
            "  Val:   Loss=1.3259, Acc=65.30%\n",
            "  💾 New best model saved! Val Acc: 65.30%\n",
            "\n",
            "Epoch 11/25\n",
            "  Batch   0/125 | Loss: 1.2811 | Acc: 62.50% | LR: 5.0e-03\n",
            "  Batch  25/125 | Loss: 0.7058 | Acc: 68.75% | LR: 5.0e-03\n",
            "  Batch  50/125 | Loss: 0.2901 | Acc: 74.02% | LR: 5.0e-03\n",
            "  Batch  75/125 | Loss: 1.4985 | Acc: 73.93% | LR: 5.0e-03\n",
            "  Batch 100/125 | Loss: 0.3953 | Acc: 74.32% | LR: 5.0e-03\n",
            "📊 Epoch 11 Summary:\n",
            "  Train: Loss=0.6648, Acc=74.87%\n",
            "  Val:   Loss=1.0913, Acc=65.75%\n",
            "  💾 New best model saved! Val Acc: 65.75%\n",
            "\n",
            "Epoch 12/25\n",
            "  Batch   0/125 | Loss: 0.7241 | Acc: 68.75% | LR: 5.0e-03\n",
            "  Batch  25/125 | Loss: 0.5336 | Acc: 72.36% | LR: 5.0e-03\n",
            "  Batch  50/125 | Loss: 1.0124 | Acc: 73.77% | LR: 5.0e-03\n",
            "  Batch  75/125 | Loss: 0.2197 | Acc: 75.08% | LR: 5.0e-03\n",
            "  Batch 100/125 | Loss: 0.6540 | Acc: 75.74% | LR: 5.0e-03\n",
            "📊 Epoch 12 Summary:\n",
            "  Train: Loss=0.6291, Acc=75.98%\n",
            "  Val:   Loss=1.1912, Acc=61.42%\n",
            "  ⏳ Patience: 1/8\n",
            "\n",
            "Epoch 13/25\n",
            "  Batch   0/125 | Loss: 0.6724 | Acc: 68.75% | LR: 5.0e-03\n",
            "  Batch  25/125 | Loss: 0.3703 | Acc: 76.20% | LR: 5.0e-03\n",
            "  Batch  50/125 | Loss: 0.5911 | Acc: 76.47% | LR: 5.0e-03\n",
            "  Batch  75/125 | Loss: 0.5184 | Acc: 76.40% | LR: 5.0e-03\n",
            "  Batch 100/125 | Loss: 0.2704 | Acc: 76.79% | LR: 5.0e-03\n",
            "📊 Epoch 13 Summary:\n",
            "  Train: Loss=0.6081, Acc=77.34%\n",
            "  Val:   Loss=1.7471, Acc=65.30%\n",
            "  ⏳ Patience: 2/8\n",
            "\n",
            "Epoch 14/25\n",
            "  Batch   0/125 | Loss: 0.1660 | Acc: 93.75% | LR: 5.0e-03\n",
            "  Batch  25/125 | Loss: 0.5429 | Acc: 77.64% | LR: 5.0e-03\n",
            "  Batch  50/125 | Loss: 0.9302 | Acc: 79.17% | LR: 5.0e-03\n",
            "  Batch  75/125 | Loss: 0.7707 | Acc: 79.03% | LR: 5.0e-03\n",
            "  Batch 100/125 | Loss: 0.8580 | Acc: 78.71% | LR: 5.0e-03\n",
            "📊 Epoch 14 Summary:\n",
            "  Train: Loss=0.5928, Acc=78.85%\n",
            "  Val:   Loss=1.2261, Acc=71.92%\n",
            "  💾 New best model saved! Val Acc: 71.92%\n",
            "\n",
            "Epoch 15/25\n",
            "  Batch   0/125 | Loss: 0.6414 | Acc: 81.25% | LR: 5.0e-03\n",
            "  Batch  25/125 | Loss: 0.4088 | Acc: 75.96% | LR: 5.0e-03\n",
            "  Batch  50/125 | Loss: 0.2658 | Acc: 76.84% | LR: 5.0e-03\n",
            "  Batch  75/125 | Loss: 0.5709 | Acc: 78.12% | LR: 5.0e-03\n",
            "  Batch 100/125 | Loss: 0.5246 | Acc: 78.47% | LR: 5.0e-03\n",
            "📊 Epoch 15 Summary:\n",
            "  Train: Loss=0.5588, Acc=78.75%\n",
            "  Val:   Loss=1.1590, Acc=61.87%\n",
            "  ⏳ Patience: 1/8\n",
            "\n",
            "Epoch 16/25\n",
            "  Batch   0/125 | Loss: 0.6283 | Acc: 81.25% | LR: 5.0e-03\n",
            "  Batch  25/125 | Loss: 0.1513 | Acc: 82.93% | LR: 5.0e-03\n",
            "  Batch  50/125 | Loss: 0.6329 | Acc: 81.99% | LR: 5.0e-03\n",
            "  Batch  75/125 | Loss: 0.4089 | Acc: 79.93% | LR: 5.0e-03\n",
            "  Batch 100/125 | Loss: 0.9788 | Acc: 79.46% | LR: 5.0e-03\n",
            "📊 Epoch 16 Summary:\n",
            "  Train: Loss=0.5398, Acc=79.71%\n",
            "  Val:   Loss=1.4088, Acc=54.57%\n",
            "  ⏳ Patience: 2/8\n",
            "\n",
            "Epoch 17/25\n",
            "  Batch   0/125 | Loss: 0.5629 | Acc: 75.00% | LR: 5.0e-03\n",
            "  Batch  25/125 | Loss: 0.5139 | Acc: 82.69% | LR: 5.0e-03\n",
            "  Batch  50/125 | Loss: 0.7819 | Acc: 80.39% | LR: 5.0e-03\n",
            "  Batch  75/125 | Loss: 0.2701 | Acc: 80.43% | LR: 5.0e-03\n",
            "  Batch 100/125 | Loss: 0.3611 | Acc: 79.58% | LR: 5.0e-03\n",
            "📊 Epoch 17 Summary:\n",
            "  Train: Loss=0.5719, Acc=79.05%\n",
            "  Val:   Loss=0.9496, Acc=65.30%\n",
            "  ⏳ Patience: 3/8\n",
            "\n",
            "Epoch 18/25\n",
            "  Batch   0/125 | Loss: 0.5307 | Acc: 68.75% | LR: 5.0e-03\n",
            "  Batch  25/125 | Loss: 0.4050 | Acc: 79.57% | LR: 5.0e-03\n",
            "  Batch  50/125 | Loss: 0.4116 | Acc: 79.04% | LR: 5.0e-03\n",
            "  Batch  75/125 | Loss: 0.5069 | Acc: 79.77% | LR: 5.0e-03\n",
            "  Batch 100/125 | Loss: 0.3695 | Acc: 80.07% | LR: 5.0e-03\n",
            "📊 Epoch 18 Summary:\n",
            "  Train: Loss=0.5660, Acc=79.86%\n",
            "  Val:   Loss=0.9955, Acc=70.09%\n",
            "  ⏳ Patience: 4/8\n",
            "\n",
            "Epoch 19/25\n",
            "  Batch   0/125 | Loss: 0.2401 | Acc: 93.75% | LR: 2.5e-03\n",
            "  Batch  25/125 | Loss: 0.3816 | Acc: 80.05% | LR: 2.5e-03\n",
            "  Batch  50/125 | Loss: 0.5985 | Acc: 81.13% | LR: 2.5e-03\n",
            "  Batch  75/125 | Loss: 0.2642 | Acc: 82.89% | LR: 2.5e-03\n",
            "  Batch 100/125 | Loss: 0.4863 | Acc: 82.05% | LR: 2.5e-03\n",
            "📊 Epoch 19 Summary:\n",
            "  Train: Loss=0.4703, Acc=82.12%\n",
            "  Val:   Loss=1.3125, Acc=64.16%\n",
            "  ⏳ Patience: 5/8\n",
            "\n",
            "Epoch 20/25\n",
            "  Batch   0/125 | Loss: 0.1369 | Acc: 93.75% | LR: 2.5e-03\n",
            "  Batch  25/125 | Loss: 0.6747 | Acc: 82.69% | LR: 2.5e-03\n",
            "  Batch  50/125 | Loss: 0.0879 | Acc: 83.82% | LR: 2.5e-03\n",
            "  Batch  75/125 | Loss: 0.3545 | Acc: 83.14% | LR: 2.5e-03\n",
            "  Batch 100/125 | Loss: 1.0192 | Acc: 83.91% | LR: 2.5e-03\n",
            "📊 Epoch 20 Summary:\n",
            "  Train: Loss=0.4527, Acc=83.69%\n",
            "  Val:   Loss=1.1099, Acc=66.67%\n",
            "  ⏳ Patience: 6/8\n",
            "\n",
            "Epoch 21/25\n",
            "  Batch   0/125 | Loss: 0.1396 | Acc: 93.75% | LR: 2.5e-03\n",
            "  Batch  25/125 | Loss: 0.5002 | Acc: 85.10% | LR: 2.5e-03\n",
            "  Batch  50/125 | Loss: 0.4978 | Acc: 83.95% | LR: 2.5e-03\n",
            "  Batch  75/125 | Loss: 0.2349 | Acc: 84.95% | LR: 2.5e-03\n",
            "  Batch 100/125 | Loss: 0.8234 | Acc: 85.46% | LR: 2.5e-03\n",
            "📊 Epoch 21 Summary:\n",
            "  Train: Loss=0.4351, Acc=84.99%\n",
            "  Val:   Loss=1.2458, Acc=71.00%\n",
            "  ⏳ Patience: 7/8\n",
            "\n",
            "Epoch 22/25\n",
            "  Batch   0/125 | Loss: 0.0883 | Acc: 93.75% | LR: 2.5e-03\n",
            "  Batch  25/125 | Loss: 0.5624 | Acc: 85.82% | LR: 2.5e-03\n",
            "  Batch  50/125 | Loss: 0.1871 | Acc: 84.31% | LR: 2.5e-03\n",
            "  Batch  75/125 | Loss: 0.5144 | Acc: 84.46% | LR: 2.5e-03\n",
            "  Batch 100/125 | Loss: 0.2037 | Acc: 85.40% | LR: 2.5e-03\n",
            "📊 Epoch 22 Summary:\n",
            "  Train: Loss=0.4387, Acc=84.99%\n",
            "  Val:   Loss=1.4957, Acc=62.79%\n",
            "  ⏳ Patience: 8/8\n",
            "\n",
            "🛑 Early stopping! Best Val Acc: 71.92%\n",
            "\n",
            "✅ Training completed! Best validation accuracy: 71.92%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# MEMORY CLEANUP AND OPTIMIZATION\n",
        "# =====================================================\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "def cleanup_memory():\n",
        "    \"\"\"Comprehensive memory cleanup\"\"\"\n",
        "    print(\"🧹 CLEANING UP MEMORY...\")\n",
        "\n",
        "    # List of large models/variables to delete\n",
        "    models_to_cleanup = [\n",
        "        'aggressive_model',\n",
        "        'stable_aggressive_model',\n",
        "        'model',           # Original CNN_LSTM model\n",
        "        'rhythm_model',    # Rhythm augmented model\n",
        "    ]\n",
        "\n",
        "    # Delete models from global scope\n",
        "    for model_name in models_to_cleanup:\n",
        "        if model_name in globals():\n",
        "            print(f\"  Deleting {model_name}...\")\n",
        "            del globals()[model_name]\n",
        "\n",
        "    # Also clean up any large data variables we don't need\n",
        "    data_to_cleanup = [\n",
        "        'improved_data',\n",
        "        'improved_labels',\n",
        "        'full_data',\n",
        "        'full_labels',\n",
        "        'orig_probs',\n",
        "        'rhythm_probs',\n",
        "        'stable_history',\n",
        "        'history'\n",
        "    ]\n",
        "\n",
        "    for data_name in data_to_cleanup:\n",
        "        if data_name in globals():\n",
        "            print(f\"  Deleting {data_name}...\")\n",
        "            del globals()[data_name]\n",
        "\n",
        "    # Force garbage collection\n",
        "    gc.collect()\n",
        "\n",
        "    # Clear GPU cache if using CUDA\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f\"  GPU memory cleared\")\n",
        "\n",
        "    # Clear MPS cache if using MPS (Mac)\n",
        "    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        torch.mps.empty_cache()\n",
        "        print(f\"  MPS memory cleared\")\n",
        "\n",
        "    print(\"✅ Memory cleanup completed!\")\n",
        "\n",
        "    # Show current memory usage if possible\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "        cached = torch.cuda.memory_reserved() / 1024**3\n",
        "        print(f\"📊 GPU Memory: {allocated:.2f}GB allocated, {cached:.2f}GB cached\")\n",
        "\n",
        "# Run cleanup\n",
        "cleanup_memory()\n",
        "\n",
        "# Recreate only the simple model (much smaller)\n",
        "print(\"\\n🎯 Recreating only the simple working model...\")\n",
        "simple_model = SimpleCNN_LSTM_LeakyReLU(num_classes=4, lstm_hidden=128).to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in simple_model.parameters())\n",
        "print(f\"📊 Simple Model: {total_params:,} parameters (~{total_params * 4 / 1024**2:.1f}MB)\")\n",
        "\n",
        "print(\"\\n✅ Memory optimized! Only keeping the working simple model.\")"
      ],
      "metadata": {
        "id": "FVkT16v5-76a",
        "outputId": "4f57cf93-337e-4005-ce66-586ba3f20339",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FVkT16v5-76a",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 CLEANING UP MEMORY...\n",
            "  Deleting aggressive_model...\n",
            "  Deleting stable_aggressive_model...\n",
            "  Deleting improved_data...\n",
            "  Deleting improved_labels...\n",
            "  Deleting full_data...\n",
            "  Deleting full_labels...\n",
            "  Deleting history...\n",
            "  GPU memory cleared\n",
            "✅ Memory cleanup completed!\n",
            "📊 GPU Memory: 2.32GB allocated, 6.91GB cached\n",
            "\n",
            "🎯 Recreating only the simple working model...\n",
            "🎯 Building SIMPLE CNN-LSTM with LeakyReLU...\n",
            "✅ Simple LeakyReLU architecture built!\n",
            "📊 Simple Model: 3,275,236 parameters (~12.5MB)\n",
            "\n",
            "✅ Memory optimized! Only keeping the working simple model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# ADVANCED DATA AUGMENTATION FOR MUSIC\n",
        "# =====================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MusicDataAugmentation:\n",
        "    \"\"\"Advanced music-specific data augmentation\"\"\"\n",
        "\n",
        "    def __init__(self, pitch_shift_range=6, time_stretch_range=0.2, noise_level=0.01):\n",
        "        self.pitch_shift_range = pitch_shift_range  # ±6 semitones\n",
        "        self.time_stretch_range = time_stretch_range  # ±20% time stretch\n",
        "        self.noise_level = noise_level\n",
        "\n",
        "    def pitch_shift(self, piano_roll, shift_amount):\n",
        "        \"\"\"Shift piano roll by semitones\"\"\"\n",
        "        if shift_amount == 0:\n",
        "            return piano_roll\n",
        "\n",
        "        shifted = torch.zeros_like(piano_roll)\n",
        "        if shift_amount > 0:\n",
        "            # Shift up\n",
        "            shifted[shift_amount:, :] = piano_roll[:-shift_amount, :]\n",
        "        else:\n",
        "            # Shift down\n",
        "            shifted[:shift_amount, :] = piano_roll[-shift_amount:, :]\n",
        "\n",
        "        return shifted\n",
        "\n",
        "    def time_stretch(self, piano_roll, stretch_factor):\n",
        "        \"\"\"Time stretch using interpolation\"\"\"\n",
        "        if abs(stretch_factor - 1.0) < 0.01:\n",
        "            return piano_roll\n",
        "\n",
        "        # Add batch and channel dims for interpolation\n",
        "        x = piano_roll.unsqueeze(0).unsqueeze(0)  # (1, 1, 128, T)\n",
        "\n",
        "        # Calculate new width\n",
        "        new_width = int(x.size(-1) * stretch_factor)\n",
        "\n",
        "        # Interpolate along time dimension\n",
        "        stretched = F.interpolate(x, size=(128, new_width), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Remove extra dims and crop/pad to original size\n",
        "        stretched = stretched.squeeze(0).squeeze(0)  # (128, new_width)\n",
        "\n",
        "        original_width = piano_roll.size(-1)\n",
        "        if new_width > original_width:\n",
        "            # Crop from center\n",
        "            start_idx = (new_width - original_width) // 2\n",
        "            stretched = stretched[:, start_idx:start_idx + original_width]\n",
        "        elif new_width < original_width:\n",
        "            # Pad to original size\n",
        "            pad_amount = original_width - new_width\n",
        "            pad_left = pad_amount // 2\n",
        "            pad_right = pad_amount - pad_left\n",
        "            stretched = F.pad(stretched, (pad_left, pad_right))\n",
        "\n",
        "        return stretched\n",
        "\n",
        "    def add_noise(self, piano_roll, noise_level):\n",
        "        \"\"\"Add subtle noise\"\"\"\n",
        "        noise = torch.randn_like(piano_roll) * noise_level\n",
        "        return torch.clamp(piano_roll + noise, 0, 1)\n",
        "\n",
        "    def velocity_variation(self, piano_roll, variation=0.1):\n",
        "        \"\"\"Vary note velocities\"\"\"\n",
        "        # Only affect non-zero entries\n",
        "        mask = piano_roll > 0\n",
        "        variation_factor = 1 + (torch.randn_like(piano_roll) * variation)\n",
        "        varied = piano_roll * variation_factor\n",
        "        varied = torch.clamp(varied, 0, 1)\n",
        "        # Keep zeros as zeros\n",
        "        varied = varied * mask.float()\n",
        "        return varied\n",
        "\n",
        "    def random_mask(self, piano_roll, mask_prob=0.02):\n",
        "        \"\"\"Randomly mask some time steps (like SpecAugment)\"\"\"\n",
        "        mask = torch.rand(piano_roll.size(-1)) > mask_prob\n",
        "        masked = piano_roll.clone()\n",
        "        masked[:, ~mask] = 0\n",
        "        return masked\n",
        "\n",
        "    def __call__(self, piano_roll, apply_prob=0.8):\n",
        "        \"\"\"Apply random augmentations\"\"\"\n",
        "        piano_roll = torch.tensor(piano_roll, dtype=torch.float32)\n",
        "\n",
        "        if torch.rand(1).item() > apply_prob:\n",
        "            return piano_roll.numpy()\n",
        "\n",
        "        # Random pitch shift\n",
        "        if torch.rand(1).item() < 0.5:\n",
        "            shift = torch.randint(-self.pitch_shift_range, self.pitch_shift_range + 1, (1,)).item()\n",
        "            piano_roll = self.pitch_shift(piano_roll, shift)\n",
        "\n",
        "        # Random time stretch\n",
        "        if torch.rand(1).item() < 0.3:\n",
        "            stretch = 1 + (torch.rand(1).item() - 0.5) * 2 * self.time_stretch_range\n",
        "            piano_roll = self.time_stretch(piano_roll, stretch)\n",
        "\n",
        "        # Random velocity variation\n",
        "        if torch.rand(1).item() < 0.4:\n",
        "            piano_roll = self.velocity_variation(piano_roll)\n",
        "\n",
        "        # Random noise\n",
        "        if torch.rand(1).item() < 0.3:\n",
        "            piano_roll = self.add_noise(piano_roll, self.noise_level)\n",
        "\n",
        "        # Random masking\n",
        "        if torch.rand(1).item() < 0.2:\n",
        "            piano_roll = self.random_mask(piano_roll)\n",
        "\n",
        "        return piano_roll.numpy()\n",
        "\n",
        "class AugmentedPianoRollDataset(Dataset):\n",
        "    \"\"\"Dataset with augmentation\"\"\"\n",
        "    def __init__(self, data, labels, augment_train=True):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.augment_train = augment_train\n",
        "        self.augmentor = MusicDataAugmentation() if augment_train else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        piano_roll = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Apply augmentation during training\n",
        "        if self.augmentor is not None:\n",
        "            piano_roll = self.augmentor(piano_roll)\n",
        "\n",
        "        # Convert to tensor and add channel dimension\n",
        "        piano_roll = torch.tensor(piano_roll, dtype=torch.float32).unsqueeze(0)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return piano_roll, label\n",
        "\n",
        "# Create augmented datasets\n",
        "print(\"🎵 Creating augmented datasets...\")\n",
        "train_dataset_aug = AugmentedPianoRollDataset(\n",
        "    tracked_data[train_mask],\n",
        "    tracked_labels[train_mask],\n",
        "    augment_train=True\n",
        ")\n",
        "\n",
        "val_dataset_clean = AugmentedPianoRollDataset(\n",
        "    tracked_data[val_mask],\n",
        "    tracked_labels[val_mask],\n",
        "    augment_train=False  # No augmentation for validation\n",
        ")\n",
        "\n",
        "# Create new data loaders\n",
        "train_loader_aug = DataLoader(train_dataset_aug, batch_size=16, shuffle=True, num_workers=0)\n",
        "val_loader_clean = DataLoader(val_dataset_clean, batch_size=16, shuffle=False, num_workers=0)\n",
        "\n",
        "print(\"✅ Augmented datasets created!\")\n",
        "print(f\"• Train samples: {len(train_dataset_aug)} (with augmentation)\")\n",
        "print(f\"• Val samples: {len(val_dataset_clean)} (clean)\")\n",
        "print(\"• Augmentations: Pitch shift, time stretch, velocity variation, noise, masking\")"
      ],
      "metadata": {
        "id": "MaIDcthvCFwy",
        "outputId": "534f0697-ef7f-490a-ba05-607ea83b343b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MaIDcthvCFwy",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎵 Creating augmented datasets...\n",
            "✅ Augmented datasets created!\n",
            "• Train samples: 1986 (with augmentation)\n",
            "• Val samples: 438 (clean)\n",
            "• Augmentations: Pitch shift, time stretch, velocity variation, noise, masking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# TRAIN SIMPLE MODEL WITH DATA AUGMENTATION\n",
        "# =====================================================\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_simple_model_with_augmentation():\n",
        "    \"\"\"Train the simple model with data augmentation for better generalization\"\"\"\n",
        "\n",
        "    print(\"🎵 TRAINING SIMPLE MODEL WITH DATA AUGMENTATION...\")\n",
        "    print(\"Expected improvements:\")\n",
        "    print(\"• Better generalization from pitch shifting\")\n",
        "    print(\"• Robustness from time stretching\")\n",
        "    print(\"• Noise tolerance from velocity variation\")\n",
        "    print(\"• Overfitting reduction from random masking\")\n",
        "\n",
        "    # Use the proven simple model architecture\n",
        "    aug_model = SimpleCNN_LSTM_LeakyReLU(num_classes=4, lstm_hidden=128).to(device)\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    optimizer = optim.AdamW(aug_model.parameters(), lr=3e-3, weight_decay=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', factor=0.7, patience=4, verbose=True\n",
        "    )\n",
        "\n",
        "    # Training setup\n",
        "    epochs = 30\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    patience = 10\n",
        "\n",
        "    print(f\"\\n🎯 Augmentation Training Configuration:\")\n",
        "    print(f\"• Model: Simple CNN-LSTM with LeakyReLU\")\n",
        "    print(f\"• Data augmentation: Pitch shift, time stretch, velocity, noise, masking\")\n",
        "    print(f\"• Learning rate: 3e-3 with ReduceLROnPlateau\")\n",
        "    print(f\"• Class weights: {class_weights}\")\n",
        "    print(f\"• Epochs: {epochs} (early stopping patience: {patience})\")\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ==========================================\n",
        "        # TRAINING PHASE WITH AUGMENTATION\n",
        "        # ==========================================\n",
        "        aug_model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader_aug):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = aug_model(data)\n",
        "            loss = criterion(outputs, target)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(aug_model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "            if batch_idx % 25 == 0:\n",
        "                print(f\"  Batch {batch_idx:3d}/{len(train_loader_aug)} | \"\n",
        "                      f\"Loss: {loss.item():.4f} | \"\n",
        "                      f\"Acc: {100.*correct/total:.2f}% | \"\n",
        "                      f\"LR: {optimizer.param_groups[0]['lr']:.1e}\")\n",
        "\n",
        "        # ==========================================\n",
        "        # VALIDATION PHASE (CLEAN DATA)\n",
        "        # ==========================================\n",
        "        aug_model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader_clean:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                outputs = aug_model(data)\n",
        "                loss = criterion(outputs, target)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += target.size(0)\n",
        "                val_correct += (predicted == target).sum().item()\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_loss = total_loss / len(train_loader_aug)\n",
        "        train_acc = 100. * correct / total\n",
        "        val_loss = val_loss / len(val_loader_clean)\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        print(f\"📊 Epoch {epoch+1} Summary:\")\n",
        "        print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
        "        print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save({\n",
        "                'model_state_dict': aug_model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'epoch': epoch,\n",
        "                'history': history\n",
        "            }, 'saved_models/simple_augmented_model.pth')\n",
        "            print(f\"  💾 New best model saved! Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "            if val_acc > 75.0:  # Great performance threshold\n",
        "                print(f\"  🎉 Excellent performance reached: {val_acc:.2f}%!\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"  ⏳ Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\n🛑 Early stopping! Best Val Acc: {best_val_acc:.2f}%\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\n✅ Augmentation training completed!\")\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "    print(f\"Improvement over baseline: {best_val_acc - 71.92:.2f} percentage points\")\n",
        "\n",
        "    return aug_model, history, best_val_acc\n",
        "\n",
        "# Run augmented training\n",
        "print(\"🚀 Starting training with data augmentation...\")\n",
        "aug_model, aug_history, aug_best_acc = train_simple_model_with_augmentation()"
      ],
      "metadata": {
        "id": "Nd8BSRNjCttK",
        "outputId": "45633a3b-b779-48b9-8f1c-7c03299b624d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Nd8BSRNjCttK",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting training with data augmentation...\n",
            "🎵 TRAINING SIMPLE MODEL WITH DATA AUGMENTATION...\n",
            "Expected improvements:\n",
            "• Better generalization from pitch shifting\n",
            "• Robustness from time stretching\n",
            "• Noise tolerance from velocity variation\n",
            "• Overfitting reduction from random masking\n",
            "🎯 Building SIMPLE CNN-LSTM with LeakyReLU...\n",
            "✅ Simple LeakyReLU architecture built!\n",
            "\n",
            "🎯 Augmentation Training Configuration:\n",
            "• Model: Simple CNN-LSTM with LeakyReLU\n",
            "• Data augmentation: Pitch shift, time stretch, velocity, noise, masking\n",
            "• Learning rate: 3e-3 with ReduceLROnPlateau\n",
            "• Class weights: tensor([0.7927, 0.7847, 1.2697, 1.4781], device='cuda:0')\n",
            "• Epochs: 30 (early stopping patience: 10)\n",
            "\n",
            "Epoch 1/30\n",
            "  Batch   0/125 | Loss: 1.3886 | Acc: 12.50% | LR: 3.0e-03\n",
            "  Batch  25/125 | Loss: 1.3647 | Acc: 22.36% | LR: 3.0e-03\n",
            "  Batch  50/125 | Loss: 1.3882 | Acc: 23.41% | LR: 3.0e-03\n",
            "  Batch  75/125 | Loss: 1.4404 | Acc: 22.94% | LR: 3.0e-03\n",
            "  Batch 100/125 | Loss: 1.3819 | Acc: 23.21% | LR: 3.0e-03\n",
            "📊 Epoch 1 Summary:\n",
            "  Train: Loss=1.3901, Acc=24.17%\n",
            "  Val:   Loss=1.3857, Acc=16.44%\n",
            "  💾 New best model saved! Val Acc: 16.44%\n",
            "\n",
            "Epoch 2/30\n",
            "  Batch   0/125 | Loss: 1.3876 | Acc: 12.50% | LR: 3.0e-03\n",
            "  Batch  25/125 | Loss: 2.2751 | Acc: 31.49% | LR: 3.0e-03\n",
            "  Batch  50/125 | Loss: 2.1782 | Acc: 32.11% | LR: 3.0e-03\n",
            "  Batch  75/125 | Loss: 1.4166 | Acc: 31.09% | LR: 3.0e-03\n",
            "  Batch 100/125 | Loss: 1.3651 | Acc: 31.81% | LR: 3.0e-03\n",
            "📊 Epoch 2 Summary:\n",
            "  Train: Loss=1.3852, Acc=31.47%\n",
            "  Val:   Loss=1.3684, Acc=15.75%\n",
            "  ⏳ Patience: 1/10\n",
            "\n",
            "Epoch 3/30\n",
            "  Batch   0/125 | Loss: 1.3959 | Acc: 6.25% | LR: 3.0e-03\n",
            "  Batch  25/125 | Loss: 1.3780 | Acc: 18.51% | LR: 3.0e-03\n",
            "  Batch  50/125 | Loss: 1.3786 | Acc: 22.79% | LR: 3.0e-03\n",
            "  Batch  75/125 | Loss: 1.3346 | Acc: 24.10% | LR: 3.0e-03\n",
            "  Batch 100/125 | Loss: 1.4027 | Acc: 24.81% | LR: 3.0e-03\n",
            "📊 Epoch 3 Summary:\n",
            "  Train: Loss=1.3637, Acc=26.84%\n",
            "  Val:   Loss=1.2929, Acc=41.55%\n",
            "  💾 New best model saved! Val Acc: 41.55%\n",
            "\n",
            "Epoch 4/30\n",
            "  Batch   0/125 | Loss: 1.3133 | Acc: 37.50% | LR: 3.0e-03\n",
            "  Batch  25/125 | Loss: 1.2349 | Acc: 31.49% | LR: 3.0e-03\n",
            "  Batch  50/125 | Loss: 1.3269 | Acc: 32.60% | LR: 3.0e-03\n",
            "  Batch  75/125 | Loss: 1.5596 | Acc: 31.58% | LR: 3.0e-03\n",
            "  Batch 100/125 | Loss: 1.2739 | Acc: 32.12% | LR: 3.0e-03\n",
            "📊 Epoch 4 Summary:\n",
            "  Train: Loss=1.3587, Acc=32.43%\n",
            "  Val:   Loss=1.1505, Acc=42.24%\n",
            "  💾 New best model saved! Val Acc: 42.24%\n",
            "\n",
            "Epoch 5/30\n",
            "  Batch   0/125 | Loss: 1.3892 | Acc: 12.50% | LR: 3.0e-03\n",
            "  Batch  25/125 | Loss: 1.2352 | Acc: 34.13% | LR: 3.0e-03\n",
            "  Batch  50/125 | Loss: 1.5271 | Acc: 35.05% | LR: 3.0e-03\n",
            "  Batch  75/125 | Loss: 1.1833 | Acc: 35.77% | LR: 3.0e-03\n",
            "  Batch 100/125 | Loss: 1.3425 | Acc: 35.95% | LR: 3.0e-03\n",
            "📊 Epoch 5 Summary:\n",
            "  Train: Loss=1.2910, Acc=36.40%\n",
            "  Val:   Loss=1.2580, Acc=40.41%\n",
            "  ⏳ Patience: 1/10\n",
            "\n",
            "Epoch 6/30\n",
            "  Batch   0/125 | Loss: 1.1777 | Acc: 31.25% | LR: 3.0e-03\n",
            "  Batch  25/125 | Loss: 1.3000 | Acc: 35.82% | LR: 3.0e-03\n",
            "  Batch  50/125 | Loss: 1.3313 | Acc: 35.78% | LR: 3.0e-03\n",
            "  Batch  75/125 | Loss: 1.1976 | Acc: 35.03% | LR: 3.0e-03\n",
            "  Batch 100/125 | Loss: 1.1059 | Acc: 34.72% | LR: 3.0e-03\n",
            "📊 Epoch 6 Summary:\n",
            "  Train: Loss=1.2504, Acc=35.10%\n",
            "  Val:   Loss=1.1514, Acc=49.54%\n",
            "  💾 New best model saved! Val Acc: 49.54%\n",
            "\n",
            "Epoch 7/30\n",
            "  Batch   0/125 | Loss: 1.1973 | Acc: 50.00% | LR: 3.0e-03\n",
            "  Batch  25/125 | Loss: 1.2518 | Acc: 30.05% | LR: 3.0e-03\n",
            "  Batch  50/125 | Loss: 0.9348 | Acc: 34.19% | LR: 3.0e-03\n",
            "  Batch  75/125 | Loss: 1.2188 | Acc: 35.94% | LR: 3.0e-03\n",
            "  Batch 100/125 | Loss: 1.2291 | Acc: 36.32% | LR: 3.0e-03\n",
            "📊 Epoch 7 Summary:\n",
            "  Train: Loss=1.2210, Acc=37.11%\n",
            "  Val:   Loss=0.9769, Acc=55.48%\n",
            "  💾 New best model saved! Val Acc: 55.48%\n",
            "\n",
            "Epoch 8/30\n",
            "  Batch   0/125 | Loss: 1.1831 | Acc: 25.00% | LR: 3.0e-03\n",
            "  Batch  25/125 | Loss: 1.3983 | Acc: 36.54% | LR: 3.0e-03\n",
            "  Batch  50/125 | Loss: 1.4901 | Acc: 36.15% | LR: 3.0e-03\n",
            "  Batch  75/125 | Loss: 1.1062 | Acc: 35.77% | LR: 3.0e-03\n",
            "  Batch 100/125 | Loss: 1.0861 | Acc: 35.58% | LR: 3.0e-03\n",
            "📊 Epoch 8 Summary:\n",
            "  Train: Loss=1.2163, Acc=36.15%\n",
            "  Val:   Loss=1.1962, Acc=43.61%\n",
            "  ⏳ Patience: 1/10\n",
            "\n",
            "Epoch 9/30\n",
            "  Batch   0/125 | Loss: 1.2845 | Acc: 31.25% | LR: 3.0e-03\n",
            "  Batch  25/125 | Loss: 1.4118 | Acc: 39.90% | LR: 3.0e-03\n",
            "  Batch  50/125 | Loss: 1.2636 | Acc: 41.18% | LR: 3.0e-03\n",
            "  Batch  75/125 | Loss: 1.4413 | Acc: 42.19% | LR: 3.0e-03\n",
            "  Batch 100/125 | Loss: 1.0059 | Acc: 42.70% | LR: 3.0e-03\n",
            "📊 Epoch 9 Summary:\n",
            "  Train: Loss=1.1691, Acc=40.94%\n",
            "  Val:   Loss=0.9671, Acc=54.79%\n",
            "  ⏳ Patience: 2/10\n",
            "\n",
            "Epoch 10/30\n",
            "  Batch   0/125 | Loss: 1.2219 | Acc: 43.75% | LR: 3.0e-03\n",
            "  Batch  25/125 | Loss: 1.3165 | Acc: 39.90% | LR: 3.0e-03\n",
            "  Batch  50/125 | Loss: 1.0922 | Acc: 41.42% | LR: 3.0e-03\n",
            "  Batch  75/125 | Loss: 0.9912 | Acc: 40.62% | LR: 3.0e-03\n",
            "  Batch 100/125 | Loss: 1.3161 | Acc: 40.10% | LR: 3.0e-03\n",
            "📊 Epoch 10 Summary:\n",
            "  Train: Loss=1.2208, Acc=38.87%\n",
            "  Val:   Loss=1.0016, Acc=47.72%\n",
            "  ⏳ Patience: 3/10\n",
            "\n",
            "Epoch 11/30\n",
            "  Batch   0/125 | Loss: 1.1712 | Acc: 37.50% | LR: 3.0e-03\n",
            "  Batch  25/125 | Loss: 1.1616 | Acc: 34.38% | LR: 3.0e-03\n",
            "  Batch  50/125 | Loss: 1.0722 | Acc: 39.46% | LR: 3.0e-03\n",
            "  Batch  75/125 | Loss: 0.9202 | Acc: 40.54% | LR: 3.0e-03\n",
            "  Batch 100/125 | Loss: 1.1403 | Acc: 39.36% | LR: 3.0e-03\n",
            "📊 Epoch 11 Summary:\n",
            "  Train: Loss=1.1816, Acc=39.33%\n",
            "  Val:   Loss=0.9879, Acc=52.28%\n",
            "  ⏳ Patience: 4/10\n",
            "\n",
            "Epoch 12/30\n",
            "  Batch   0/125 | Loss: 1.8851 | Acc: 43.75% | LR: 3.0e-03\n",
            "  Batch  25/125 | Loss: 0.9117 | Acc: 45.43% | LR: 3.0e-03\n",
            "  Batch  50/125 | Loss: 1.1787 | Acc: 42.28% | LR: 3.0e-03\n",
            "  Batch  75/125 | Loss: 0.8848 | Acc: 42.93% | LR: 3.0e-03\n",
            "  Batch 100/125 | Loss: 1.4627 | Acc: 42.39% | LR: 3.0e-03\n",
            "📊 Epoch 12 Summary:\n",
            "  Train: Loss=1.1897, Acc=41.54%\n",
            "  Val:   Loss=1.0763, Acc=45.43%\n",
            "  ⏳ Patience: 5/10\n",
            "\n",
            "Epoch 13/30\n",
            "  Batch   0/125 | Loss: 1.0404 | Acc: 37.50% | LR: 2.1e-03\n",
            "  Batch  25/125 | Loss: 1.1327 | Acc: 40.14% | LR: 2.1e-03\n",
            "  Batch  50/125 | Loss: 0.7640 | Acc: 41.91% | LR: 2.1e-03\n",
            "  Batch  75/125 | Loss: 0.9011 | Acc: 40.62% | LR: 2.1e-03\n",
            "  Batch 100/125 | Loss: 0.8177 | Acc: 42.02% | LR: 2.1e-03\n",
            "📊 Epoch 13 Summary:\n",
            "  Train: Loss=1.1326, Acc=43.10%\n",
            "  Val:   Loss=1.4832, Acc=49.54%\n",
            "  ⏳ Patience: 6/10\n",
            "\n",
            "Epoch 14/30\n",
            "  Batch   0/125 | Loss: 0.9405 | Acc: 43.75% | LR: 2.1e-03\n",
            "  Batch  25/125 | Loss: 0.8532 | Acc: 42.79% | LR: 2.1e-03\n",
            "  Batch  50/125 | Loss: 1.3058 | Acc: 41.67% | LR: 2.1e-03\n",
            "  Batch  75/125 | Loss: 0.8467 | Acc: 42.35% | LR: 2.1e-03\n",
            "  Batch 100/125 | Loss: 1.4924 | Acc: 41.77% | LR: 2.1e-03\n",
            "📊 Epoch 14 Summary:\n",
            "  Train: Loss=1.1331, Acc=42.15%\n",
            "  Val:   Loss=0.9606, Acc=57.31%\n",
            "  💾 New best model saved! Val Acc: 57.31%\n",
            "\n",
            "Epoch 15/30\n",
            "  Batch   0/125 | Loss: 1.0733 | Acc: 25.00% | LR: 2.1e-03\n",
            "  Batch  25/125 | Loss: 1.0767 | Acc: 38.46% | LR: 2.1e-03\n",
            "  Batch  50/125 | Loss: 1.0523 | Acc: 39.58% | LR: 2.1e-03\n",
            "  Batch  75/125 | Loss: 1.1570 | Acc: 43.17% | LR: 2.1e-03\n",
            "  Batch 100/125 | Loss: 1.2492 | Acc: 43.01% | LR: 2.1e-03\n",
            "📊 Epoch 15 Summary:\n",
            "  Train: Loss=1.0798, Acc=43.76%\n",
            "  Val:   Loss=0.9849, Acc=55.94%\n",
            "  ⏳ Patience: 1/10\n",
            "\n",
            "Epoch 16/30\n",
            "  Batch   0/125 | Loss: 0.6621 | Acc: 68.75% | LR: 2.1e-03\n",
            "  Batch  25/125 | Loss: 1.0286 | Acc: 41.59% | LR: 2.1e-03\n",
            "  Batch  50/125 | Loss: 0.9784 | Acc: 43.26% | LR: 2.1e-03\n",
            "  Batch  75/125 | Loss: 0.9160 | Acc: 44.57% | LR: 2.1e-03\n",
            "  Batch 100/125 | Loss: 1.2304 | Acc: 44.43% | LR: 2.1e-03\n",
            "📊 Epoch 16 Summary:\n",
            "  Train: Loss=1.0942, Acc=44.21%\n",
            "  Val:   Loss=0.9948, Acc=54.34%\n",
            "  ⏳ Patience: 2/10\n",
            "\n",
            "Epoch 17/30\n",
            "  Batch   0/125 | Loss: 1.0063 | Acc: 43.75% | LR: 2.1e-03\n",
            "  Batch  25/125 | Loss: 0.9019 | Acc: 44.47% | LR: 2.1e-03\n",
            "  Batch  50/125 | Loss: 1.6257 | Acc: 44.24% | LR: 2.1e-03\n",
            "  Batch  75/125 | Loss: 1.2356 | Acc: 45.81% | LR: 2.1e-03\n",
            "  Batch 100/125 | Loss: 1.1846 | Acc: 46.35% | LR: 2.1e-03\n",
            "📊 Epoch 17 Summary:\n",
            "  Train: Loss=1.0915, Acc=45.82%\n",
            "  Val:   Loss=0.9462, Acc=56.85%\n",
            "  ⏳ Patience: 3/10\n",
            "\n",
            "Epoch 18/30\n",
            "  Batch   0/125 | Loss: 1.7199 | Acc: 50.00% | LR: 2.1e-03\n",
            "  Batch  25/125 | Loss: 1.1460 | Acc: 49.04% | LR: 2.1e-03\n",
            "  Batch  50/125 | Loss: 0.9342 | Acc: 47.30% | LR: 2.1e-03\n",
            "  Batch  75/125 | Loss: 1.0949 | Acc: 46.63% | LR: 2.1e-03\n",
            "  Batch 100/125 | Loss: 1.1929 | Acc: 45.54% | LR: 2.1e-03\n",
            "📊 Epoch 18 Summary:\n",
            "  Train: Loss=1.0734, Acc=46.02%\n",
            "  Val:   Loss=1.0800, Acc=58.68%\n",
            "  💾 New best model saved! Val Acc: 58.68%\n",
            "\n",
            "Epoch 19/30\n",
            "  Batch   0/125 | Loss: 0.9737 | Acc: 50.00% | LR: 2.1e-03\n",
            "  Batch  25/125 | Loss: 0.7333 | Acc: 43.99% | LR: 2.1e-03\n",
            "  Batch  50/125 | Loss: 1.0898 | Acc: 45.47% | LR: 2.1e-03\n",
            "  Batch  75/125 | Loss: 1.1415 | Acc: 47.20% | LR: 2.1e-03\n",
            "  Batch 100/125 | Loss: 1.0203 | Acc: 47.83% | LR: 2.1e-03\n",
            "📊 Epoch 19 Summary:\n",
            "  Train: Loss=1.0378, Acc=48.24%\n",
            "  Val:   Loss=1.2269, Acc=54.79%\n",
            "  ⏳ Patience: 1/10\n",
            "\n",
            "Epoch 20/30\n",
            "  Batch   0/125 | Loss: 1.2476 | Acc: 43.75% | LR: 2.1e-03\n",
            "  Batch  25/125 | Loss: 1.5831 | Acc: 48.32% | LR: 2.1e-03\n",
            "  Batch  50/125 | Loss: 0.8076 | Acc: 48.65% | LR: 2.1e-03\n",
            "  Batch  75/125 | Loss: 1.0062 | Acc: 48.19% | LR: 2.1e-03\n",
            "  Batch 100/125 | Loss: 1.2514 | Acc: 47.77% | LR: 2.1e-03\n",
            "📊 Epoch 20 Summary:\n",
            "  Train: Loss=1.0683, Acc=47.08%\n",
            "  Val:   Loss=1.2046, Acc=57.08%\n",
            "  ⏳ Patience: 2/10\n",
            "\n",
            "Epoch 21/30\n",
            "  Batch   0/125 | Loss: 0.9178 | Acc: 50.00% | LR: 2.1e-03\n",
            "  Batch  25/125 | Loss: 1.2692 | Acc: 47.36% | LR: 2.1e-03\n",
            "  Batch  50/125 | Loss: 0.8854 | Acc: 47.67% | LR: 2.1e-03\n",
            "  Batch  75/125 | Loss: 0.6474 | Acc: 46.46% | LR: 2.1e-03\n",
            "  Batch 100/125 | Loss: 0.6763 | Acc: 48.14% | LR: 2.1e-03\n",
            "📊 Epoch 21 Summary:\n",
            "  Train: Loss=1.0293, Acc=47.53%\n",
            "  Val:   Loss=1.0199, Acc=66.89%\n",
            "  💾 New best model saved! Val Acc: 66.89%\n",
            "\n",
            "Epoch 22/30\n",
            "  Batch   0/125 | Loss: 1.3232 | Acc: 25.00% | LR: 2.1e-03\n",
            "  Batch  25/125 | Loss: 0.8254 | Acc: 49.28% | LR: 2.1e-03\n",
            "  Batch  50/125 | Loss: 0.8297 | Acc: 49.75% | LR: 2.1e-03\n",
            "  Batch  75/125 | Loss: 1.8315 | Acc: 49.10% | LR: 2.1e-03\n",
            "  Batch 100/125 | Loss: 1.0627 | Acc: 48.39% | LR: 2.1e-03\n",
            "📊 Epoch 22 Summary:\n",
            "  Train: Loss=1.0545, Acc=47.08%\n",
            "  Val:   Loss=0.9881, Acc=64.16%\n",
            "  ⏳ Patience: 1/10\n",
            "\n",
            "Epoch 23/30\n",
            "  Batch   0/125 | Loss: 1.2552 | Acc: 56.25% | LR: 2.1e-03\n",
            "  Batch  25/125 | Loss: 0.5369 | Acc: 50.96% | LR: 2.1e-03\n",
            "  Batch  50/125 | Loss: 1.0472 | Acc: 50.25% | LR: 2.1e-03\n",
            "  Batch  75/125 | Loss: 0.8861 | Acc: 48.85% | LR: 2.1e-03\n",
            "  Batch 100/125 | Loss: 0.8700 | Acc: 48.02% | LR: 2.1e-03\n",
            "📊 Epoch 23 Summary:\n",
            "  Train: Loss=1.0147, Acc=49.90%\n",
            "  Val:   Loss=1.0378, Acc=62.10%\n",
            "  ⏳ Patience: 2/10\n",
            "\n",
            "Epoch 24/30\n",
            "  Batch   0/125 | Loss: 1.1578 | Acc: 31.25% | LR: 2.1e-03\n",
            "  Batch  25/125 | Loss: 1.3893 | Acc: 48.08% | LR: 2.1e-03\n",
            "  Batch  50/125 | Loss: 1.2229 | Acc: 45.96% | LR: 2.1e-03\n",
            "  Batch  75/125 | Loss: 1.1372 | Acc: 45.48% | LR: 2.1e-03\n",
            "  Batch 100/125 | Loss: 1.4608 | Acc: 46.29% | LR: 2.1e-03\n",
            "📊 Epoch 24 Summary:\n",
            "  Train: Loss=1.0517, Acc=46.58%\n",
            "  Val:   Loss=1.1715, Acc=52.28%\n",
            "  ⏳ Patience: 3/10\n",
            "\n",
            "Epoch 25/30\n",
            "  Batch   0/125 | Loss: 1.3944 | Acc: 31.25% | LR: 2.1e-03\n",
            "  Batch  25/125 | Loss: 0.8050 | Acc: 52.40% | LR: 2.1e-03\n",
            "  Batch  50/125 | Loss: 0.8103 | Acc: 49.51% | LR: 2.1e-03\n",
            "  Batch  75/125 | Loss: 0.5170 | Acc: 50.08% | LR: 2.1e-03\n",
            "  Batch 100/125 | Loss: 1.3991 | Acc: 51.11% | LR: 2.1e-03\n",
            "📊 Epoch 25 Summary:\n",
            "  Train: Loss=1.0020, Acc=51.51%\n",
            "  Val:   Loss=0.9874, Acc=67.12%\n",
            "  💾 New best model saved! Val Acc: 67.12%\n",
            "\n",
            "Epoch 26/30\n",
            "  Batch   0/125 | Loss: 0.8181 | Acc: 56.25% | LR: 2.1e-03\n",
            "  Batch  25/125 | Loss: 0.9990 | Acc: 48.56% | LR: 2.1e-03\n",
            "  Batch  50/125 | Loss: 1.3512 | Acc: 48.53% | LR: 2.1e-03\n",
            "  Batch  75/125 | Loss: 0.9603 | Acc: 48.85% | LR: 2.1e-03\n",
            "  Batch 100/125 | Loss: 1.0144 | Acc: 48.51% | LR: 2.1e-03\n",
            "📊 Epoch 26 Summary:\n",
            "  Train: Loss=1.0145, Acc=48.49%\n",
            "  Val:   Loss=1.0812, Acc=60.96%\n",
            "  ⏳ Patience: 1/10\n",
            "\n",
            "Epoch 27/30\n",
            "  Batch   0/125 | Loss: 1.1670 | Acc: 31.25% | LR: 2.1e-03\n",
            "  Batch  25/125 | Loss: 1.1233 | Acc: 49.28% | LR: 2.1e-03\n",
            "  Batch  50/125 | Loss: 0.9447 | Acc: 47.79% | LR: 2.1e-03\n",
            "  Batch  75/125 | Loss: 1.5527 | Acc: 47.78% | LR: 2.1e-03\n",
            "  Batch 100/125 | Loss: 0.9833 | Acc: 48.76% | LR: 2.1e-03\n",
            "📊 Epoch 27 Summary:\n",
            "  Train: Loss=1.0255, Acc=49.45%\n",
            "  Val:   Loss=0.9548, Acc=71.23%\n",
            "  💾 New best model saved! Val Acc: 71.23%\n",
            "\n",
            "Epoch 28/30\n",
            "  Batch   0/125 | Loss: 0.8447 | Acc: 56.25% | LR: 2.1e-03\n",
            "  Batch  25/125 | Loss: 1.1047 | Acc: 51.68% | LR: 2.1e-03\n",
            "  Batch  50/125 | Loss: 0.7492 | Acc: 50.98% | LR: 2.1e-03\n",
            "  Batch  75/125 | Loss: 0.8395 | Acc: 51.81% | LR: 2.1e-03\n",
            "  Batch 100/125 | Loss: 0.8023 | Acc: 51.42% | LR: 2.1e-03\n",
            "📊 Epoch 28 Summary:\n",
            "  Train: Loss=1.0040, Acc=51.36%\n",
            "  Val:   Loss=1.0272, Acc=68.72%\n",
            "  ⏳ Patience: 1/10\n",
            "\n",
            "Epoch 29/30\n",
            "  Batch   0/125 | Loss: 0.7488 | Acc: 56.25% | LR: 2.1e-03\n",
            "  Batch  25/125 | Loss: 0.8763 | Acc: 51.20% | LR: 2.1e-03\n",
            "  Batch  50/125 | Loss: 0.7082 | Acc: 52.94% | LR: 2.1e-03\n",
            "  Batch  75/125 | Loss: 1.2751 | Acc: 50.82% | LR: 2.1e-03\n",
            "  Batch 100/125 | Loss: 1.0007 | Acc: 51.05% | LR: 2.1e-03\n",
            "📊 Epoch 29 Summary:\n",
            "  Train: Loss=0.9837, Acc=50.50%\n",
            "  Val:   Loss=1.0669, Acc=63.47%\n",
            "  ⏳ Patience: 2/10\n",
            "\n",
            "Epoch 30/30\n",
            "  Batch   0/125 | Loss: 0.8764 | Acc: 56.25% | LR: 2.1e-03\n",
            "  Batch  25/125 | Loss: 0.9865 | Acc: 54.09% | LR: 2.1e-03\n",
            "  Batch  50/125 | Loss: 0.8323 | Acc: 49.75% | LR: 2.1e-03\n",
            "  Batch  75/125 | Loss: 0.9596 | Acc: 50.33% | LR: 2.1e-03\n",
            "  Batch 100/125 | Loss: 0.6964 | Acc: 50.31% | LR: 2.1e-03\n",
            "📊 Epoch 30 Summary:\n",
            "  Train: Loss=0.9938, Acc=50.05%\n",
            "  Val:   Loss=0.9701, Acc=71.23%\n",
            "  ⏳ Patience: 3/10\n",
            "\n",
            "✅ Augmentation training completed!\n",
            "Best validation accuracy: 71.23%\n",
            "Improvement over baseline: -0.69 percentage points\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# SIMPLE CNN-LSTM-TRANSFORMER HYBRID\n",
        "# =====================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Lightweight positional encoding\"\"\"\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "class SimpleCNN_LSTM_Transformer(nn.Module):\n",
        "    def __init__(self, num_classes=4, lstm_hidden=128, transformer_dim=256, num_heads=8, num_layers=2):\n",
        "        super(SimpleCNN_LSTM_Transformer, self).__init__()\n",
        "\n",
        "        print(\"🎯 Building SIMPLE CNN-LSTM-Transformer...\")\n",
        "        print(f\"• CNN: 3 blocks (like working model)\")\n",
        "        print(f\"• LSTM: {lstm_hidden} hidden units\")\n",
        "        print(f\"• Transformer: {transformer_dim} dim, {num_heads} heads, {num_layers} layers\")\n",
        "        print(f\"• Designed for stability and performance\")\n",
        "\n",
        "        # ==========================================\n",
        "        # CNN BACKBONE (SAME AS WORKING MODEL)\n",
        "        # ==========================================\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(32, 32, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout2d(0.1),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout2d(0.15),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(128, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout2d(0.2),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "\n",
        "        # ==========================================\n",
        "        # LSTM LAYER (SAME AS WORKING MODEL)\n",
        "        # ==========================================\n",
        "        self.feature_size = 128 * 16\n",
        "        self.lstm_hidden = lstm_hidden\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.feature_size,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=0.2,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # ==========================================\n",
        "        # LIGHTWEIGHT TRANSFORMER\n",
        "        # ==========================================\n",
        "        self.transformer_dim = transformer_dim\n",
        "\n",
        "        # Project LSTM output to transformer dimension\n",
        "        self.lstm_to_transformer = nn.Linear(lstm_hidden * 2, transformer_dim)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(transformer_dim)\n",
        "\n",
        "        # Lightweight transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=transformer_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=transformer_dim * 2,  # Smaller feedforward\n",
        "            dropout=0.1,\n",
        "            activation='relu',  # Stick with ReLU for stability\n",
        "            batch_first=True,\n",
        "            norm_first=False  # Post-norm for stability\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers,\n",
        "            norm=nn.LayerNorm(transformer_dim)\n",
        "        )\n",
        "\n",
        "        # ==========================================\n",
        "        # SIMPLE ATTENTION & CLASSIFICATION\n",
        "        # ==========================================\n",
        "\n",
        "        # Simple global attention pooling\n",
        "        self.attention_pooling = nn.MultiheadAttention(\n",
        "            embed_dim=transformer_dim,\n",
        "            num_heads=4,  # Fewer heads for simplicity\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Simple classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(transformer_dim),\n",
        "            nn.Linear(transformer_dim, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "        print(\"✅ Simple CNN-LSTM-Transformer built!\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # ==========================================\n",
        "        # CNN FEATURE EXTRACTION\n",
        "        # ==========================================\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        # Reshape for LSTM\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = x.contiguous().view(batch_size, x.size(1), -1)\n",
        "\n",
        "        # ==========================================\n",
        "        # LSTM PROCESSING\n",
        "        # ==========================================\n",
        "        lstm_out, _ = self.lstm(x)  # (batch, seq_len, lstm_hidden*2)\n",
        "\n",
        "        # ==========================================\n",
        "        # TRANSFORMER PROCESSING\n",
        "        # ==========================================\n",
        "\n",
        "        # Project to transformer dimension\n",
        "        transformer_input = self.lstm_to_transformer(lstm_out)  # (batch, seq_len, transformer_dim)\n",
        "\n",
        "        # Add positional encoding\n",
        "        seq_len = transformer_input.size(1)\n",
        "        transformer_input = transformer_input.transpose(0, 1)  # (seq_len, batch, transformer_dim)\n",
        "        transformer_input = self.pos_encoding(transformer_input)\n",
        "        transformer_input = transformer_input.transpose(0, 1)  # (batch, seq_len, transformer_dim)\n",
        "\n",
        "        # Transformer encoding\n",
        "        transformer_out = self.transformer_encoder(transformer_input)  # (batch, seq_len, transformer_dim)\n",
        "\n",
        "        # ==========================================\n",
        "        # ATTENTION POOLING & CLASSIFICATION\n",
        "        # ==========================================\n",
        "\n",
        "        # Global attention pooling\n",
        "        attended, _ = self.attention_pooling(\n",
        "            transformer_out, transformer_out, transformer_out\n",
        "        )\n",
        "\n",
        "        # Global average pooling\n",
        "        pooled = torch.mean(attended, dim=1)  # (batch, transformer_dim)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(pooled)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Create the simple transformer model\n",
        "simple_transformer_model = SimpleCNN_LSTM_Transformer(\n",
        "    num_classes=4,\n",
        "    lstm_hidden=128,\n",
        "    transformer_dim=256,  # Reasonable size\n",
        "    num_heads=8,         # Moderate number of heads\n",
        "    num_layers=2         # Just 2 layers for stability\n",
        ").to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in simple_transformer_model.parameters())\n",
        "print(f\"📊 Simple Transformer Model: {total_params:,} parameters\")\n",
        "\n",
        "# Test forward pass\n",
        "test_input = torch.randn(4, 1, 128, 4500).to(device)\n",
        "with torch.no_grad():\n",
        "    output = simple_transformer_model(test_input)\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"✅ Simple transformer model forward pass successful!\")"
      ],
      "metadata": {
        "id": "awVkd2_-DbRy",
        "outputId": "faa12984-9aa3-4af7-afb8-79439b8f6283",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "awVkd2_-DbRy",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Building SIMPLE CNN-LSTM-Transformer...\n",
            "• CNN: 3 blocks (like working model)\n",
            "• LSTM: 128 hidden units\n",
            "• Transformer: 256 dim, 8 heads, 2 layers\n",
            "• Designed for stability and performance\n",
            "✅ Simple CNN-LSTM-Transformer built!\n",
            "📊 Simple Transformer Model: 4,396,260 parameters\n",
            "Output shape: torch.Size([4, 4])\n",
            "✅ Simple transformer model forward pass successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# SIMPLE CNN-LSTM-TRANSFORMER HYBRID\n",
        "# =====================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Lightweight positional encoding\"\"\"\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "class SimpleCNN_LSTM_Transformer(nn.Module):\n",
        "    def __init__(self, num_classes=4, lstm_hidden=128, transformer_dim=256, num_heads=8, num_layers=2):\n",
        "        super(SimpleCNN_LSTM_Transformer, self).__init__()\n",
        "\n",
        "        print(\"🎯 Building SIMPLE CNN-LSTM-Transformer...\")\n",
        "        print(f\"• CNN: 3 blocks (like working model)\")\n",
        "        print(f\"• LSTM: {lstm_hidden} hidden units\")\n",
        "        print(f\"• Transformer: {transformer_dim} dim, {num_heads} heads, {num_layers} layers\")\n",
        "        print(f\"• Designed for stability and performance\")\n",
        "\n",
        "        # ==========================================\n",
        "        # CNN BACKBONE (SAME AS WORKING MODEL)\n",
        "        # ==========================================\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(32, 32, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout2d(0.1),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout2d(0.15),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(128, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout2d(0.2),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "\n",
        "        # ==========================================\n",
        "        # LSTM LAYER (SAME AS WORKING MODEL)\n",
        "        # ==========================================\n",
        "        self.feature_size = 128 * 16\n",
        "        self.lstm_hidden = lstm_hidden\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.feature_size,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=0.2,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # ==========================================\n",
        "        # LIGHTWEIGHT TRANSFORMER\n",
        "        # ==========================================\n",
        "        self.transformer_dim = transformer_dim\n",
        "\n",
        "        # Project LSTM output to transformer dimension\n",
        "        self.lstm_to_transformer = nn.Linear(lstm_hidden * 2, transformer_dim)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(transformer_dim)\n",
        "\n",
        "        # Lightweight transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=transformer_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=transformer_dim * 2,  # Smaller feedforward\n",
        "            dropout=0.1,\n",
        "            activation='relu',  # Stick with ReLU for stability\n",
        "            batch_first=True,\n",
        "            norm_first=False  # Post-norm for stability\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers,\n",
        "            norm=nn.LayerNorm(transformer_dim)\n",
        "        )\n",
        "\n",
        "        # ==========================================\n",
        "        # SIMPLE ATTENTION & CLASSIFICATION\n",
        "        # ==========================================\n",
        "\n",
        "        # Simple global attention pooling\n",
        "        self.attention_pooling = nn.MultiheadAttention(\n",
        "            embed_dim=transformer_dim,\n",
        "            num_heads=4,  # Fewer heads for simplicity\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Simple classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(transformer_dim),\n",
        "            nn.Linear(transformer_dim, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "        print(\"✅ Simple CNN-LSTM-Transformer built!\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # ==========================================\n",
        "        # CNN FEATURE EXTRACTION\n",
        "        # ==========================================\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        # Reshape for LSTM\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = x.contiguous().view(batch_size, x.size(1), -1)\n",
        "\n",
        "        # ==========================================\n",
        "        # LSTM PROCESSING\n",
        "        # ==========================================\n",
        "        lstm_out, _ = self.lstm(x)  # (batch, seq_len, lstm_hidden*2)\n",
        "\n",
        "        # ==========================================\n",
        "        # TRANSFORMER PROCESSING\n",
        "        # ==========================================\n",
        "\n",
        "        # Project to transformer dimension\n",
        "        transformer_input = self.lstm_to_transformer(lstm_out)  # (batch, seq_len, transformer_dim)\n",
        "\n",
        "        # Add positional encoding\n",
        "        seq_len = transformer_input.size(1)\n",
        "        transformer_input = transformer_input.transpose(0, 1)  # (seq_len, batch, transformer_dim)\n",
        "        transformer_input = self.pos_encoding(transformer_input)\n",
        "        transformer_input = transformer_input.transpose(0, 1)  # (batch, seq_len, transformer_dim)\n",
        "\n",
        "        # Transformer encoding\n",
        "        transformer_out = self.transformer_encoder(transformer_input)  # (batch, seq_len, transformer_dim)\n",
        "\n",
        "        # ==========================================\n",
        "        # ATTENTION POOLING & CLASSIFICATION\n",
        "        # ==========================================\n",
        "\n",
        "        # Global attention pooling\n",
        "        attended, _ = self.attention_pooling(\n",
        "            transformer_out, transformer_out, transformer_out\n",
        "        )\n",
        "\n",
        "        # Global average pooling\n",
        "        pooled = torch.mean(attended, dim=1)  # (batch, transformer_dim)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(pooled)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Create the simple transformer model\n",
        "simple_transformer_model = SimpleCNN_LSTM_Transformer(\n",
        "    num_classes=4,\n",
        "    lstm_hidden=128,\n",
        "    transformer_dim=256,  # Reasonable size\n",
        "    num_heads=8,         # Moderate number of heads\n",
        "    num_layers=2         # Just 2 layers for stability\n",
        ").to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in simple_transformer_model.parameters())\n",
        "print(f\"📊 Simple Transformer Model: {total_params:,} parameters\")\n",
        "\n",
        "# Test forward pass\n",
        "test_input = torch.randn(4, 1, 128, 4500).to(device)\n",
        "with torch.no_grad():\n",
        "    output = simple_transformer_model(test_input)\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"✅ Simple transformer model forward pass successful!\")"
      ],
      "metadata": {
        "id": "ZiBLgemTIbg-",
        "outputId": "84b9a53b-bd18-4d55-91d4-09dd308247e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ZiBLgemTIbg-",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Building SIMPLE CNN-LSTM-Transformer...\n",
            "• CNN: 3 blocks (like working model)\n",
            "• LSTM: 128 hidden units\n",
            "• Transformer: 256 dim, 8 heads, 2 layers\n",
            "• Designed for stability and performance\n",
            "✅ Simple CNN-LSTM-Transformer built!\n",
            "📊 Simple Transformer Model: 4,396,260 parameters\n",
            "Output shape: torch.Size([4, 4])\n",
            "✅ Simple transformer model forward pass successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# TRAIN SIMPLE CNN-LSTM-TRANSFORMER\n",
        "# =====================================================\n",
        "\n",
        "def train_simple_transformer_model():\n",
        "    \"\"\"Train the simple transformer model\"\"\"\n",
        "\n",
        "    print(\"🚀 TRAINING SIMPLE CNN-LSTM-TRANSFORMER...\")\n",
        "    print(\"Expected benefits:\")\n",
        "    print(\"• Better long-range dependencies from transformer\")\n",
        "    print(\"• Improved musical pattern recognition\")\n",
        "    print(\"• Stable training with proven CNN-LSTM base\")\n",
        "\n",
        "    # Conservative training settings\n",
        "    optimizer = optim.AdamW(\n",
        "        simple_transformer_model.parameters(),\n",
        "        lr=2e-3,  # Slightly lower LR for transformer stability\n",
        "        weight_decay=1e-4\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', factor=0.7, patience=5, verbose=True\n",
        "    )\n",
        "\n",
        "    epochs = 30\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    patience = 12\n",
        "\n",
        "    print(f\"\\n🎯 Simple Transformer Training Configuration:\")\n",
        "    print(f\"• Learning rate: 2e-3 (conservative for transformer)\")\n",
        "    print(f\"• Gradient clipping: max_norm=1.0\")\n",
        "    print(f\"• Epochs: {epochs} (patience: {patience})\")\n",
        "    print(f\"• Using augmented training data\")\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ==========================================\n",
        "        # TRAINING PHASE\n",
        "        # ==========================================\n",
        "        simple_transformer_model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader_aug):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = simple_transformer_model(data)\n",
        "            loss = criterion(outputs, target)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping (important for transformers)\n",
        "            torch.nn.utils.clip_grad_norm_(simple_transformer_model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "            if batch_idx % 25 == 0:\n",
        "                print(f\"  Batch {batch_idx:3d}/{len(train_loader_aug)} | \"\n",
        "                      f\"Loss: {loss.item():.4f} | \"\n",
        "                      f\"Acc: {100.*correct/total:.2f}% | \"\n",
        "                      f\"LR: {optimizer.param_groups[0]['lr']:.1e}\")\n",
        "\n",
        "        # ==========================================\n",
        "        # VALIDATION PHASE\n",
        "        # ==========================================\n",
        "        simple_transformer_model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader_clean:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                outputs = simple_transformer_model(data)\n",
        "                loss = criterion(outputs, target)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += target.size(0)\n",
        "                val_correct += (predicted == target).sum().item()\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_loss = total_loss / len(train_loader_aug)\n",
        "        train_acc = 100. * correct / total\n",
        "        val_loss = val_loss / len(val_loader_clean)\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        print(f\"📊 Epoch {epoch+1} Summary:\")\n",
        "        print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
        "        print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save({\n",
        "                'model_state_dict': simple_transformer_model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'epoch': epoch,\n",
        "                'history': history\n",
        "            }, 'saved_models/simple_transformer_model.pth')\n",
        "            print(f\"  💾 New best model saved! Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "            if val_acc > 75.0:\n",
        "                print(f\"  🎉 Excellent performance reached: {val_acc:.2f}%!\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"  ⏳ Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\n🛑 Early stopping! Best Val Acc: {best_val_acc:.2f}%\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\n✅ Simple transformer training completed!\")\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "    print(f\"Improvement over baseline: {best_val_acc - 71.92:.2f} percentage points\")\n",
        "\n",
        "    return simple_transformer_model, history, best_val_acc\n",
        "\n",
        "# Run transformer training\n",
        "print(\"🚀 Starting training of Simple CNN-LSTM-Transformer...\")\n",
        "transformer_model, transformer_history, transformer_best_acc = train_simple_transformer_model()"
      ],
      "metadata": {
        "id": "k_4bBISLIoXL",
        "outputId": "898a00ea-2522-4878-ffab-c5ec0f59af21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "k_4bBISLIoXL",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting training of Simple CNN-LSTM-Transformer...\n",
            "🚀 TRAINING SIMPLE CNN-LSTM-TRANSFORMER...\n",
            "Expected benefits:\n",
            "• Better long-range dependencies from transformer\n",
            "• Improved musical pattern recognition\n",
            "• Stable training with proven CNN-LSTM base\n",
            "\n",
            "🎯 Simple Transformer Training Configuration:\n",
            "• Learning rate: 2e-3 (conservative for transformer)\n",
            "• Gradient clipping: max_norm=1.0\n",
            "• Epochs: 30 (patience: 12)\n",
            "• Using augmented training data\n",
            "\n",
            "Epoch 1/30\n",
            "  Batch   0/125 | Loss: 1.4834 | Acc: 12.50% | LR: 2.0e-03\n",
            "  Batch  25/125 | Loss: 1.3548 | Acc: 22.60% | LR: 2.0e-03\n",
            "  Batch  50/125 | Loss: 1.4184 | Acc: 22.92% | LR: 2.0e-03\n",
            "  Batch  75/125 | Loss: 1.4112 | Acc: 22.37% | LR: 2.0e-03\n",
            "  Batch 100/125 | Loss: 1.4195 | Acc: 24.44% | LR: 2.0e-03\n",
            "📊 Epoch 1 Summary:\n",
            "  Train: Loss=1.4184, Acc=23.92%\n",
            "  Val:   Loss=1.4007, Acc=35.16%\n",
            "  💾 New best model saved! Val Acc: 35.16%\n",
            "\n",
            "Epoch 2/30\n",
            "  Batch   0/125 | Loss: 1.4570 | Acc: 12.50% | LR: 2.0e-03\n",
            "  Batch  25/125 | Loss: 1.3635 | Acc: 27.40% | LR: 2.0e-03\n",
            "  Batch  50/125 | Loss: 1.4279 | Acc: 23.65% | LR: 2.0e-03\n",
            "  Batch  75/125 | Loss: 1.3958 | Acc: 23.11% | LR: 2.0e-03\n",
            "  Batch 100/125 | Loss: 1.4533 | Acc: 22.28% | LR: 2.0e-03\n",
            "📊 Epoch 2 Summary:\n",
            "  Train: Loss=1.3945, Acc=22.96%\n",
            "  Val:   Loss=1.3733, Acc=36.53%\n",
            "  💾 New best model saved! Val Acc: 36.53%\n",
            "\n",
            "Epoch 3/30\n",
            "  Batch   0/125 | Loss: 1.4022 | Acc: 18.75% | LR: 2.0e-03\n",
            "  Batch  25/125 | Loss: 1.4267 | Acc: 27.16% | LR: 2.0e-03\n",
            "  Batch  50/125 | Loss: 1.3077 | Acc: 26.47% | LR: 2.0e-03\n",
            "  Batch  75/125 | Loss: 1.3676 | Acc: 25.82% | LR: 2.0e-03\n",
            "  Batch 100/125 | Loss: 1.4183 | Acc: 24.50% | LR: 2.0e-03\n",
            "📊 Epoch 3 Summary:\n",
            "  Train: Loss=1.3916, Acc=24.27%\n",
            "  Val:   Loss=1.4067, Acc=12.10%\n",
            "  ⏳ Patience: 1/12\n",
            "\n",
            "Epoch 4/30\n",
            "  Batch   0/125 | Loss: 1.3671 | Acc: 31.25% | LR: 2.0e-03\n",
            "  Batch  25/125 | Loss: 1.4054 | Acc: 26.44% | LR: 2.0e-03\n",
            "  Batch  50/125 | Loss: 1.3912 | Acc: 23.41% | LR: 2.0e-03\n",
            "  Batch  75/125 | Loss: 1.3807 | Acc: 25.99% | LR: 2.0e-03\n",
            "  Batch 100/125 | Loss: 1.3934 | Acc: 26.30% | LR: 2.0e-03\n",
            "📊 Epoch 4 Summary:\n",
            "  Train: Loss=1.3895, Acc=25.23%\n",
            "  Val:   Loss=1.3944, Acc=12.10%\n",
            "  ⏳ Patience: 2/12\n",
            "\n",
            "Epoch 5/30\n",
            "  Batch   0/125 | Loss: 1.3758 | Acc: 31.25% | LR: 2.0e-03\n",
            "  Batch  25/125 | Loss: 1.4257 | Acc: 26.92% | LR: 2.0e-03\n",
            "  Batch  50/125 | Loss: 1.3952 | Acc: 25.37% | LR: 2.0e-03\n",
            "  Batch  75/125 | Loss: 1.3939 | Acc: 26.23% | LR: 2.0e-03\n",
            "  Batch 100/125 | Loss: 1.3877 | Acc: 25.37% | LR: 2.0e-03\n",
            "📊 Epoch 5 Summary:\n",
            "  Train: Loss=1.3895, Acc=24.77%\n",
            "  Val:   Loss=1.3859, Acc=16.21%\n",
            "  ⏳ Patience: 3/12\n",
            "\n",
            "Epoch 6/30\n",
            "  Batch   0/125 | Loss: 1.3791 | Acc: 37.50% | LR: 2.0e-03\n",
            "  Batch  25/125 | Loss: 1.2962 | Acc: 27.64% | LR: 2.0e-03\n",
            "  Batch  50/125 | Loss: 1.3999 | Acc: 25.98% | LR: 2.0e-03\n",
            "  Batch  75/125 | Loss: 1.3728 | Acc: 24.10% | LR: 2.0e-03\n",
            "  Batch 100/125 | Loss: 1.3839 | Acc: 23.51% | LR: 2.0e-03\n",
            "📊 Epoch 6 Summary:\n",
            "  Train: Loss=1.3889, Acc=25.08%\n",
            "  Val:   Loss=1.3910, Acc=35.16%\n",
            "  ⏳ Patience: 4/12\n",
            "\n",
            "Epoch 7/30\n",
            "  Batch   0/125 | Loss: 1.3853 | Acc: 37.50% | LR: 2.0e-03\n",
            "  Batch  25/125 | Loss: 1.3817 | Acc: 22.36% | LR: 2.0e-03\n",
            "  Batch  50/125 | Loss: 1.3779 | Acc: 22.67% | LR: 2.0e-03\n",
            "  Batch  75/125 | Loss: 1.3952 | Acc: 21.38% | LR: 2.0e-03\n",
            "  Batch 100/125 | Loss: 1.3912 | Acc: 21.91% | LR: 2.0e-03\n",
            "📊 Epoch 7 Summary:\n",
            "  Train: Loss=1.3878, Acc=21.85%\n",
            "  Val:   Loss=1.3976, Acc=12.10%\n",
            "  ⏳ Patience: 5/12\n",
            "\n",
            "Epoch 8/30\n",
            "  Batch   0/125 | Loss: 1.3980 | Acc: 6.25% | LR: 2.0e-03\n",
            "  Batch  25/125 | Loss: 1.3989 | Acc: 21.88% | LR: 2.0e-03\n",
            "  Batch  50/125 | Loss: 1.3914 | Acc: 22.18% | LR: 2.0e-03\n",
            "  Batch  75/125 | Loss: 1.3921 | Acc: 21.13% | LR: 2.0e-03\n",
            "  Batch 100/125 | Loss: 1.3710 | Acc: 22.34% | LR: 2.0e-03\n",
            "📊 Epoch 8 Summary:\n",
            "  Train: Loss=1.3869, Acc=22.61%\n",
            "  Val:   Loss=1.4367, Acc=12.10%\n",
            "  ⏳ Patience: 6/12\n",
            "\n",
            "Epoch 9/30\n",
            "  Batch   0/125 | Loss: 1.3748 | Acc: 25.00% | LR: 1.4e-03\n",
            "  Batch  25/125 | Loss: 1.3432 | Acc: 27.16% | LR: 1.4e-03\n",
            "  Batch  50/125 | Loss: 1.3776 | Acc: 22.79% | LR: 1.4e-03\n",
            "  Batch  75/125 | Loss: 1.3925 | Acc: 22.37% | LR: 1.4e-03\n",
            "  Batch 100/125 | Loss: 1.3679 | Acc: 22.59% | LR: 1.4e-03\n",
            "📊 Epoch 9 Summary:\n",
            "  Train: Loss=1.3860, Acc=22.26%\n",
            "  Val:   Loss=1.4089, Acc=12.10%\n",
            "  ⏳ Patience: 7/12\n",
            "\n",
            "Epoch 10/30\n",
            "  Batch   0/125 | Loss: 1.3650 | Acc: 37.50% | LR: 1.4e-03\n",
            "  Batch  25/125 | Loss: 1.3623 | Acc: 20.91% | LR: 1.4e-03\n",
            "  Batch  50/125 | Loss: 1.3839 | Acc: 21.20% | LR: 1.4e-03\n",
            "  Batch  75/125 | Loss: 1.3636 | Acc: 23.36% | LR: 1.4e-03\n",
            "  Batch 100/125 | Loss: 1.4138 | Acc: 22.52% | LR: 1.4e-03\n",
            "📊 Epoch 10 Summary:\n",
            "  Train: Loss=1.3857, Acc=22.41%\n",
            "  Val:   Loss=1.4010, Acc=12.10%\n",
            "  ⏳ Patience: 8/12\n",
            "\n",
            "Epoch 11/30\n",
            "  Batch   0/125 | Loss: 1.3785 | Acc: 25.00% | LR: 1.4e-03\n",
            "  Batch  25/125 | Loss: 1.4023 | Acc: 22.84% | LR: 1.4e-03\n",
            "  Batch  50/125 | Loss: 1.3863 | Acc: 23.90% | LR: 1.4e-03\n",
            "  Batch  75/125 | Loss: 1.3961 | Acc: 23.11% | LR: 1.4e-03\n",
            "  Batch 100/125 | Loss: 1.3724 | Acc: 22.65% | LR: 1.4e-03\n",
            "📊 Epoch 11 Summary:\n",
            "  Train: Loss=1.3859, Acc=22.31%\n",
            "  Val:   Loss=1.4001, Acc=12.10%\n",
            "  ⏳ Patience: 9/12\n",
            "\n",
            "Epoch 12/30\n",
            "  Batch   0/125 | Loss: 1.3882 | Acc: 25.00% | LR: 1.4e-03\n",
            "  Batch  25/125 | Loss: 1.4074 | Acc: 18.99% | LR: 1.4e-03\n",
            "  Batch  50/125 | Loss: 1.3850 | Acc: 18.87% | LR: 1.4e-03\n",
            "  Batch  75/125 | Loss: 1.3860 | Acc: 21.30% | LR: 1.4e-03\n",
            "  Batch 100/125 | Loss: 1.3814 | Acc: 22.40% | LR: 1.4e-03\n",
            "📊 Epoch 12 Summary:\n",
            "  Train: Loss=1.3860, Acc=22.26%\n",
            "  Val:   Loss=1.4087, Acc=12.10%\n",
            "  ⏳ Patience: 10/12\n",
            "\n",
            "Epoch 13/30\n",
            "  Batch   0/125 | Loss: 1.4041 | Acc: 12.50% | LR: 1.4e-03\n",
            "  Batch  25/125 | Loss: 1.3629 | Acc: 24.28% | LR: 1.4e-03\n",
            "  Batch  50/125 | Loss: 1.4046 | Acc: 22.92% | LR: 1.4e-03\n",
            "  Batch  75/125 | Loss: 1.3843 | Acc: 22.78% | LR: 1.4e-03\n",
            "  Batch 100/125 | Loss: 1.3707 | Acc: 22.71% | LR: 1.4e-03\n",
            "📊 Epoch 13 Summary:\n",
            "  Train: Loss=1.3854, Acc=22.31%\n",
            "  Val:   Loss=1.4031, Acc=12.10%\n",
            "  ⏳ Patience: 11/12\n",
            "\n",
            "Epoch 14/30\n",
            "  Batch   0/125 | Loss: 1.3974 | Acc: 12.50% | LR: 1.4e-03\n",
            "  Batch  25/125 | Loss: 1.4265 | Acc: 26.68% | LR: 1.4e-03\n",
            "  Batch  50/125 | Loss: 1.3942 | Acc: 23.41% | LR: 1.4e-03\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2064719491.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;31m# Run transformer training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🚀 Starting training of Simple CNN-LSTM-Transformer...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m \u001b[0mtransformer_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_best_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_simple_transformer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2064719491.py\u001b[0m in \u001b[0;36mtrain_simple_transformer_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader_aug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ttJ-s25RI_aV"
      },
      "id": "ttJ-s25RI_aV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}