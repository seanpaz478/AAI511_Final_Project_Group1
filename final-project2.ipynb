{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install pretty_midi kagglehub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caJC4Lf_md2R",
        "outputId": "c8ee68d4-cb36-4731-bc6a-08f921c5092f"
      },
      "id": "caJC4Lf_md2R",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (2.0.2)\n",
            "Collecting mido>=1.1.16 (from pretty_midi)\n",
            "  Downloading mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.7.14)\n",
            "Downloading mido-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pretty_midi\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592286 sha256=45f1fb4224481557f2a51446049006ab691dcef846af086dcf25225bc091a632\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/95/ac/15ceaeb2823b04d8e638fd1495357adb8d26c00ccac9d7782e\n",
            "Successfully built pretty_midi\n",
            "Installing collected packages: mido, pretty_midi\n",
            "Successfully installed mido-1.3.3 pretty_midi-0.2.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "998891eb",
      "metadata": {
        "id": "998891eb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import zipfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import numpy as np\n",
        "import pretty_midi\n",
        "import torch\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6406d2f8",
      "metadata": {
        "id": "6406d2f8"
      },
      "outputs": [],
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2c06d7fe",
      "metadata": {
        "id": "2c06d7fe"
      },
      "outputs": [],
      "source": [
        "if not hasattr(np, 'int'):\n",
        "    np.int = int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "dd63e1d1",
      "metadata": {
        "id": "dd63e1d1"
      },
      "outputs": [],
      "source": [
        "# Define your model class (must match the architecture used in final-project1)\n",
        "class CNN_LSTM_Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=4, lstm_hidden=256):\n",
        "        super(CNN_LSTM_Classifier, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.2),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.3),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.4),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "        self.feature_size = 128 * 16\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.feature_size,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=0.3,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=lstm_hidden * 2,\n",
        "            num_heads=8,\n",
        "            dropout=0.3,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(lstm_hidden * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = x.contiguous().view(batch_size, x.size(1), -1)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
        "        pooled = torch.mean(attn_out, dim=1)\n",
        "        output = self.classifier(pooled)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "89e8b887",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89e8b887",
        "outputId": "4f57181d-aee5-414e-d78f-387317635aa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab.\n",
            "‚úÖ Loaded: /content/saved_models/original_cnn_lstm.pth\n",
            "‚úÖ Loaded: /content/saved_models/rhythm_augmented_cnn_lstm.pth\n"
          ]
        }
      ],
      "source": [
        "# Load the models\n",
        "from IPython import get_ipython\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    print(\"Running in Google Colab.\")\n",
        "    original_path = os.path.join('/content/saved_models', 'original_cnn_lstm.pth')\n",
        "    rhythm_path = os.path.join('/content/saved_models', 'rhythm_augmented_cnn_lstm.pth')\n",
        "else:\n",
        "    print(\"Not running in Google Colab.\")\n",
        "    original_path = os.path.join('saved_models', 'original_cnn_lstm.pth')\n",
        "    rhythm_path = os.path.join('saved_models', 'rhythm_augmented_cnn_lstm.pth')\n",
        "\n",
        "model = CNN_LSTM_Classifier(num_classes=4, lstm_hidden=256).to(device)\n",
        "model.load_state_dict(torch.load(original_path, map_location=device))\n",
        "print(f\"‚úÖ Loaded: {original_path}\")\n",
        "\n",
        "rhythm_model = CNN_LSTM_Classifier(num_classes=4, lstm_hidden=256).to(device)\n",
        "rhythm_model.load_state_dict(torch.load(rhythm_path, map_location=device))\n",
        "print(f\"‚úÖ Loaded: {rhythm_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "afa70024",
      "metadata": {
        "id": "afa70024"
      },
      "outputs": [],
      "source": [
        "TARGET_COMPOSERS = [\n",
        "    'Bach',\n",
        "    'Beethoven',\n",
        "    'Chopin',\n",
        "    'Mozart',\n",
        "]\n",
        "\n",
        "path = kagglehub.dataset_download(\"blanderbuss/midi-classic-music\")\n",
        "\n",
        "zip_path = os.path.join(path, 'midiclassics.zip')\n",
        "extract_path = os.path.join('data', 'kaggle', 'midiclassics')\n",
        "\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# list files in extract_path that contain the target composers in name\n",
        "for composer in TARGET_COMPOSERS:\n",
        "    composer_files = [f for f in os.listdir(extract_path) if composer.lower() in f.lower()]\n",
        "\n",
        "# Only keep directories that contain a target composer's name\n",
        "for item in os.listdir(extract_path):\n",
        "    item_path = os.path.join(extract_path, item)\n",
        "    if not any(composer.lower() in item.lower() for composer in TARGET_COMPOSERS):\n",
        "        if os.path.isfile(item_path):\n",
        "            os.remove(item_path)\n",
        "        elif os.path.isdir(item_path):\n",
        "            shutil.rmtree(item_path)\n",
        "\n",
        "# also delete \"C.P.E.Bach\" files. This was the son of J.S. Bach, and we want to keep only the main composers\n",
        "for item in os.listdir(extract_path):\n",
        "    if 'C.P.E.Bach' in item:\n",
        "        item_path = os.path.join(extract_path, item)\n",
        "        if os.path.isfile(item_path):\n",
        "            os.remove(item_path)\n",
        "        elif os.path.isdir(item_path):\n",
        "            shutil.rmtree(item_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c7eb056a",
      "metadata": {
        "id": "c7eb056a"
      },
      "outputs": [],
      "source": [
        "class PianoRollDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        # Add channel dimension for CNN: (1, 128, T)\n",
        "        return self.data[idx].unsqueeze(0), self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ad781104",
      "metadata": {
        "id": "ad781104"
      },
      "outputs": [],
      "source": [
        "def get_piano_roll_improved(midi_path, fs=100, target_duration=45.0):\n",
        "    \"\"\"\n",
        "    Improved MIDI to piano roll conversion with musical awareness\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pm = pretty_midi.PrettyMIDI(midi_path)\n",
        "\n",
        "        # Get the actual duration of the piece\n",
        "        actual_duration = pm.get_end_time()\n",
        "\n",
        "        # If piece is very short, skip it\n",
        "        if actual_duration < 10.0:  # Less than 10 seconds\n",
        "            return None\n",
        "\n",
        "        # For long pieces, extract multiple segments\n",
        "        if actual_duration > target_duration * 1.5:\n",
        "            # Extract from different parts of the piece\n",
        "            segments = []\n",
        "            num_segments = min(3, int(actual_duration // target_duration))\n",
        "\n",
        "            for i in range(num_segments):\n",
        "                start_time = i * (actual_duration / num_segments)\n",
        "                end_time = start_time + target_duration\n",
        "\n",
        "                # Create a copy and trim\n",
        "                pm_segment = pretty_midi.PrettyMIDI()\n",
        "                for instrument in pm.instruments:\n",
        "                    new_instrument = pretty_midi.Instrument(\n",
        "                        program=instrument.program,\n",
        "                        is_drum=instrument.is_drum,\n",
        "                        name=instrument.name\n",
        "                    )\n",
        "\n",
        "                    for note in instrument.notes:\n",
        "                        if start_time <= note.start < end_time:\n",
        "                            new_note = pretty_midi.Note(\n",
        "                                velocity=note.velocity,\n",
        "                                pitch=note.pitch,\n",
        "                                start=note.start - start_time,\n",
        "                                end=min(note.end - start_time, target_duration)\n",
        "                            )\n",
        "                            new_instrument.notes.append(new_note)\n",
        "\n",
        "                    if new_instrument.notes:\n",
        "                        pm_segment.instruments.append(new_instrument)\n",
        "\n",
        "                if pm_segment.instruments:\n",
        "                    piano_roll = pm_segment.get_piano_roll(fs=fs)\n",
        "                    target_length = int(target_duration * fs)\n",
        "\n",
        "                    if piano_roll.shape[1] > target_length:\n",
        "                        piano_roll = piano_roll[:, :target_length]\n",
        "                    else:\n",
        "                        pad_width = target_length - piano_roll.shape[1]\n",
        "                        piano_roll = np.pad(piano_roll, ((0,0),(0,pad_width)), mode='constant')\n",
        "\n",
        "                    segments.append(piano_roll)\n",
        "\n",
        "            return segments\n",
        "\n",
        "        else:\n",
        "            # For normal length pieces, use the whole piece\n",
        "            piano_roll = pm.get_piano_roll(fs=fs)\n",
        "            target_length = int(target_duration * fs)\n",
        "\n",
        "            if piano_roll.shape[1] > target_length:\n",
        "                # Take from the middle rather than truncating end\n",
        "                start_idx = (piano_roll.shape[1] - target_length) // 2\n",
        "                piano_roll = piano_roll[:, start_idx:start_idx + target_length]\n",
        "            else:\n",
        "                pad_width = target_length - piano_roll.shape[1]\n",
        "                piano_roll = np.pad(piano_roll, ((0,0),(0,pad_width)), mode='constant')\n",
        "\n",
        "            return [piano_roll]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {midi_path}: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "230bdd12",
      "metadata": {
        "id": "230bdd12"
      },
      "outputs": [],
      "source": [
        "def normalize_piano_roll(piano_roll):\n",
        "    \"\"\"\n",
        "    Apply musical normalization to piano roll\n",
        "    \"\"\"\n",
        "    # 1. Velocity normalization (already 0-1 from pretty_midi)\n",
        "    normalized = piano_roll.copy()\n",
        "\n",
        "    # 2. Optional: Focus on active pitch range\n",
        "    active_pitches = np.any(normalized > 0, axis=1)\n",
        "    if np.any(active_pitches):\n",
        "        first_active = np.argmax(active_pitches)\n",
        "        last_active = len(active_pitches) - 1 - np.argmax(active_pitches[::-1])\n",
        "\n",
        "        # Ensure we keep a reasonable range (at least 60 semitones = 5 octaves)\n",
        "        min_range = 60\n",
        "        current_range = last_active - first_active + 1\n",
        "\n",
        "        if current_range < min_range:\n",
        "            expand = (min_range - current_range) // 2\n",
        "            first_active = max(0, first_active - expand)\n",
        "            last_active = min(127, last_active + expand)\n",
        "\n",
        "    return normalized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b1b1f0c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1b1f0c8",
        "outputId": "ee52c0c7-90c1-4479-f41f-7cb5753d738d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Improved data processing functions defined!\n",
            "Key improvements:\n",
            "‚Ä¢ Intelligent segment extraction for long pieces\n",
            "‚Ä¢ Musical boundary awareness\n",
            "‚Ä¢ Better normalization\n",
            "‚Ä¢ Feature extraction for analysis\n"
          ]
        }
      ],
      "source": [
        "def extract_musical_features(piano_roll):\n",
        "    \"\"\"\n",
        "    Extract features that capture musical style\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "\n",
        "    # Temporal features\n",
        "    note_density_timeline = np.sum(piano_roll > 0, axis=0)\n",
        "    features['avg_notes_per_time'] = np.mean(note_density_timeline)\n",
        "    features['note_density_variance'] = np.var(note_density_timeline)\n",
        "\n",
        "    # Pitch features\n",
        "    pitch_activity = np.sum(piano_roll > 0, axis=1)\n",
        "    active_pitches = pitch_activity > 0\n",
        "    if np.any(active_pitches):\n",
        "        features['pitch_range'] = np.sum(active_pitches)\n",
        "        features['lowest_pitch'] = np.argmax(active_pitches)\n",
        "        features['highest_pitch'] = 127 - np.argmax(active_pitches[::-1])\n",
        "    else:\n",
        "        features['pitch_range'] = 0\n",
        "        features['lowest_pitch'] = 60  # Middle C\n",
        "        features['highest_pitch'] = 60\n",
        "\n",
        "    # Rhythmic features\n",
        "    onset_pattern = np.diff(note_density_timeline > 0).astype(int)\n",
        "    features['onset_density'] = np.sum(onset_pattern == 1) / len(onset_pattern)\n",
        "\n",
        "    return features\n",
        "\n",
        "print(\"‚úÖ Improved data processing functions defined!\")\n",
        "print(\"Key improvements:\")\n",
        "print(\"‚Ä¢ Intelligent segment extraction for long pieces\")\n",
        "print(\"‚Ä¢ Musical boundary awareness\")\n",
        "print(\"‚Ä¢ Better normalization\")\n",
        "print(\"‚Ä¢ Feature extraction for analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9771e8b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9771e8b1",
        "outputId": "d1dd304d-41cf-41a2-d9ee-67d704bb311f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting improved data loading...\n",
            "üéµ LOADING DATASET WITH IMPROVED PROCESSING...\n",
            "Improvements over original:\n",
            "‚Ä¢ Intelligent segment extraction for long pieces\n",
            "‚Ä¢ Better handling of piece lengths\n",
            "‚Ä¢ Musical feature extraction\n",
            "‚Ä¢ Quality filtering\n",
            "\n",
            "--- Processing Bach ---\n",
            "  Processed 10 files, created 25 segments...\n",
            "  Processed 20 files, created 51 segments...\n",
            "  Processed 30 files, created 77 segments...\n",
            "  Processed 40 files, created 104 segments...\n",
            "  Processed 50 files, created 130 segments...\n",
            "  Processed 60 files, created 154 segments...\n",
            "  Processed 70 files, created 182 segments...\n",
            "  Processed 80 files, created 211 segments...\n",
            "  Processed 90 files, created 233 segments...\n",
            "  Processed 100 files, created 261 segments...\n",
            "  Processed 110 files, created 286 segments...\n",
            "  Processed 120 files, created 312 segments...\n",
            "‚úÖ Bach: 120 files ‚Üí 312 segments\n",
            "  Final data shape: (312, 128, 4500)\n",
            "\n",
            "--- Processing Beethoven ---\n",
            "  Processed 10 files, created 26 segments...\n",
            "  Processed 20 files, created 52 segments...\n",
            "Error processing data/kaggle/midiclassics/Beethoven/Anhang 14-3.mid: Could not decode key with 3 flats and mode 255\n",
            "  Processed 30 files, created 78 segments...\n",
            "  Processed 40 files, created 105 segments...\n",
            "  Processed 50 files, created 126 segments...\n",
            "  Processed 60 files, created 153 segments...\n",
            "  Processed 70 files, created 176 segments...\n",
            "  Processed 80 files, created 201 segments...\n",
            "  Processed 90 files, created 226 segments...\n",
            "  Processed 100 files, created 251 segments...\n",
            "  Processed 110 files, created 279 segments...\n",
            "‚úÖ Beethoven: 119 files ‚Üí 304 segments\n",
            "  Final data shape: (304, 128, 4500)\n",
            "\n",
            "--- Processing Chopin ---\n",
            "  Processed 10 files, created 23 segments...\n",
            "  Processed 20 files, created 45 segments...\n",
            "  Processed 30 files, created 66 segments...\n",
            "  Processed 40 files, created 92 segments...\n",
            "  Processed 50 files, created 117 segments...\n",
            "  Processed 60 files, created 135 segments...\n",
            "  Processed 70 files, created 157 segments...\n",
            "  Processed 80 files, created 178 segments...\n",
            "  Processed 90 files, created 203 segments...\n",
            "  Processed 100 files, created 232 segments...\n",
            "  Processed 110 files, created 258 segments...\n",
            "  Processed 120 files, created 286 segments...\n",
            "‚úÖ Chopin: 120 files ‚Üí 286 segments\n",
            "  Final data shape: (286, 128, 4500)\n",
            "\n",
            "--- Processing Mozart ---\n",
            "  Processed 10 files, created 28 segments...\n",
            "  Processed 20 files, created 53 segments...\n",
            "  Processed 30 files, created 78 segments...\n",
            "  Processed 40 files, created 106 segments...\n",
            "  Processed 50 files, created 131 segments...\n",
            "  Processed 60 files, created 157 segments...\n",
            "  Processed 70 files, created 187 segments...\n",
            "  Processed 80 files, created 216 segments...\n",
            "  Processed 90 files, created 244 segments...\n",
            "‚úÖ Mozart: 90 files ‚Üí 244 segments\n",
            "  Final data shape: (244, 128, 4500)\n",
            "\n",
            "üéØ FINAL IMPROVED DATASET:\n",
            "Total samples: 1146\n",
            "Data shape: (1146, 128, 4500)\n",
            "Label distribution: [312 304 286 244]\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# IMPROVED DATA LOADING WITH BETTER PROCESSING\n",
        "# =====================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def load_improved_dataset(extract_path, target_composers, target_duration=45.0, max_files_per_composer=None):\n",
        "    \"\"\"\n",
        "    Load dataset with improved processing that addresses previous shortcomings\n",
        "    \"\"\"\n",
        "    print(\"üéµ LOADING DATASET WITH IMPROVED PROCESSING...\")\n",
        "    print(\"Improvements over original:\")\n",
        "    print(\"‚Ä¢ Intelligent segment extraction for long pieces\")\n",
        "    print(\"‚Ä¢ Better handling of piece lengths\")\n",
        "    print(\"‚Ä¢ Musical feature extraction\")\n",
        "    print(\"‚Ä¢ Quality filtering\")\n",
        "\n",
        "    composer_to_idx = {c: i for i, c in enumerate(target_composers)}\n",
        "    all_data = []\n",
        "    all_labels = []\n",
        "    all_features = []\n",
        "\n",
        "    for composer in target_composers:\n",
        "        print(f\"\\n--- Processing {composer} ---\")\n",
        "        composer_dir = os.path.join(extract_path, composer)\n",
        "\n",
        "        if not os.path.isdir(composer_dir):\n",
        "            print(f\"Directory not found: {composer_dir}\")\n",
        "            continue\n",
        "\n",
        "        composer_data = []\n",
        "        composer_labels = []\n",
        "        composer_features = []\n",
        "        files_processed = 0\n",
        "        segments_created = 0\n",
        "\n",
        "        midi_files = [f for f in os.listdir(composer_dir)\n",
        "                     if f.lower().endswith(('.mid', '.midi'))]\n",
        "\n",
        "        if max_files_per_composer:\n",
        "            midi_files = midi_files[:max_files_per_composer]\n",
        "\n",
        "        for file in midi_files:\n",
        "            midi_path = os.path.join(composer_dir, file)\n",
        "\n",
        "            try:\n",
        "                # Use improved processing\n",
        "                segments = get_piano_roll_improved(midi_path, target_duration=target_duration)\n",
        "\n",
        "                if segments is None:\n",
        "                    continue\n",
        "\n",
        "                for segment in segments:\n",
        "                    # Normalize the segment\n",
        "                    normalized_segment = normalize_piano_roll(segment)\n",
        "\n",
        "                    # Extract musical features\n",
        "                    features = extract_musical_features(normalized_segment)\n",
        "\n",
        "                    # Quality check: skip if too sparse\n",
        "                    note_density = features['avg_notes_per_time']\n",
        "                    if note_density < 0.1:  # Very sparse, likely poor quality\n",
        "                        continue\n",
        "\n",
        "                    composer_data.append(normalized_segment)\n",
        "                    composer_labels.append(composer_to_idx[composer])\n",
        "                    composer_features.append(features)\n",
        "                    segments_created += 1\n",
        "\n",
        "                files_processed += 1\n",
        "\n",
        "                if files_processed % 10 == 0:\n",
        "                    print(f\"  Processed {files_processed} files, created {segments_created} segments...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error processing {file}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"‚úÖ {composer}: {files_processed} files ‚Üí {segments_created} segments\")\n",
        "\n",
        "        if composer_data:\n",
        "            composer_data = np.array(composer_data)\n",
        "            composer_labels = np.array(composer_labels)\n",
        "\n",
        "            all_data.append(composer_data)\n",
        "            all_labels.append(composer_labels)\n",
        "            all_features.extend(composer_features)\n",
        "\n",
        "            print(f\"  Final data shape: {composer_data.shape}\")\n",
        "\n",
        "    # Combine all data\n",
        "    if all_data:\n",
        "        data = np.concatenate(all_data, axis=0)\n",
        "        labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "        print(f\"\\nüéØ FINAL IMPROVED DATASET:\")\n",
        "        print(f\"Total samples: {len(data)}\")\n",
        "        print(f\"Data shape: {data.shape}\")\n",
        "        print(f\"Label distribution: {np.bincount(labels)}\")\n",
        "\n",
        "        return data, labels, all_features\n",
        "    else:\n",
        "        print(\"‚ùå No data loaded!\")\n",
        "        return None, None, None\n",
        "\n",
        "# Load the improved dataset\n",
        "print(\"üöÄ Starting improved data loading...\")\n",
        "improved_data, improved_labels, features = load_improved_dataset(\n",
        "    extract_path,\n",
        "    TARGET_COMPOSERS,\n",
        "    target_duration=45.0,  # 45 seconds per segment\n",
        "    max_files_per_composer=120  # Limit for testing - remove for full dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "80f24ee1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80f24ee1",
        "outputId": "7bed835d-4b3d-4ef3-f82a-063995dd83cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ TESTING MODELS ON IMPROVED DATASET\n",
            "==================================================\n",
            "\n",
            "üìä Original Model Results:\n",
            "Accuracy: 68.32%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Bach      0.717     0.875     0.788       312\n",
            "   Beethoven      0.682     0.480     0.564       304\n",
            "      Chopin      0.775     0.724     0.749       286\n",
            "      Mozart      0.553     0.643     0.595       244\n",
            "\n",
            "    accuracy                          0.683      1146\n",
            "   macro avg      0.682     0.681     0.674      1146\n",
            "weighted avg      0.687     0.683     0.677      1146\n",
            "\n",
            "\n",
            "üìä Rhythm Model Results:\n",
            "Accuracy: 72.08%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Bach      0.839     0.817     0.828       312\n",
            "   Beethoven      0.623     0.724     0.670       304\n",
            "      Chopin      0.835     0.689     0.755       286\n",
            "      Mozart      0.609     0.631     0.620       244\n",
            "\n",
            "    accuracy                          0.721      1146\n",
            "   macro avg      0.726     0.715     0.718      1146\n",
            "weighted avg      0.732     0.721     0.723      1146\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# TEST TRAINED MODELS ON IMPROVED DATASET\n",
        "# =====================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create dataset and dataloader for improved data\n",
        "improved_dataset = PianoRollDataset(improved_data, improved_labels)\n",
        "test_loader = DataLoader(improved_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "def evaluate_model(model, dataloader, model_name):\n",
        "    \"\"\"Evaluate a single model\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * np.mean(np.array(all_preds) == np.array(all_targets))\n",
        "\n",
        "    print(f\"\\nüìä {model_name} Results:\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_targets, all_preds,\n",
        "                              target_names=TARGET_COMPOSERS,\n",
        "                              digits=3))\n",
        "\n",
        "    return all_preds, all_targets, all_probs, accuracy\n",
        "\n",
        "# Test both models\n",
        "print(\"üß™ TESTING MODELS ON IMPROVED DATASET\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Test original model\n",
        "orig_preds, targets, orig_probs, orig_acc = evaluate_model(model, test_loader, \"Original Model\")\n",
        "\n",
        "# Test rhythm model\n",
        "rhythm_preds, _, rhythm_probs, rhythm_acc = evaluate_model(rhythm_model, test_loader, \"Rhythm Model\")\n",
        "\n",
        "# Convert to numpy arrays\n",
        "orig_probs = np.array(orig_probs)\n",
        "rhythm_probs = np.array(rhythm_probs)\n",
        "targets = np.array(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d0aefe6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0aefe6d",
        "outputId": "9be63c07-0ae2-4d45-d997-8b4486464e5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Non-overlapping segmentation function defined!\n"
          ]
        }
      ],
      "source": [
        "def get_piano_roll_non_overlapping_segments(midi_path, fs=100, segment_duration=45.0):\n",
        "    \"\"\"\n",
        "    Extract non-overlapping segments to avoid data leakage with LSTM\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pm = pretty_midi.PrettyMIDI(midi_path)\n",
        "\n",
        "        # Get the actual duration of the piece\n",
        "        actual_duration = pm.get_end_time()\n",
        "\n",
        "        # If piece is very short, skip it\n",
        "        if actual_duration < 15.0:  # Less than 15 seconds\n",
        "            return None\n",
        "\n",
        "        segments = []\n",
        "        segment_size = segment_duration\n",
        "\n",
        "        # Calculate number of non-overlapping segments\n",
        "        num_segments = int(actual_duration // segment_size)\n",
        "\n",
        "        # If piece is shorter than one segment, use the whole piece (padded)\n",
        "        if num_segments == 0:\n",
        "            piano_roll = pm.get_piano_roll(fs=fs)\n",
        "            target_length = int(segment_duration * fs)\n",
        "\n",
        "            if piano_roll.shape[1] < target_length:\n",
        "                pad_width = target_length - piano_roll.shape[1]\n",
        "                piano_roll = np.pad(piano_roll, ((0,0),(0,pad_width)), mode='constant')\n",
        "            else:\n",
        "                piano_roll = piano_roll[:, :target_length]\n",
        "\n",
        "            segments.append(piano_roll)\n",
        "            return segments\n",
        "\n",
        "        # Extract non-overlapping segments\n",
        "        for i in range(num_segments):\n",
        "            start_time = i * segment_size\n",
        "            end_time = start_time + segment_size\n",
        "\n",
        "            # Create a copy and trim\n",
        "            pm_segment = pretty_midi.PrettyMIDI()\n",
        "            for instrument in pm.instruments:\n",
        "                new_instrument = pretty_midi.Instrument(\n",
        "                    program=instrument.program,\n",
        "                    is_drum=instrument.is_drum,\n",
        "                    name=instrument.name\n",
        "                )\n",
        "\n",
        "                for note in instrument.notes:\n",
        "                    if start_time <= note.start < end_time:\n",
        "                        new_note = pretty_midi.Note(\n",
        "                            velocity=note.velocity,\n",
        "                            pitch=note.pitch,\n",
        "                            start=note.start - start_time,\n",
        "                            end=min(note.end - start_time, segment_duration)\n",
        "                        )\n",
        "                        new_instrument.notes.append(new_note)\n",
        "\n",
        "                if new_instrument.notes:\n",
        "                    pm_segment.instruments.append(new_instrument)\n",
        "\n",
        "            if pm_segment.instruments:\n",
        "                piano_roll = pm_segment.get_piano_roll(fs=fs)\n",
        "                target_length = int(segment_duration * fs)\n",
        "\n",
        "                if piano_roll.shape[1] > target_length:\n",
        "                    piano_roll = piano_roll[:, :target_length]\n",
        "                else:\n",
        "                    pad_width = target_length - piano_roll.shape[1]\n",
        "                    piano_roll = np.pad(piano_roll, ((0,0),(0,pad_width)), mode='constant')\n",
        "\n",
        "                segments.append(piano_roll)\n",
        "\n",
        "        return segments if segments else None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {midi_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ Non-overlapping segmentation function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "43715e17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43715e17",
        "outputId": "3baeafb0-c3ca-48bf-9370-b0c6437e425b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting FULL non-overlapping segment data loading...\n",
            "üéµ LOADING FULL DATASET WITH NON-OVERLAPPING SEGMENTS...\n",
            "Benefits for LSTM training:\n",
            "‚Ä¢ Segment duration: 45.0s (no overlap)\n",
            "‚Ä¢ Using ALL available files (no limits)\n",
            "‚Ä¢ No data leakage between train/test\n",
            "‚Ä¢ Cleaner temporal boundaries\n",
            "‚Ä¢ Will address class imbalance later during training\n",
            "\n",
            "--- Processing Bach ---\n",
            "  Found 131 MIDI files for Bach\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processed 50/131 files, created 394 segments...\n",
            "  Processed 100/131 files, created 715 segments...\n",
            "‚úÖ Bach: 131/131 files ‚Üí 977 segments\n",
            "  Final data shape: (977, 128, 4500)\n",
            "\n",
            "--- Processing Beethoven ---\n",
            "  Found 134 MIDI files for Beethoven\n",
            "Error processing data/kaggle/midiclassics/Beethoven/Anhang 14-3.mid: Could not decode key with 3 flats and mode 255\n",
            "  Processed 50/134 files, created 357 segments...\n",
            "  Processed 100/134 files, created 765 segments...\n",
            "‚úÖ Beethoven: 133/134 files ‚Üí 987 segments\n",
            "  Final data shape: (987, 128, 4500)\n",
            "\n",
            "--- Processing Chopin ---\n",
            "  Found 136 MIDI files for Chopin\n",
            "  Processed 50/136 files, created 218 segments...\n",
            "  Processed 100/136 files, created 449 segments...\n",
            "‚úÖ Chopin: 136/136 files ‚Üí 610 segments\n",
            "  Final data shape: (610, 128, 4500)\n",
            "\n",
            "--- Processing Mozart ---\n",
            "  Found 90 MIDI files for Mozart\n",
            "  Processed 50/90 files, created 271 segments...\n",
            "‚úÖ Mozart: 90/90 files ‚Üí 524 segments\n",
            "  Final data shape: (524, 128, 4500)\n",
            "\n",
            "üéØ FINAL FULL NON-OVERLAPPING DATASET:\n",
            "Total files processed: 490\n",
            "Total samples: 3098\n",
            "Data shape: (3098, 128, 4500)\n",
            "Label distribution: [977 987 610 524]\n",
            "  Bach: 977 samples (31.5%)\n",
            "  Beethoven: 987 samples (31.9%)\n",
            "  Chopin: 610 samples (19.7%)\n",
            "  Mozart: 524 samples (16.9%)\n"
          ]
        }
      ],
      "source": [
        "def load_dataset_non_overlapping_full(extract_path, target_composers, segment_duration=45.0):\n",
        "    \"\"\"\n",
        "    Load FULL dataset with non-overlapping segments - using ALL available files\n",
        "    \"\"\"\n",
        "    print(\"üéµ LOADING FULL DATASET WITH NON-OVERLAPPING SEGMENTS...\")\n",
        "    print(\"Benefits for LSTM training:\")\n",
        "    print(f\"‚Ä¢ Segment duration: {segment_duration}s (no overlap)\")\n",
        "    print(\"‚Ä¢ Using ALL available files (no limits)\")\n",
        "    print(\"‚Ä¢ No data leakage between train/test\")\n",
        "    print(\"‚Ä¢ Cleaner temporal boundaries\")\n",
        "    print(\"‚Ä¢ Will address class imbalance later during training\")\n",
        "\n",
        "    composer_to_idx = {c: i for i, c in enumerate(target_composers)}\n",
        "    all_data = []\n",
        "    all_labels = []\n",
        "    all_features = []\n",
        "\n",
        "    total_files_processed = 0\n",
        "    total_segments_created = 0\n",
        "\n",
        "    for composer in target_composers:\n",
        "        print(f\"\\n--- Processing {composer} ---\")\n",
        "        composer_dir = os.path.join(extract_path, composer)\n",
        "\n",
        "        if not os.path.isdir(composer_dir):\n",
        "            print(f\"Directory not found: {composer_dir}\")\n",
        "            continue\n",
        "\n",
        "        composer_data = []\n",
        "        composer_labels = []\n",
        "        composer_features = []\n",
        "        files_processed = 0\n",
        "        segments_created = 0\n",
        "\n",
        "        # Get ALL MIDI files - no limit\n",
        "        midi_files = [f for f in os.listdir(composer_dir)\n",
        "                     if f.lower().endswith(('.mid', '.midi'))]\n",
        "\n",
        "        print(f\"  Found {len(midi_files)} MIDI files for {composer}\")\n",
        "\n",
        "        for file in midi_files:\n",
        "            midi_path = os.path.join(composer_dir, file)\n",
        "\n",
        "            try:\n",
        "                # Use non-overlapping segmentation\n",
        "                segments = get_piano_roll_non_overlapping_segments(\n",
        "                    midi_path,\n",
        "                    segment_duration=segment_duration\n",
        "                )\n",
        "\n",
        "                if segments is None:\n",
        "                    continue\n",
        "\n",
        "                for segment in segments:\n",
        "                    # Normalize the segment\n",
        "                    normalized_segment = normalize_piano_roll(segment)\n",
        "\n",
        "                    # Extract musical features\n",
        "                    features = extract_musical_features(normalized_segment)\n",
        "\n",
        "                    # Quality check: skip if too sparse\n",
        "                    note_density = features['avg_notes_per_time']\n",
        "                    if note_density < 0.1:  # Very sparse, likely poor quality\n",
        "                        continue\n",
        "\n",
        "                    composer_data.append(normalized_segment)\n",
        "                    composer_labels.append(composer_to_idx[composer])\n",
        "                    composer_features.append(features)\n",
        "                    segments_created += 1\n",
        "\n",
        "                files_processed += 1\n",
        "\n",
        "                # Progress update every 50 files for full dataset\n",
        "                if files_processed % 50 == 0:\n",
        "                    print(f\"  Processed {files_processed}/{len(midi_files)} files, created {segments_created} segments...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error processing {file}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"‚úÖ {composer}: {files_processed}/{len(midi_files)} files ‚Üí {segments_created} segments\")\n",
        "\n",
        "        if composer_data:\n",
        "            composer_data = np.array(composer_data)\n",
        "            composer_labels = np.array(composer_labels)\n",
        "\n",
        "            all_data.append(composer_data)\n",
        "            all_labels.append(composer_labels)\n",
        "            all_features.extend(composer_features)\n",
        "\n",
        "            print(f\"  Final data shape: {composer_data.shape}\")\n",
        "\n",
        "        total_files_processed += files_processed\n",
        "        total_segments_created += segments_created\n",
        "\n",
        "    # Combine all data\n",
        "    if all_data:\n",
        "        data = np.concatenate(all_data, axis=0)\n",
        "        labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "        print(f\"\\nüéØ FINAL FULL NON-OVERLAPPING DATASET:\")\n",
        "        print(f\"Total files processed: {total_files_processed}\")\n",
        "        print(f\"Total samples: {len(data)}\")\n",
        "        print(f\"Data shape: {data.shape}\")\n",
        "        print(f\"Label distribution: {np.bincount(labels)}\")\n",
        "\n",
        "        # Show class distribution percentages\n",
        "        for i, composer in enumerate(target_composers):\n",
        "            count = np.sum(labels == i)\n",
        "            percentage = (count / len(labels)) * 100\n",
        "            print(f\"  {composer}: {count} samples ({percentage:.1f}%)\")\n",
        "\n",
        "        return data, labels, all_features\n",
        "    else:\n",
        "        print(\"‚ùå No data loaded!\")\n",
        "        return None, None, None\n",
        "\n",
        "# Load the FULL dataset with non-overlapping segments\n",
        "print(\"üöÄ Starting FULL non-overlapping segment data loading...\")\n",
        "full_data, full_labels, full_features = load_dataset_non_overlapping_full(\n",
        "    extract_path,\n",
        "    TARGET_COMPOSERS,\n",
        "    segment_duration=45.0  # 45-second segments, no overlap, ALL files\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "1ecadfdd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ecadfdd",
        "outputId": "60dc84a6-510d-47fc-ae75-03a0718643f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Creating AGGRESSIVE model for A100 40GB...\n",
            "üöÄ Building AGGRESSIVE CNN-LSTM-Transformer for A100 40GB...\n",
            "‚Ä¢ Deep CNN feature extraction (6 blocks)\n",
            "‚Ä¢ Large LSTM temporal modeling (hidden: 512)\n",
            "‚Ä¢ Deep Transformer self-attention (dim: 1024, heads: 16, layers: 8)\n",
            "‚Ä¢ Multi-scale feature fusion\n",
            "‚Ä¢ Advanced attention mechanisms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ AGGRESSIVE CNN-LSTM-Transformer architecture built!\n",
            "Input shape: torch.Size([4, 1, 128, 4500])\n",
            "Main output shape: torch.Size([4, 4])\n",
            "Auxiliary output shape: torch.Size([4, 4])\n",
            "‚úÖ Aggressive model forward pass successful!\n",
            "\n",
            "üìä AGGRESSIVE Model Statistics:\n",
            "Total parameters: 185,688,776\n",
            "Trainable parameters: 185,688,776\n",
            "Model size: ~708.3 MB\n",
            "Estimated GPU memory (training): ~2833.4 MB\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# AGGRESSIVE CNN-LSTM-TRANSFORMER FOR A100 40GB\n",
        "# =====================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Enhanced positional encoding for transformer\"\"\"\n",
        "    def __init__(self, d_model, max_len=10000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "class AggressiveCNN_LSTM_Transformer(nn.Module):\n",
        "    def __init__(self, num_classes=4, lstm_hidden=512, transformer_dim=1024, num_heads=16, num_layers=8):\n",
        "        super(AggressiveCNN_LSTM_Transformer, self).__init__()\n",
        "\n",
        "        print(\"üöÄ Building AGGRESSIVE CNN-LSTM-Transformer for A100 40GB...\")\n",
        "        print(f\"‚Ä¢ Deep CNN feature extraction (6 blocks)\")\n",
        "        print(f\"‚Ä¢ Large LSTM temporal modeling (hidden: {lstm_hidden})\")\n",
        "        print(f\"‚Ä¢ Deep Transformer self-attention (dim: {transformer_dim}, heads: {num_heads}, layers: {num_layers})\")\n",
        "        print(f\"‚Ä¢ Multi-scale feature fusion\")\n",
        "        print(f\"‚Ä¢ Advanced attention mechanisms\")\n",
        "\n",
        "        # ==========================================\n",
        "        # DEEP CNN BACKBONE - 6 BLOCKS\n",
        "        # ==========================================\n",
        "\n",
        "        # Block 1: Initial feature extraction\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.1),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))  # 128x64 -> 64x32\n",
        "        )\n",
        "\n",
        "        # Block 2: Deeper features\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.15),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))  # 64x32 -> 32x16\n",
        "        )\n",
        "\n",
        "        # Block 3: More complex patterns\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.2),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))  # 32x16 -> 16x8\n",
        "        )\n",
        "\n",
        "        # Block 4: High-level features\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.25),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))  # 16x8 -> 8x4\n",
        "        )\n",
        "\n",
        "        # Block 5: Abstract features\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(512, 768, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(768),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(768, 768, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(768),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.3),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))  # 8x4 -> 4x2\n",
        "        )\n",
        "\n",
        "        # Block 6: Final feature extraction\n",
        "        self.conv6 = nn.Sequential(\n",
        "            nn.Conv2d(768, 1024, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(1024, 1024, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.35),\n",
        "            nn.AdaptiveAvgPool2d((2, 1))  # Ensure consistent output: 2x1\n",
        "        )\n",
        "\n",
        "        # ==========================================\n",
        "        # LARGE BIDIRECTIONAL LSTM\n",
        "        # ==========================================\n",
        "        self.feature_size = 1024 * 2  # 1024 channels * 2x1 spatial\n",
        "        self.lstm_hidden = lstm_hidden\n",
        "\n",
        "        # Multi-layer LSTM with larger capacity\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.feature_size,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=4,  # Deeper LSTM\n",
        "            batch_first=True,\n",
        "            dropout=0.3,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Additional LSTM for temporal refinement\n",
        "        self.lstm_refine = nn.LSTM(\n",
        "            input_size=lstm_hidden * 2,\n",
        "            hidden_size=lstm_hidden // 2,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=0.2,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # ==========================================\n",
        "        # DEEP TRANSFORMER ENCODER\n",
        "        # ==========================================\n",
        "        self.transformer_dim = transformer_dim\n",
        "\n",
        "        # Project LSTM output to transformer dimension\n",
        "        self.lstm_to_transformer = nn.Sequential(\n",
        "            nn.Linear(lstm_hidden, transformer_dim),\n",
        "            nn.LayerNorm(transformer_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(transformer_dim, max_len=10000)\n",
        "\n",
        "        # Deep transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=transformer_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=transformer_dim * 4,  # Large feedforward\n",
        "            dropout=0.1,\n",
        "            activation='gelu',  # GELU activation for better performance\n",
        "            batch_first=True,\n",
        "            norm_first=True  # Pre-norm for better training stability\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers,\n",
        "            norm=nn.LayerNorm(transformer_dim)\n",
        "        )\n",
        "\n",
        "        # ==========================================\n",
        "        # MULTI-SCALE ATTENTION & FUSION\n",
        "        # ==========================================\n",
        "\n",
        "        # Multi-scale attention heads\n",
        "        self.global_attention = nn.MultiheadAttention(\n",
        "            embed_dim=transformer_dim,\n",
        "            num_heads=num_heads,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.local_attention = nn.MultiheadAttention(\n",
        "            embed_dim=transformer_dim,\n",
        "            num_heads=num_heads // 2,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Cross-attention between global and local features\n",
        "        self.cross_attention = nn.MultiheadAttention(\n",
        "            embed_dim=transformer_dim,\n",
        "            num_heads=num_heads // 2,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Feature fusion\n",
        "        self.feature_fusion = nn.Sequential(\n",
        "            nn.Linear(transformer_dim * 3, transformer_dim),\n",
        "            nn.LayerNorm(transformer_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        # ==========================================\n",
        "        # ADVANCED CLASSIFICATION HEAD\n",
        "        # ==========================================\n",
        "\n",
        "        # Hierarchical classification with multiple paths\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(transformer_dim),\n",
        "            nn.Linear(transformer_dim, 2048),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        # Auxiliary classifier for regularization\n",
        "        self.aux_classifier = nn.Sequential(\n",
        "            nn.Linear(lstm_hidden, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ AGGRESSIVE CNN-LSTM-Transformer architecture built!\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # ==========================================\n",
        "        # DEEP CNN FEATURE EXTRACTION\n",
        "        # ==========================================\n",
        "        x = self.conv1(x)      # (batch, 64, 64, T/2)\n",
        "        x = self.conv2(x)      # (batch, 128, 32, T/4)\n",
        "        x = self.conv3(x)      # (batch, 256, 16, T/8)\n",
        "        x = self.conv4(x)      # (batch, 512, 8, T/16)\n",
        "        x = self.conv5(x)      # (batch, 768, 4, T/32)\n",
        "        x = self.conv6(x)      # (batch, 1024, 2, T/64)\n",
        "\n",
        "        # Reshape for LSTM: (batch, time_steps, features)\n",
        "        x = x.permute(0, 3, 1, 2)  # (batch, T/64, 1024, 2)\n",
        "        x = x.contiguous().view(batch_size, x.size(1), -1)  # (batch, T/64, 1024*2)\n",
        "\n",
        "        # ==========================================\n",
        "        # LARGE LSTM PROCESSING\n",
        "        # ==========================================\n",
        "        lstm_out, _ = self.lstm(x)  # (batch, T/64, lstm_hidden*2)\n",
        "        lstm_refined, _ = self.lstm_refine(lstm_out)  # (batch, T/64, lstm_hidden)\n",
        "\n",
        "        # Auxiliary classification from LSTM features (for regularization)\n",
        "        lstm_pooled = torch.mean(lstm_refined, dim=1)\n",
        "        aux_output = self.aux_classifier(lstm_pooled)\n",
        "\n",
        "        # ==========================================\n",
        "        # DEEP TRANSFORMER PROCESSING\n",
        "        # ==========================================\n",
        "\n",
        "        # Project to transformer dimension\n",
        "        transformer_input = self.lstm_to_transformer(lstm_refined)  # (batch, T/64, transformer_dim)\n",
        "\n",
        "        # Add positional encoding\n",
        "        transformer_input = transformer_input.transpose(0, 1)  # (T/64, batch, transformer_dim)\n",
        "        transformer_input = self.pos_encoding(transformer_input)\n",
        "        transformer_input = transformer_input.transpose(0, 1)  # (batch, T/64, transformer_dim)\n",
        "\n",
        "        # Deep transformer encoding\n",
        "        transformer_out = self.transformer_encoder(transformer_input)  # (batch, T/64, transformer_dim)\n",
        "\n",
        "        # ==========================================\n",
        "        # MULTI-SCALE ATTENTION & FUSION\n",
        "        # ==========================================\n",
        "\n",
        "        # Global attention (full sequence)\n",
        "        global_attended, _ = self.global_attention(\n",
        "            transformer_out, transformer_out, transformer_out\n",
        "        )\n",
        "\n",
        "        # Local attention (sliding window - simulate by chunking)\n",
        "        seq_len = transformer_out.size(1)\n",
        "        if seq_len > 16:\n",
        "            # Use overlapping windows\n",
        "            local_features = []\n",
        "            window_size = min(16, seq_len)\n",
        "            for i in range(0, max(1, seq_len - window_size + 1), window_size // 2):\n",
        "                end_idx = min(i + window_size, seq_len)\n",
        "                window = transformer_out[:, i:end_idx, :]\n",
        "                local_att, _ = self.local_attention(window, window, window)\n",
        "                local_features.append(torch.mean(local_att, dim=1, keepdim=True))\n",
        "            local_attended = torch.cat(local_features, dim=1)\n",
        "        else:\n",
        "            local_attended, _ = self.local_attention(\n",
        "                transformer_out, transformer_out, transformer_out\n",
        "            )\n",
        "\n",
        "        # Cross attention between global and local\n",
        "        cross_attended, _ = self.cross_attention(\n",
        "            global_attended, local_attended, local_attended\n",
        "        )\n",
        "\n",
        "        # Fusion of multi-scale features\n",
        "        # Pool to same size for concatenation\n",
        "        global_pooled = torch.mean(global_attended, dim=1)\n",
        "        local_pooled = torch.mean(local_attended, dim=1)\n",
        "        cross_pooled = torch.mean(cross_attended, dim=1)\n",
        "\n",
        "        fused_features = torch.cat([global_pooled, local_pooled, cross_pooled], dim=1)\n",
        "        final_features = self.feature_fusion(fused_features)\n",
        "\n",
        "        # ==========================================\n",
        "        # CLASSIFICATION\n",
        "        # ==========================================\n",
        "        main_output = self.classifier(final_features)\n",
        "\n",
        "        return main_output, aux_output\n",
        "\n",
        "# Create the aggressive model\n",
        "print(\"üöÄ Creating AGGRESSIVE model for A100 40GB...\")\n",
        "aggressive_model = AggressiveCNN_LSTM_Transformer(\n",
        "    num_classes=4,\n",
        "    lstm_hidden=512,        # Doubled from 256\n",
        "    transformer_dim=1024,   # Doubled from 512\n",
        "    num_heads=16,          # Doubled from 8\n",
        "    num_layers=8           # Doubled from 4\n",
        ").to(device)\n",
        "\n",
        "# Test with dummy input\n",
        "test_input = torch.randn(4, 1, 128, 4500).to(device)  # Larger batch\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    main_out, aux_out = aggressive_model(test_input)\n",
        "    print(f\"Main output shape: {main_out.shape}\")\n",
        "    print(f\"Auxiliary output shape: {aux_out.shape}\")\n",
        "    print(f\"‚úÖ Aggressive model forward pass successful!\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in aggressive_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in aggressive_model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nüìä AGGRESSIVE Model Statistics:\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n",
        "print(f\"Estimated GPU memory (training): ~{total_params * 16 / 1024 / 1024:.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "145287c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "145287c4",
        "outputId": "a413d900-63dc-47aa-821a-92a32e2e7bc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting piece-tracked data loading...\n",
            "üéµ LOADING DATASET WITH PIECE TRACKING...\n",
            "Key improvements:\n",
            "‚Ä¢ Track piece identity for each segment\n",
            "‚Ä¢ Enable consecutive segment modeling\n",
            "‚Ä¢ Support contrastive learning approaches\n",
            "‚Ä¢ Segment duration: 45.0s\n",
            "\n",
            "--- Processing Bach ---\n",
            "  Found 131 MIDI files for Bach\n",
            "  Processed 50/131 files, created 394 segments...\n",
            "  Processed 100/131 files, created 715 segments...\n",
            "‚úÖ Bach: 131/131 files ‚Üí 977 segments\n",
            "\n",
            "--- Processing Beethoven ---\n",
            "  Found 134 MIDI files for Beethoven\n",
            "Error processing data/kaggle/midiclassics/Beethoven/Anhang 14-3.mid: Could not decode key with 3 flats and mode 255\n",
            "  Processed 50/134 files, created 357 segments...\n",
            "  Processed 100/134 files, created 765 segments...\n",
            "‚úÖ Beethoven: 133/134 files ‚Üí 987 segments\n",
            "\n",
            "--- Processing Chopin ---\n",
            "  Found 136 MIDI files for Chopin\n",
            "  Processed 50/136 files, created 218 segments...\n",
            "  Processed 100/136 files, created 449 segments...\n",
            "‚úÖ Chopin: 136/136 files ‚Üí 610 segments\n",
            "\n",
            "--- Processing Mozart ---\n",
            "  Found 90 MIDI files for Mozart\n",
            "  Processed 50/90 files, created 271 segments...\n",
            "‚úÖ Mozart: 90/90 files ‚Üí 524 segments\n",
            "\n",
            "üéØ FINAL PIECE-TRACKED DATASET:\n",
            "Total files processed: 490\n",
            "Total segments: 3098\n",
            "Total unique pieces: 490\n",
            "Data shape: (3098, 128, 4500)\n",
            "Label distribution: [977 987 610 524]\n",
            "Segments per piece - Mean: 6.3, Min: 1, Max: 30\n",
            "  Bach: 977 segments (31.5%)\n",
            "  Beethoven: 987 segments (31.9%)\n",
            "  Chopin: 610 segments (19.7%)\n",
            "  Mozart: 524 segments (16.9%)\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# SEQUENCE-AWARE DATA LOADING WITH PIECE TRACKING\n",
        "# =====================================================\n",
        "\n",
        "def load_dataset_with_piece_tracking(extract_path, target_composers, segment_duration=45.0):\n",
        "    \"\"\"\n",
        "    Load dataset while tracking which segments belong to the same piece\n",
        "    This enables sequence-aware training that distinguishes:\n",
        "    - Consecutive segments from SAME piece vs DIFFERENT pieces\n",
        "    - Same composer vs different composer relationships\n",
        "    \"\"\"\n",
        "    print(\"üéµ LOADING DATASET WITH PIECE TRACKING...\")\n",
        "    print(\"Key improvements:\")\n",
        "    print(f\"‚Ä¢ Track piece identity for each segment\")\n",
        "    print(f\"‚Ä¢ Enable consecutive segment modeling\")\n",
        "    print(f\"‚Ä¢ Support contrastive learning approaches\")\n",
        "    print(f\"‚Ä¢ Segment duration: {segment_duration}s\")\n",
        "\n",
        "    composer_to_idx = {c: i for i, c in enumerate(target_composers)}\n",
        "    all_data = []\n",
        "    all_labels = []\n",
        "    all_piece_ids = []  # NEW: Track which piece each segment comes from\n",
        "    all_piece_names = []  # NEW: Track piece names for analysis\n",
        "\n",
        "    piece_id_counter = 0\n",
        "    total_files_processed = 0\n",
        "    total_segments_created = 0\n",
        "\n",
        "    for composer in target_composers:\n",
        "        print(f\"\\n--- Processing {composer} ---\")\n",
        "        composer_dir = os.path.join(extract_path, composer)\n",
        "\n",
        "        if not os.path.isdir(composer_dir):\n",
        "            print(f\"Directory not found: {composer_dir}\")\n",
        "            continue\n",
        "\n",
        "        midi_files = [f for f in os.listdir(composer_dir)\n",
        "                     if f.lower().endswith(('.mid', '.midi'))]\n",
        "\n",
        "        print(f\"  Found {len(midi_files)} MIDI files for {composer}\")\n",
        "        files_processed = 0\n",
        "        segments_created = 0\n",
        "\n",
        "        for file in midi_files:\n",
        "            midi_path = os.path.join(composer_dir, file)\n",
        "\n",
        "            try:\n",
        "                # Use non-overlapping segmentation\n",
        "                segments = get_piano_roll_non_overlapping_segments(\n",
        "                    midi_path, segment_duration=segment_duration\n",
        "                )\n",
        "\n",
        "                if segments is None or len(segments) == 0:\n",
        "                    continue\n",
        "\n",
        "                # All segments from this file get the same piece_id\n",
        "                current_piece_id = piece_id_counter\n",
        "                piece_name = f\"{composer}_{file}\"\n",
        "                piece_id_counter += 1\n",
        "\n",
        "                valid_segments_from_piece = 0\n",
        "\n",
        "                for segment_idx, segment in enumerate(segments):\n",
        "                    # Normalize the segment\n",
        "                    normalized_segment = normalize_piano_roll(segment)\n",
        "\n",
        "                    # Extract musical features\n",
        "                    features = extract_musical_features(normalized_segment)\n",
        "\n",
        "                    # Quality check: skip if too sparse\n",
        "                    if features['avg_notes_per_time'] < 0.1:\n",
        "                        continue\n",
        "\n",
        "                    all_data.append(normalized_segment)\n",
        "                    all_labels.append(composer_to_idx[composer])\n",
        "                    all_piece_ids.append(current_piece_id)\n",
        "                    all_piece_names.append(f\"{piece_name}_seg{segment_idx}\")\n",
        "\n",
        "                    valid_segments_from_piece += 1\n",
        "                    segments_created += 1\n",
        "\n",
        "                if valid_segments_from_piece > 0:\n",
        "                    files_processed += 1\n",
        "\n",
        "                # Progress update\n",
        "                if files_processed % 50 == 0:\n",
        "                    print(f\"  Processed {files_processed}/{len(midi_files)} files, created {segments_created} segments...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error processing {file}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"‚úÖ {composer}: {files_processed}/{len(midi_files)} files ‚Üí {segments_created} segments\")\n",
        "        total_files_processed += files_processed\n",
        "        total_segments_created += segments_created\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    data = np.array(all_data)\n",
        "    labels = np.array(all_labels)\n",
        "    piece_ids = np.array(all_piece_ids)\n",
        "\n",
        "    print(f\"\\nüéØ FINAL PIECE-TRACKED DATASET:\")\n",
        "    print(f\"Total files processed: {total_files_processed}\")\n",
        "    print(f\"Total segments: {len(data)}\")\n",
        "    print(f\"Total unique pieces: {len(np.unique(piece_ids))}\")\n",
        "    print(f\"Data shape: {data.shape}\")\n",
        "    print(f\"Label distribution: {np.bincount(labels)}\")\n",
        "\n",
        "    # Analyze segments per piece\n",
        "    segments_per_piece = []\n",
        "    for piece_id in np.unique(piece_ids):\n",
        "        count = np.sum(piece_ids == piece_id)\n",
        "        segments_per_piece.append(count)\n",
        "\n",
        "    print(f\"Segments per piece - Mean: {np.mean(segments_per_piece):.1f}, \"\n",
        "          f\"Min: {np.min(segments_per_piece)}, Max: {np.max(segments_per_piece)}\")\n",
        "\n",
        "    # Show class distribution percentages\n",
        "    for i, composer in enumerate(target_composers):\n",
        "        count = np.sum(labels == i)\n",
        "        percentage = (count / len(labels)) * 100\n",
        "        print(f\"  {composer}: {count} segments ({percentage:.1f}%)\")\n",
        "\n",
        "    return data, labels, piece_ids, all_piece_names\n",
        "\n",
        "# Load the dataset with piece tracking\n",
        "print(\"üöÄ Starting piece-tracked data loading...\")\n",
        "tracked_data, tracked_labels, tracked_piece_ids, piece_names = load_dataset_with_piece_tracking(\n",
        "    extract_path,\n",
        "    TARGET_COMPOSERS,\n",
        "    segment_duration=45.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "3a85def4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a85def4",
        "outputId": "b9550997-87f4-4dec-f3c0-d5209298e1b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öñÔ∏è COMPUTING CLASS WEIGHTS...\n",
            "Original distribution:\n",
            "  Bach: 977 samples (31.5%)\n",
            "  Beethoven: 987 samples (31.9%)\n",
            "  Chopin: 610 samples (19.7%)\n",
            "  Mozart: 524 samples (16.9%)\n",
            "\n",
            "Computed class weights:\n",
            "  Bach: 0.793\n",
            "  Beethoven: 0.785\n",
            "  Chopin: 1.270\n",
            "  Mozart: 1.478\n",
            "\n",
            "‚úÖ Class weights ready for CrossEntropyLoss\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# CLASS WEIGHTS IMPLEMENTATION\n",
        "# =====================================================\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch.nn as nn\n",
        "\n",
        "# Compute class weights for our imbalanced dataset\n",
        "def compute_class_weights(labels, target_composers):\n",
        "    \"\"\"\n",
        "    Compute class weights to handle imbalance\n",
        "    \"\"\"\n",
        "    print(\"‚öñÔ∏è COMPUTING CLASS WEIGHTS...\")\n",
        "\n",
        "    # Get unique labels and compute balanced weights\n",
        "    unique_labels = np.unique(labels)\n",
        "    class_weights = compute_class_weight(\n",
        "        'balanced',\n",
        "        classes=unique_labels,\n",
        "        y=labels\n",
        "    )\n",
        "\n",
        "    print(f\"Original distribution:\")\n",
        "    for i, composer in enumerate(target_composers):\n",
        "        count = np.sum(labels == i)\n",
        "        percentage = (count / len(labels)) * 100\n",
        "        print(f\"  {composer}: {count} samples ({percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nComputed class weights:\")\n",
        "    for i, (composer, weight) in enumerate(zip(target_composers, class_weights)):\n",
        "        print(f\"  {composer}: {weight:.3f}\")\n",
        "\n",
        "    # Convert to PyTorch tensor\n",
        "    class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
        "\n",
        "    print(f\"\\n‚úÖ Class weights ready for CrossEntropyLoss\")\n",
        "    return class_weights_tensor\n",
        "\n",
        "# Compute class weights for our tracked dataset\n",
        "class_weights = compute_class_weights(tracked_labels, TARGET_COMPOSERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5673124f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5673124f",
        "outputId": "fb58f68f-28ee-4de5-fad8-7b23dd95b255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Creating sequence-aware dataset...\n",
            "üìä SEQUENCE DATASET CREATED:\n",
            "‚Ä¢ Sequence length: 2\n",
            "‚Ä¢ Total pieces: 490\n",
            "‚Ä¢ Total sequences: 5706\n",
            "‚Ä¢ Include single segments: True\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# SEQUENCE-AWARE DATASET FOR CONSECUTIVE SEGMENTS\n",
        "# =====================================================\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "class SequenceAwareDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset that creates sequences of consecutive segments from same pieces\n",
        "    This enables the model to learn temporal relationships within compositions\n",
        "    \"\"\"\n",
        "    def __init__(self, data, labels, piece_ids, sequence_length=2, include_singles=True):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "        self.piece_ids = torch.tensor(piece_ids, dtype=torch.long)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.include_singles = include_singles\n",
        "\n",
        "        # Group segments by piece\n",
        "        self.piece_segments = {}\n",
        "        for idx, piece_id in enumerate(piece_ids):\n",
        "            piece_id = int(piece_id)\n",
        "            if piece_id not in self.piece_segments:\n",
        "                self.piece_segments[piece_id] = []\n",
        "            self.piece_segments[piece_id].append(idx)\n",
        "\n",
        "        # Create sequence indices\n",
        "        self.sequence_indices = self._create_sequence_indices()\n",
        "\n",
        "        print(f\"üìä SEQUENCE DATASET CREATED:\")\n",
        "        print(f\"‚Ä¢ Sequence length: {sequence_length}\")\n",
        "        print(f\"‚Ä¢ Total pieces: {len(self.piece_segments)}\")\n",
        "        print(f\"‚Ä¢ Total sequences: {len(self.sequence_indices)}\")\n",
        "        print(f\"‚Ä¢ Include single segments: {include_singles}\")\n",
        "\n",
        "    def _create_sequence_indices(self):\n",
        "        \"\"\"Create indices for all possible sequences\"\"\"\n",
        "        sequences = []\n",
        "\n",
        "        # Add consecutive sequences from same pieces\n",
        "        for piece_id, segment_indices in self.piece_segments.items():\n",
        "            if len(segment_indices) >= self.sequence_length:\n",
        "                # Create all possible consecutive sequences from this piece\n",
        "                for start_idx in range(len(segment_indices) - self.sequence_length + 1):\n",
        "                    seq_indices = segment_indices[start_idx:start_idx + self.sequence_length]\n",
        "                    sequences.append({\n",
        "                        'type': 'sequence',\n",
        "                        'indices': seq_indices,\n",
        "                        'piece_id': piece_id,\n",
        "                        'label': int(self.labels[seq_indices[0]])  # All should have same label\n",
        "                    })\n",
        "\n",
        "        # Optionally add single segments\n",
        "        if self.include_singles:\n",
        "            for piece_id, segment_indices in self.piece_segments.items():\n",
        "                for idx in segment_indices:\n",
        "                    sequences.append({\n",
        "                        'type': 'single',\n",
        "                        'indices': [idx],\n",
        "                        'piece_id': piece_id,\n",
        "                        'label': int(self.labels[idx])\n",
        "                    })\n",
        "\n",
        "        return sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequence_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence_info = self.sequence_indices[idx]\n",
        "        indices = sequence_info['indices']\n",
        "\n",
        "        if len(indices) == 1:\n",
        "            # Single segment\n",
        "            segment = self.data[indices[0]].unsqueeze(0)  # Add channel dim\n",
        "            return segment, sequence_info['label'], sequence_info['piece_id'], 'single'\n",
        "        else:\n",
        "            # Multiple segments - stack them\n",
        "            segments = []\n",
        "            for seg_idx in indices:\n",
        "                segments.append(self.data[seg_idx].unsqueeze(0))  # Add channel dim\n",
        "\n",
        "            # Stack segments: (sequence_length, 1, 128, T)\n",
        "            sequence = torch.stack(segments, dim=0)\n",
        "\n",
        "            return sequence, sequence_info['label'], sequence_info['piece_id'], 'sequence'\n",
        "\n",
        "# Create sequence-aware dataset\n",
        "print(\"üîó Creating sequence-aware dataset...\")\n",
        "sequence_dataset = SequenceAwareDataset(\n",
        "    tracked_data,\n",
        "    tracked_labels,\n",
        "    tracked_piece_ids,\n",
        "    sequence_length=2,  # Start with pairs\n",
        "    include_singles=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b7683e92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7683e92",
        "outputId": "60b4856c-57e8-4b2a-bc1e-2848818b4d33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ CREATING BALANCED TRAINING SETUP...\n",
            "Using class weights: tensor([0.7927, 0.7847, 1.2697, 1.4781], device='cuda:0')\n",
            "üìä DATA SPLIT:\n",
            "Train pieces: 318 | segments: 1986\n",
            "Val pieces:   74 | segments: 438\n",
            "Test pieces:  98 | segments: 674\n",
            "\n",
            "Train distribution:\n",
            "  Bach: 606 (30.5%)\n",
            "  Beethoven: 600 (30.2%)\n",
            "  Chopin: 443 (22.3%)\n",
            "  Mozart: 337 (17.0%)\n",
            "\n",
            "Val distribution:\n",
            "  Bach: 154 (35.2%)\n",
            "  Beethoven: 160 (36.5%)\n",
            "  Chopin: 53 (12.1%)\n",
            "  Mozart: 71 (16.2%)\n",
            "\n",
            "Test distribution:\n",
            "  Bach: 217 (32.2%)\n",
            "  Beethoven: 227 (33.7%)\n",
            "  Chopin: 114 (16.9%)\n",
            "  Mozart: 116 (17.2%)\n",
            "\n",
            "‚úÖ Weighted loss function created with class weights\n",
            "\n",
            "üìö DATA LOADERS CREATED:\n",
            "‚Ä¢ Batch size: 16\n",
            "‚Ä¢ Train batches: 125\n",
            "‚Ä¢ Val batches: 28\n",
            "‚Ä¢ Test batches: 43\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# TRAINING SETUP WITH CLASS WEIGHTS\n",
        "# =====================================================\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def create_balanced_training_setup(data, labels, piece_ids, class_weights, test_size=0.2, val_size=0.15):\n",
        "    \"\"\"\n",
        "    Create training setup with class weights for imbalance handling\n",
        "    \"\"\"\n",
        "    print(\"üéØ CREATING BALANCED TRAINING SETUP...\")\n",
        "    print(f\"Using class weights: {class_weights}\")\n",
        "\n",
        "    # Split data at the piece level to avoid data leakage\n",
        "    unique_pieces = np.unique(piece_ids)\n",
        "    piece_labels = np.array([labels[piece_ids == pid][0] for pid in unique_pieces])\n",
        "\n",
        "    # Split pieces into train/val/test\n",
        "    train_pieces, test_pieces, _, _ = train_test_split(\n",
        "        unique_pieces, piece_labels,\n",
        "        test_size=test_size,\n",
        "        stratify=piece_labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    train_pieces, val_pieces, _, _ = train_test_split(\n",
        "        train_pieces, piece_labels[np.isin(unique_pieces, train_pieces)],\n",
        "        test_size=val_size/(1-test_size),\n",
        "        stratify=piece_labels[np.isin(unique_pieces, train_pieces)],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Create masks for train/val/test\n",
        "    train_mask = np.isin(piece_ids, train_pieces)\n",
        "    val_mask = np.isin(piece_ids, val_pieces)\n",
        "    test_mask = np.isin(piece_ids, test_pieces)\n",
        "\n",
        "    print(f\"üìä DATA SPLIT:\")\n",
        "    print(f\"Train pieces: {len(train_pieces)} | segments: {np.sum(train_mask)}\")\n",
        "    print(f\"Val pieces:   {len(val_pieces)} | segments: {np.sum(val_mask)}\")\n",
        "    print(f\"Test pieces:  {len(test_pieces)} | segments: {np.sum(test_mask)}\")\n",
        "\n",
        "    # Show class distribution per split\n",
        "    for split_name, mask in [(\"Train\", train_mask), (\"Val\", val_mask), (\"Test\", test_mask)]:\n",
        "        split_labels = labels[mask]\n",
        "        print(f\"\\n{split_name} distribution:\")\n",
        "        for i, composer in enumerate(TARGET_COMPOSERS):\n",
        "            count = np.sum(split_labels == i)\n",
        "            percentage = (count / len(split_labels)) * 100 if len(split_labels) > 0 else 0\n",
        "            print(f\"  {composer}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = PianoRollDataset(data[train_mask], labels[train_mask])\n",
        "    val_dataset = PianoRollDataset(data[val_mask], labels[val_mask])\n",
        "    test_dataset = PianoRollDataset(data[test_mask], labels[test_mask])\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset, train_mask, val_mask, test_mask\n",
        "\n",
        "# Create the balanced training setup\n",
        "train_dataset, val_dataset, test_dataset, train_mask, val_mask, test_mask = create_balanced_training_setup(\n",
        "    tracked_data, tracked_labels, tracked_piece_ids, class_weights\n",
        ")\n",
        "\n",
        "# Create weighted loss function\n",
        "weighted_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "print(f\"\\n‚úÖ Weighted loss function created with class weights\")\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 16  # Smaller batch size for stability\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"\\nüìö DATA LOADERS CREATED:\")\n",
        "print(f\"‚Ä¢ Batch size: {batch_size}\")\n",
        "print(f\"‚Ä¢ Train batches: {len(train_loader)}\")\n",
        "print(f\"‚Ä¢ Val batches: {len(val_loader)}\")\n",
        "print(f\"‚Ä¢ Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "73cd4b0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73cd4b0a",
        "outputId": "f93b773e-c659-4342-d48c-9a390fa43dc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting training of AGGRESSIVE CNN-LSTM-Transformer...\n",
            "üöÄ STARTING AGGRESSIVE MODEL TRAINING...\n",
            "‚Ä¢ Model parameters: 185,688,776\n",
            "‚Ä¢ Training samples: 1986\n",
            "‚Ä¢ Validation samples: 438\n",
            "‚Ä¢ Epochs: 50\n",
            "‚Ä¢ Initial learning rate: 0.001\n",
            "‚Ä¢ Using mixed precision: True\n",
            "‚Ä¢ Class weights: tensor([0.7927, 0.7847, 1.2697, 1.4781], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-838904434.py:48: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
            "/tmp/ipython-input-838904434.py:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ Training Configuration:\n",
            "‚Ä¢ Optimizer: AdamW (lr=0.001, weight_decay=1e-4)\n",
            "‚Ä¢ Scheduler: CosineAnnealingWarmRestarts (T_0=10, T_mult=2)\n",
            "‚Ä¢ Main loss weight: 0.7, Auxiliary loss weight: 0.3\n",
            "‚Ä¢ Early stopping patience: 15\n",
            "‚Ä¢ Mixed precision: CUDA\n",
            "\n",
            "üìà Epoch 1/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3910 | Acc: 18.75% | LR: 1.00e-03\n",
            "  Batch  20/125 | Loss: 1.3624 | Acc: 24.11% | LR: 1.00e-03\n",
            "  Batch  40/125 | Loss: 1.4440 | Acc: 25.30% | LR: 1.00e-03\n",
            "  Batch  60/125 | Loss: 1.4427 | Acc: 24.59% | LR: 1.00e-03\n",
            "  Batch  80/125 | Loss: 1.4067 | Acc: 24.69% | LR: 1.00e-03\n",
            "  Batch 100/125 | Loss: 1.4420 | Acc: 24.69% | LR: 1.00e-03\n",
            "  Batch 120/125 | Loss: 1.3724 | Acc: 24.54% | LR: 1.00e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-838904434.py:146: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Epoch 1 Summary:\n",
            "  Train: Loss=1.4035, Acc=24.82%\n",
            "  Val:   Loss=1.3629, Acc=36.53%\n",
            "  LR: 9.76e-04, Time: 38.7s\n",
            "  üíæ New best model saved! Val Acc: 36.53%\n",
            "\n",
            "üìà Epoch 2/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.4618 | Acc: 6.25% | LR: 9.76e-04\n",
            "  Batch  20/125 | Loss: 1.3778 | Acc: 25.00% | LR: 9.76e-04\n",
            "  Batch  40/125 | Loss: 1.3749 | Acc: 25.00% | LR: 9.76e-04\n",
            "  Batch  60/125 | Loss: 1.4004 | Acc: 24.90% | LR: 9.76e-04\n",
            "  Batch  80/125 | Loss: 1.4130 | Acc: 23.46% | LR: 9.76e-04\n",
            "  Batch 100/125 | Loss: 1.3832 | Acc: 23.76% | LR: 9.76e-04\n",
            "  Batch 120/125 | Loss: 1.3558 | Acc: 23.66% | LR: 9.76e-04\n",
            "\n",
            "üìä Epoch 2 Summary:\n",
            "  Train: Loss=1.3920, Acc=23.72%\n",
            "  Val:   Loss=1.3971, Acc=12.10%\n",
            "  LR: 9.05e-04, Time: 38.0s\n",
            "  ‚è≥ Patience: 1/15\n",
            "\n",
            "üìà Epoch 3/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.4042 | Acc: 18.75% | LR: 9.05e-04\n",
            "  Batch  20/125 | Loss: 1.3450 | Acc: 30.36% | LR: 9.05e-04\n",
            "  Batch  40/125 | Loss: 1.3912 | Acc: 26.37% | LR: 9.05e-04\n",
            "  Batch  60/125 | Loss: 1.4629 | Acc: 25.72% | LR: 9.05e-04\n",
            "  Batch  80/125 | Loss: 1.3831 | Acc: 25.15% | LR: 9.05e-04\n",
            "  Batch 100/125 | Loss: 1.3794 | Acc: 25.12% | LR: 9.05e-04\n",
            "  Batch 120/125 | Loss: 1.3839 | Acc: 24.23% | LR: 9.05e-04\n",
            "\n",
            "üìä Epoch 3 Summary:\n",
            "  Train: Loss=1.3916, Acc=24.37%\n",
            "  Val:   Loss=1.4279, Acc=12.10%\n",
            "  LR: 7.94e-04, Time: 38.0s\n",
            "  ‚è≥ Patience: 2/15\n",
            "\n",
            "üìà Epoch 4/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3418 | Acc: 43.75% | LR: 7.94e-04\n",
            "  Batch  20/125 | Loss: 1.4172 | Acc: 19.94% | LR: 7.94e-04\n",
            "  Batch  40/125 | Loss: 1.4081 | Acc: 20.58% | LR: 7.94e-04\n",
            "  Batch  60/125 | Loss: 1.4268 | Acc: 22.54% | LR: 7.94e-04\n",
            "  Batch  80/125 | Loss: 1.3805 | Acc: 22.22% | LR: 7.94e-04\n",
            "  Batch 100/125 | Loss: 1.3813 | Acc: 23.45% | LR: 7.94e-04\n",
            "  Batch 120/125 | Loss: 1.4031 | Acc: 22.83% | LR: 7.94e-04\n",
            "\n",
            "üìä Epoch 4 Summary:\n",
            "  Train: Loss=1.3907, Acc=22.76%\n",
            "  Val:   Loss=1.4118, Acc=12.10%\n",
            "  LR: 6.55e-04, Time: 38.0s\n",
            "  ‚è≥ Patience: 3/15\n",
            "\n",
            "üìà Epoch 5/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3701 | Acc: 31.25% | LR: 6.55e-04\n",
            "  Batch  20/125 | Loss: 1.3784 | Acc: 24.40% | LR: 6.55e-04\n",
            "  Batch  40/125 | Loss: 1.3692 | Acc: 23.63% | LR: 6.55e-04\n",
            "  Batch  60/125 | Loss: 1.3661 | Acc: 23.05% | LR: 6.55e-04\n",
            "  Batch  80/125 | Loss: 1.4424 | Acc: 23.53% | LR: 6.55e-04\n",
            "  Batch 100/125 | Loss: 1.4028 | Acc: 22.96% | LR: 6.55e-04\n",
            "  Batch 120/125 | Loss: 1.3999 | Acc: 23.61% | LR: 6.55e-04\n",
            "\n",
            "üìä Epoch 5 Summary:\n",
            "  Train: Loss=1.3876, Acc=23.72%\n",
            "  Val:   Loss=1.3879, Acc=35.16%\n",
            "  LR: 5.01e-04, Time: 38.0s\n",
            "  ‚è≥ Patience: 4/15\n",
            "\n",
            "üìà Epoch 6/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3757 | Acc: 31.25% | LR: 5.01e-04\n",
            "  Batch  20/125 | Loss: 1.3850 | Acc: 27.98% | LR: 5.01e-04\n",
            "  Batch  40/125 | Loss: 1.4352 | Acc: 25.76% | LR: 5.01e-04\n",
            "  Batch  60/125 | Loss: 1.3505 | Acc: 23.98% | LR: 5.01e-04\n",
            "  Batch  80/125 | Loss: 1.3497 | Acc: 24.77% | LR: 5.01e-04\n",
            "  Batch 100/125 | Loss: 1.3689 | Acc: 24.01% | LR: 5.01e-04\n",
            "  Batch 120/125 | Loss: 1.3727 | Acc: 24.64% | LR: 5.01e-04\n",
            "\n",
            "üìä Epoch 6 Summary:\n",
            "  Train: Loss=1.3856, Acc=24.52%\n",
            "  Val:   Loss=1.4161, Acc=12.10%\n",
            "  LR: 3.46e-04, Time: 38.0s\n",
            "  ‚è≥ Patience: 5/15\n",
            "\n",
            "üìà Epoch 7/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.4202 | Acc: 12.50% | LR: 3.46e-04\n",
            "  Batch  20/125 | Loss: 1.3751 | Acc: 19.64% | LR: 3.46e-04\n",
            "  Batch  40/125 | Loss: 1.3797 | Acc: 19.05% | LR: 3.46e-04\n",
            "  Batch  60/125 | Loss: 1.3752 | Acc: 22.95% | LR: 3.46e-04\n",
            "  Batch  80/125 | Loss: 1.4062 | Acc: 23.46% | LR: 3.46e-04\n",
            "  Batch 100/125 | Loss: 1.3793 | Acc: 23.89% | LR: 3.46e-04\n",
            "  Batch 120/125 | Loss: 1.4058 | Acc: 23.40% | LR: 3.46e-04\n",
            "\n",
            "üìä Epoch 7 Summary:\n",
            "  Train: Loss=1.3880, Acc=23.36%\n",
            "  Val:   Loss=1.4096, Acc=12.10%\n",
            "  LR: 2.07e-04, Time: 38.0s\n",
            "  ‚è≥ Patience: 6/15\n",
            "\n",
            "üìà Epoch 8/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3533 | Acc: 37.50% | LR: 2.07e-04\n",
            "  Batch  20/125 | Loss: 1.3954 | Acc: 25.60% | LR: 2.07e-04\n",
            "  Batch  40/125 | Loss: 1.4030 | Acc: 24.70% | LR: 2.07e-04\n",
            "  Batch  60/125 | Loss: 1.4021 | Acc: 23.57% | LR: 2.07e-04\n",
            "  Batch  80/125 | Loss: 1.4024 | Acc: 22.76% | LR: 2.07e-04\n",
            "  Batch 100/125 | Loss: 1.3997 | Acc: 22.65% | LR: 2.07e-04\n",
            "  Batch 120/125 | Loss: 1.3912 | Acc: 22.00% | LR: 2.07e-04\n",
            "\n",
            "üìä Epoch 8 Summary:\n",
            "  Train: Loss=1.3846, Acc=22.05%\n",
            "  Val:   Loss=1.4008, Acc=12.10%\n",
            "  LR: 9.64e-05, Time: 38.0s\n",
            "  ‚è≥ Patience: 7/15\n",
            "\n",
            "üìà Epoch 9/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.4031 | Acc: 12.50% | LR: 9.64e-05\n",
            "  Batch  20/125 | Loss: 1.3769 | Acc: 22.62% | LR: 9.64e-05\n",
            "  Batch  40/125 | Loss: 1.4171 | Acc: 21.19% | LR: 9.64e-05\n",
            "  Batch  60/125 | Loss: 1.3692 | Acc: 22.54% | LR: 9.64e-05\n",
            "  Batch  80/125 | Loss: 1.3755 | Acc: 22.38% | LR: 9.64e-05\n",
            "  Batch 100/125 | Loss: 1.3781 | Acc: 22.71% | LR: 9.64e-05\n",
            "  Batch 120/125 | Loss: 1.3912 | Acc: 22.16% | LR: 9.64e-05\n",
            "\n",
            "üìä Epoch 9 Summary:\n",
            "  Train: Loss=1.3841, Acc=22.36%\n",
            "  Val:   Loss=1.4012, Acc=12.10%\n",
            "  LR: 2.54e-05, Time: 38.0s\n",
            "  ‚è≥ Patience: 8/15\n",
            "\n",
            "üìà Epoch 10/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3814 | Acc: 18.75% | LR: 2.54e-05\n",
            "  Batch  20/125 | Loss: 1.3932 | Acc: 22.62% | LR: 2.54e-05\n",
            "  Batch  40/125 | Loss: 1.3913 | Acc: 22.41% | LR: 2.54e-05\n",
            "  Batch  60/125 | Loss: 1.3642 | Acc: 22.03% | LR: 2.54e-05\n",
            "  Batch  80/125 | Loss: 1.3945 | Acc: 21.84% | LR: 2.54e-05\n",
            "  Batch 100/125 | Loss: 1.3699 | Acc: 22.09% | LR: 2.54e-05\n",
            "  Batch 120/125 | Loss: 1.3967 | Acc: 22.52% | LR: 2.54e-05\n",
            "\n",
            "üìä Epoch 10 Summary:\n",
            "  Train: Loss=1.3850, Acc=22.26%\n",
            "  Val:   Loss=1.4016, Acc=12.10%\n",
            "  LR: 1.00e-03, Time: 38.0s\n",
            "  ‚è≥ Patience: 9/15\n",
            "\n",
            "üìà Epoch 11/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: 1.3755 | Acc: 25.00% | LR: 1.00e-03\n",
            "  Batch  20/125 | Loss: 1.3741 | Acc: 20.24% | LR: 1.00e-03\n",
            "  Batch  40/125 | Loss: 1.3921 | Acc: 21.49% | LR: 1.00e-03\n",
            "  Batch  60/125 | Loss: 1.3805 | Acc: 21.72% | LR: 1.00e-03\n",
            "  Batch  80/125 | Loss: 1.3334 | Acc: 22.99% | LR: 1.00e-03\n",
            "  Batch 100/125 | Loss: nan | Acc: 23.45% | LR: 1.00e-03\n",
            "  Batch 120/125 | Loss: nan | Acc: 23.61% | LR: 1.00e-03\n",
            "\n",
            "üìä Epoch 11 Summary:\n",
            "  Train: Loss=nan, Acc=23.82%\n",
            "  Val:   Loss=1.4435, Acc=12.10%\n",
            "  LR: 9.94e-04, Time: 37.6s\n",
            "  ‚è≥ Patience: 10/15\n",
            "\n",
            "üìà Epoch 12/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: nan | Acc: 12.50% | LR: 9.94e-04\n",
            "  Batch  20/125 | Loss: nan | Acc: 22.02% | LR: 9.94e-04\n",
            "  Batch  40/125 | Loss: nan | Acc: 22.10% | LR: 9.94e-04\n",
            "  Batch  60/125 | Loss: nan | Acc: 21.82% | LR: 9.94e-04\n",
            "  Batch  80/125 | Loss: nan | Acc: 21.68% | LR: 9.94e-04\n",
            "  Batch 100/125 | Loss: nan | Acc: 21.41% | LR: 9.94e-04\n",
            "  Batch 120/125 | Loss: 1.3923 | Acc: 20.66% | LR: 9.94e-04\n",
            "\n",
            "üìä Epoch 12 Summary:\n",
            "  Train: Loss=nan, Acc=20.64%\n",
            "  Val:   Loss=nan, Acc=35.16%\n",
            "  LR: 9.76e-04, Time: 36.4s\n",
            "  ‚è≥ Patience: 11/15\n",
            "\n",
            "üìà Epoch 13/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: nan | Acc: 37.50% | LR: 9.76e-04\n",
            "  Batch  20/125 | Loss: nan | Acc: 30.65% | LR: 9.76e-04\n",
            "  Batch  40/125 | Loss: nan | Acc: 32.01% | LR: 9.76e-04\n",
            "  Batch  60/125 | Loss: nan | Acc: 30.74% | LR: 9.76e-04\n",
            "  Batch  80/125 | Loss: nan | Acc: 30.40% | LR: 9.76e-04\n",
            "  Batch 100/125 | Loss: nan | Acc: 30.75% | LR: 9.76e-04\n",
            "  Batch 120/125 | Loss: nan | Acc: 30.84% | LR: 9.76e-04\n",
            "\n",
            "üìä Epoch 13 Summary:\n",
            "  Train: Loss=nan, Acc=30.51%\n",
            "  Val:   Loss=nan, Acc=35.16%\n",
            "  LR: 9.46e-04, Time: 36.3s\n",
            "  ‚è≥ Patience: 12/15\n",
            "\n",
            "üìà Epoch 14/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: nan | Acc: 37.50% | LR: 9.46e-04\n",
            "  Batch  20/125 | Loss: nan | Acc: 31.55% | LR: 9.46e-04\n",
            "  Batch  40/125 | Loss: nan | Acc: 30.95% | LR: 9.46e-04\n",
            "  Batch  60/125 | Loss: nan | Acc: 30.94% | LR: 9.46e-04\n",
            "  Batch  80/125 | Loss: nan | Acc: 30.40% | LR: 9.46e-04\n",
            "  Batch 100/125 | Loss: nan | Acc: 29.46% | LR: 9.46e-04\n",
            "  Batch 120/125 | Loss: nan | Acc: 30.53% | LR: 9.46e-04\n",
            "\n",
            "üìä Epoch 14 Summary:\n",
            "  Train: Loss=nan, Acc=30.51%\n",
            "  Val:   Loss=nan, Acc=35.16%\n",
            "  LR: 9.05e-04, Time: 36.4s\n",
            "  ‚è≥ Patience: 13/15\n",
            "\n",
            "üìà Epoch 15/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: nan | Acc: 50.00% | LR: 9.05e-04\n",
            "  Batch  20/125 | Loss: nan | Acc: 32.44% | LR: 9.05e-04\n",
            "  Batch  40/125 | Loss: nan | Acc: 31.71% | LR: 9.05e-04\n",
            "  Batch  60/125 | Loss: nan | Acc: 32.58% | LR: 9.05e-04\n",
            "  Batch  80/125 | Loss: nan | Acc: 31.87% | LR: 9.05e-04\n",
            "  Batch 100/125 | Loss: nan | Acc: 31.37% | LR: 9.05e-04\n",
            "  Batch 120/125 | Loss: nan | Acc: 30.84% | LR: 9.05e-04\n",
            "\n",
            "üìä Epoch 15 Summary:\n",
            "  Train: Loss=nan, Acc=30.51%\n",
            "  Val:   Loss=nan, Acc=35.16%\n",
            "  LR: 8.54e-04, Time: 36.3s\n",
            "  ‚è≥ Patience: 14/15\n",
            "\n",
            "üìà Epoch 16/50\n",
            "--------------------------------------------------\n",
            "  Batch   0/125 | Loss: nan | Acc: 31.25% | LR: 8.54e-04\n",
            "  Batch  20/125 | Loss: nan | Acc: 30.36% | LR: 8.54e-04\n",
            "  Batch  40/125 | Loss: nan | Acc: 27.74% | LR: 8.54e-04\n",
            "  Batch  60/125 | Loss: nan | Acc: 28.79% | LR: 8.54e-04\n",
            "  Batch  80/125 | Loss: nan | Acc: 29.71% | LR: 8.54e-04\n",
            "  Batch 100/125 | Loss: nan | Acc: 30.69% | LR: 8.54e-04\n",
            "  Batch 120/125 | Loss: nan | Acc: 30.68% | LR: 8.54e-04\n",
            "\n",
            "üìä Epoch 16 Summary:\n",
            "  Train: Loss=nan, Acc=30.51%\n",
            "  Val:   Loss=nan, Acc=35.16%\n",
            "  LR: 7.94e-04, Time: 36.3s\n",
            "  ‚è≥ Patience: 15/15\n",
            "\n",
            "üõë Early stopping triggered! Best Val Acc: 36.53%\n",
            "\n",
            "‚úÖ Training completed!\n",
            "Best validation accuracy: 36.53%\n",
            "Model saved to: saved_models/aggressive_cnn_lstm_transformer.pth\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# ADVANCED TRAINING LOOP FOR AGGRESSIVE MODEL\n",
        "# =====================================================\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_aggressive_model(model, train_loader, val_loader, class_weights,\n",
        "                          epochs=50, initial_lr=1e-3, save_path='aggressive_model.pth'):\n",
        "    \"\"\"\n",
        "    Advanced training with:\n",
        "    - Auxiliary loss for regularization\n",
        "    - Cosine annealing with warm restarts\n",
        "    - Mixed precision training\n",
        "    - Advanced metrics tracking\n",
        "    - Early stopping with patience\n",
        "    \"\"\"\n",
        "    print(\"üöÄ STARTING AGGRESSIVE MODEL TRAINING...\")\n",
        "    print(f\"‚Ä¢ Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"‚Ä¢ Training samples: {len(train_loader.dataset)}\")\n",
        "    print(f\"‚Ä¢ Validation samples: {len(val_loader.dataset)}\")\n",
        "    print(f\"‚Ä¢ Epochs: {epochs}\")\n",
        "    print(f\"‚Ä¢ Initial learning rate: {initial_lr}\")\n",
        "    print(f\"‚Ä¢ Using mixed precision: True\")\n",
        "    print(f\"‚Ä¢ Class weights: {class_weights}\")\n",
        "\n",
        "    # Setup optimizers and schedulers\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=initial_lr,\n",
        "        weight_decay=1e-4,\n",
        "        betas=(0.9, 0.999),\n",
        "        eps=1e-8\n",
        "    )\n",
        "\n",
        "    # Cosine annealing with warm restarts\n",
        "    scheduler = CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=10,  # Restart every 10 epochs\n",
        "        T_mult=2,  # Double the period after each restart\n",
        "        eta_min=1e-6\n",
        "    )\n",
        "\n",
        "    # Mixed precision scaler\n",
        "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
        "\n",
        "    # Loss functions\n",
        "    main_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    aux_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    # Training tracking\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [], 'train_main_loss': [], 'train_aux_loss': [],\n",
        "        'val_loss': [], 'val_acc': [], 'val_main_loss': [], 'val_aux_loss': [],\n",
        "        'lr': [], 'epoch_time': []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    patience = 15  # Early stopping patience\n",
        "\n",
        "    print(\"\\nüéØ Training Configuration:\")\n",
        "    print(f\"‚Ä¢ Optimizer: AdamW (lr={initial_lr}, weight_decay=1e-4)\")\n",
        "    print(f\"‚Ä¢ Scheduler: CosineAnnealingWarmRestarts (T_0=10, T_mult=2)\")\n",
        "    print(f\"‚Ä¢ Main loss weight: 0.7, Auxiliary loss weight: 0.3\")\n",
        "    print(f\"‚Ä¢ Early stopping patience: {patience}\")\n",
        "    print(f\"‚Ä¢ Mixed precision: {'CUDA' if scaler else 'Disabled'}\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        # ==========================================\n",
        "        # TRAINING PHASE\n",
        "        # ==========================================\n",
        "        model.train()\n",
        "        train_metrics = defaultdict(float)\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        print(f\"\\nüìà Epoch {epoch+1}/{epochs}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Mixed precision forward pass\n",
        "            if scaler:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    main_output, aux_output = model(data)\n",
        "                    main_loss = main_criterion(main_output, target)\n",
        "                    aux_loss = aux_criterion(aux_output, target)\n",
        "                    # Combine losses with weights\n",
        "                    total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
        "\n",
        "                # Mixed precision backward pass\n",
        "                scaler.scale(total_loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                main_output, aux_output = model(data)\n",
        "                main_loss = main_criterion(main_output, target)\n",
        "                aux_loss = aux_criterion(aux_output, target)\n",
        "                total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
        "\n",
        "                total_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Track metrics\n",
        "            train_metrics['total_loss'] += total_loss.item()\n",
        "            train_metrics['main_loss'] += main_loss.item()\n",
        "            train_metrics['aux_loss'] += aux_loss.item()\n",
        "\n",
        "            # Accuracy from main output\n",
        "            _, predicted = torch.max(main_output.data, 1)\n",
        "            train_total += target.size(0)\n",
        "            train_correct += (predicted == target).sum().item()\n",
        "\n",
        "            # Progress update\n",
        "            if batch_idx % 20 == 0:\n",
        "                current_lr = optimizer.param_groups[0]['lr']\n",
        "                print(f\"  Batch {batch_idx:3d}/{len(train_loader)} | \"\n",
        "                      f\"Loss: {total_loss.item():.4f} | \"\n",
        "                      f\"Acc: {100.*train_correct/train_total:.2f}% | \"\n",
        "                      f\"LR: {current_lr:.2e}\")\n",
        "\n",
        "        # ==========================================\n",
        "        # VALIDATION PHASE\n",
        "        # ==========================================\n",
        "        model.eval()\n",
        "        val_metrics = defaultdict(float)\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "\n",
        "                if scaler:\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        main_output, aux_output = model(data)\n",
        "                        main_loss = main_criterion(main_output, target)\n",
        "                        aux_loss = aux_criterion(aux_output, target)\n",
        "                        total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
        "                else:\n",
        "                    main_output, aux_output = model(data)\n",
        "                    main_loss = main_criterion(main_output, target)\n",
        "                    aux_loss = aux_criterion(aux_output, target)\n",
        "                    total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
        "\n",
        "                val_metrics['total_loss'] += total_loss.item()\n",
        "                val_metrics['main_loss'] += main_loss.item()\n",
        "                val_metrics['aux_loss'] += aux_loss.item()\n",
        "\n",
        "                _, predicted = torch.max(main_output.data, 1)\n",
        "                val_total += target.size(0)\n",
        "                val_correct += (predicted == target).sum().item()\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        train_loss = train_metrics['total_loss'] / len(train_loader)\n",
        "        train_acc = 100. * train_correct / train_total\n",
        "        val_loss = val_metrics['total_loss'] / len(val_loader)\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['train_main_loss'].append(train_metrics['main_loss'] / len(train_loader))\n",
        "        history['train_aux_loss'].append(train_metrics['aux_loss'] / len(train_loader))\n",
        "\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['val_main_loss'].append(val_metrics['main_loss'] / len(val_loader))\n",
        "        history['val_aux_loss'].append(val_metrics['aux_loss'] / len(val_loader))\n",
        "\n",
        "        history['lr'].append(current_lr)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        history['epoch_time'].append(epoch_time)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"\\nüìä Epoch {epoch+1} Summary:\")\n",
        "        print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
        "        print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
        "        print(f\"  LR: {current_lr:.2e}, Time: {epoch_time:.1f}s\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_val_acc': best_val_acc,\n",
        "                'history': history\n",
        "            }, save_path)\n",
        "            print(f\"  üíæ New best model saved! Val Acc: {val_acc:.2f}%\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"  ‚è≥ Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\nüõë Early stopping triggered! Best Val Acc: {best_val_acc:.2f}%\")\n",
        "            break\n",
        "\n",
        "        # Memory cleanup\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"\\n‚úÖ Training completed!\")\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "    print(f\"Model saved to: {save_path}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "# Create save directory\n",
        "os.makedirs('saved_models', exist_ok=True)\n",
        "\n",
        "# Start training the aggressive model\n",
        "print(\"üöÄ Starting training of AGGRESSIVE CNN-LSTM-Transformer...\")\n",
        "history = train_aggressive_model(\n",
        "    aggressive_model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    class_weights,\n",
        "    epochs=50,\n",
        "    initial_lr=1e-3,\n",
        "    save_path='saved_models/aggressive_cnn_lstm_transformer.pth'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# TRAINING STABILITY FIXES FOR AGGRESSIVE MODEL\n",
        "# =====================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "def train_stable_aggressive_model(model, train_loader, val_loader, class_weights,\n",
        "                                 epochs=50, initial_lr=5e-4, save_path='stable_aggressive_model.pth'):\n",
        "    \"\"\"\n",
        "    STABLE training with fixes for NaN loss:\n",
        "    - Gradient clipping\n",
        "    - Lower learning rate\n",
        "    - Gradient accumulation\n",
        "    - Better initialization\n",
        "    - Loss scaling protection\n",
        "    - Model parameter monitoring\n",
        "    \"\"\"\n",
        "    print(\"üîß STARTING STABLE AGGRESSIVE MODEL TRAINING...\")\n",
        "    print(\"üõ°Ô∏è STABILITY FIXES APPLIED:\")\n",
        "    print(\"‚Ä¢ Gradient clipping (max_norm=1.0)\")\n",
        "    print(\"‚Ä¢ Lower learning rate (5e-4 ‚Üí 1e-4)\")\n",
        "    print(\"‚Ä¢ Gradient accumulation (effective batch size x2)\")\n",
        "    print(\"‚Ä¢ Weight initialization check\")\n",
        "    print(\"‚Ä¢ NaN detection and recovery\")\n",
        "    print(\"‚Ä¢ Mixed precision with loss scaling\")\n",
        "\n",
        "    # ==========================================\n",
        "    # MODEL STABILITY CHECKS\n",
        "    # ==========================================\n",
        "\n",
        "    # Check for NaN in initial weights\n",
        "    def check_model_weights(model, name=\"\"):\n",
        "        nan_count = 0\n",
        "        inf_count = 0\n",
        "        total_params = 0\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if torch.isnan(param).any():\n",
        "                print(f\"‚ö†Ô∏è NaN detected in {name}\")\n",
        "                nan_count += 1\n",
        "            if torch.isinf(param).any():\n",
        "                print(f\"‚ö†Ô∏è Inf detected in {name}\")\n",
        "                inf_count += 1\n",
        "            total_params += param.numel()\n",
        "\n",
        "        print(f\"Model check: {nan_count} NaN params, {inf_count} Inf params, {total_params:,} total\")\n",
        "        return nan_count == 0 and inf_count == 0\n",
        "\n",
        "    # Initialize model weights properly\n",
        "    def init_weights(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_normal_(m.weight, gain=0.1)  # Smaller gain\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu', a=0.1)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LSTM):\n",
        "            for name, param in m.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    nn.init.orthogonal_(param, gain=0.1)\n",
        "                elif 'bias' in name:\n",
        "                    nn.init.constant_(param, 0)\n",
        "\n",
        "    print(\"üîÑ Reinitializing model weights with smaller scale...\")\n",
        "    model.apply(init_weights)\n",
        "\n",
        "    if not check_model_weights(model, \"After reinitialization\"):\n",
        "        print(\"‚ùå Model has NaN/Inf after initialization!\")\n",
        "        return None\n",
        "\n",
        "    print(\"‚úÖ Model weights are stable\")\n",
        "\n",
        "    # ==========================================\n",
        "    # OPTIMIZER & SCHEDULER SETUP\n",
        "    # ==========================================\n",
        "\n",
        "    # Much more conservative optimizer settings\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=initial_lr,  # Reduced from 1e-3 to 5e-4\n",
        "        weight_decay=1e-5,  # Reduced weight decay\n",
        "        betas=(0.9, 0.98),  # More stable betas\n",
        "        eps=1e-6  # Larger epsilon\n",
        "    )\n",
        "\n",
        "    # More conservative scheduler\n",
        "    scheduler = CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=20,  # Longer restart period\n",
        "        T_mult=1,  # No multiplication\n",
        "        eta_min=1e-7\n",
        "    )\n",
        "\n",
        "    # Mixed precision with careful scaling\n",
        "    scaler = torch.cuda.amp.GradScaler(\n",
        "        init_scale=2**10,  # Smaller initial scale\n",
        "        growth_factor=1.1,  # Slower growth\n",
        "        backoff_factor=0.8,  # More aggressive backoff\n",
        "        growth_interval=100  # Less frequent growth\n",
        "    ) if device.type == 'cuda' else None\n",
        "\n",
        "    # Loss functions with label smoothing for stability\n",
        "    main_criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
        "    aux_criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
        "\n",
        "    # Training tracking\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [], 'train_main_loss': [], 'train_aux_loss': [],\n",
        "        'val_loss': [], 'val_acc': [], 'val_main_loss': [], 'val_aux_loss': [],\n",
        "        'lr': [], 'epoch_time': [], 'grad_norm': []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    patience = 20  # Increased patience\n",
        "    accumulation_steps = 2  # Gradient accumulation\n",
        "\n",
        "    print(f\"\\nüéØ STABLE Training Configuration:\")\n",
        "    print(f\"‚Ä¢ Learning rate: {initial_lr} (reduced)\")\n",
        "    print(f\"‚Ä¢ Weight decay: 1e-5 (reduced)\")\n",
        "    print(f\"‚Ä¢ Gradient clipping: max_norm=1.0\")\n",
        "    print(f\"‚Ä¢ Gradient accumulation: {accumulation_steps} steps\")\n",
        "    print(f\"‚Ä¢ Label smoothing: 0.1\")\n",
        "    print(f\"‚Ä¢ Patience: {patience}\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        # ==========================================\n",
        "        # TRAINING PHASE WITH STABILITY\n",
        "        # ==========================================\n",
        "        model.train()\n",
        "        train_metrics = defaultdict(float)\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        grad_norms = []\n",
        "\n",
        "        print(f\"\\nüìà Epoch {epoch+1}/{epochs}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Mixed precision forward pass\n",
        "            if scaler:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    main_output, aux_output = model(data)\n",
        "                    main_loss = main_criterion(main_output, target)\n",
        "                    aux_loss = aux_criterion(aux_output, target)\n",
        "                    total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
        "                    # Scale loss for gradient accumulation\n",
        "                    total_loss = total_loss / accumulation_steps\n",
        "\n",
        "                # Backward pass\n",
        "                scaler.scale(total_loss).backward()\n",
        "            else:\n",
        "                main_output, aux_output = model(data)\n",
        "                main_loss = main_criterion(main_output, target)\n",
        "                aux_loss = aux_criterion(aux_output, target)\n",
        "                total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
        "                total_loss = total_loss / accumulation_steps\n",
        "                total_loss.backward()\n",
        "\n",
        "            # Gradient accumulation\n",
        "            if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                if scaler:\n",
        "                    # Gradient clipping before optimizer step\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    grad_norms.append(grad_norm.item())\n",
        "\n",
        "                    # Check for NaN gradients\n",
        "                    if torch.isnan(grad_norm) or torch.isinf(grad_norm):\n",
        "                        print(f\"‚ö†Ô∏è NaN/Inf gradient detected at batch {batch_idx}, skipping...\")\n",
        "                        scaler.update()\n",
        "                        optimizer.zero_grad()\n",
        "                        continue\n",
        "\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    grad_norms.append(grad_norm.item())\n",
        "\n",
        "                    if torch.isnan(grad_norm) or torch.isinf(grad_norm):\n",
        "                        print(f\"‚ö†Ô∏è NaN/Inf gradient detected at batch {batch_idx}, skipping...\")\n",
        "                        optimizer.zero_grad()\n",
        "                        continue\n",
        "\n",
        "                    optimizer.step()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Track metrics (scale back the loss)\n",
        "            actual_loss = total_loss.item() * accumulation_steps\n",
        "            if not (torch.isnan(torch.tensor(actual_loss)) or torch.isinf(torch.tensor(actual_loss))):\n",
        "                train_metrics['total_loss'] += actual_loss\n",
        "                train_metrics['main_loss'] += main_loss.item()\n",
        "                train_metrics['aux_loss'] += aux_loss.item()\n",
        "\n",
        "                # Accuracy from main output\n",
        "                _, predicted = torch.max(main_output.data, 1)\n",
        "                train_total += target.size(0)\n",
        "                train_correct += (predicted == target).sum().item()\n",
        "\n",
        "            # Progress update\n",
        "            if batch_idx % 20 == 0:\n",
        "                current_lr = optimizer.param_groups[0]['lr']\n",
        "                avg_grad_norm = np.mean(grad_norms[-10:]) if grad_norms else 0.0\n",
        "                print(f\"  Batch {batch_idx:3d}/{len(train_loader)} | \"\n",
        "                      f\"Loss: {actual_loss:.4f} | \"\n",
        "                      f\"Acc: {100.*train_correct/train_total:.2f}% | \"\n",
        "                      f\"LR: {current_lr:.2e} | \"\n",
        "                      f\"GradNorm: {avg_grad_norm:.3f}\")\n",
        "\n",
        "        # ==========================================\n",
        "        # VALIDATION PHASE\n",
        "        # ==========================================\n",
        "        model.eval()\n",
        "        val_metrics = defaultdict(float)\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "\n",
        "                if scaler:\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        main_output, aux_output = model(data)\n",
        "                        main_loss = main_criterion(main_output, target)\n",
        "                        aux_loss = aux_criterion(aux_output, target)\n",
        "                        total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
        "                else:\n",
        "                    main_output, aux_output = model(data)\n",
        "                    main_loss = main_criterion(main_output, target)\n",
        "                    aux_loss = aux_criterion(aux_output, target)\n",
        "                    total_loss = 0.7 * main_loss + 0.3 * aux_loss\n",
        "\n",
        "                # Only add if not NaN\n",
        "                if not torch.isnan(total_loss):\n",
        "                    val_metrics['total_loss'] += total_loss.item()\n",
        "                    val_metrics['main_loss'] += main_loss.item()\n",
        "                    val_metrics['aux_loss'] += aux_loss.item()\n",
        "\n",
        "                    _, predicted = torch.max(main_output.data, 1)\n",
        "                    val_total += target.size(0)\n",
        "                    val_correct += (predicted == target).sum().item()\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        if len(train_loader) > 0 and train_total > 0:\n",
        "            train_loss = train_metrics['total_loss'] / len(train_loader)\n",
        "            train_acc = 100. * train_correct / train_total\n",
        "        else:\n",
        "            train_loss = float('inf')\n",
        "            train_acc = 0.0\n",
        "\n",
        "        if len(val_loader) > 0 and val_total > 0:\n",
        "            val_loss = val_metrics['total_loss'] / len(val_loader)\n",
        "            val_acc = 100. * val_correct / val_total\n",
        "        else:\n",
        "            val_loss = float('inf')\n",
        "            val_acc = 0.0\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Record history\n",
        "        if not (torch.isnan(torch.tensor(train_loss)) or torch.isnan(torch.tensor(val_loss))):\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['train_acc'].append(train_acc)\n",
        "            history['train_main_loss'].append(train_metrics['main_loss'] / len(train_loader))\n",
        "            history['train_aux_loss'].append(train_metrics['aux_loss'] / len(train_loader))\n",
        "\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['val_acc'].append(val_acc)\n",
        "            history['val_main_loss'].append(val_metrics['main_loss'] / len(val_loader))\n",
        "            history['val_aux_loss'].append(val_metrics['aux_loss'] / len(val_loader))\n",
        "\n",
        "            history['lr'].append(current_lr)\n",
        "            history['grad_norm'].append(np.mean(grad_norms) if grad_norms else 0.0)\n",
        "\n",
        "            epoch_time = time.time() - epoch_start_time\n",
        "            history['epoch_time'].append(epoch_time)\n",
        "\n",
        "            # Print epoch summary\n",
        "            print(f\"\\nüìä Epoch {epoch+1} Summary:\")\n",
        "            print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
        "            print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
        "            print(f\"  LR: {current_lr:.2e}, Time: {epoch_time:.1f}s\")\n",
        "            print(f\"  Avg Grad Norm: {np.mean(grad_norms):.3f}\")\n",
        "\n",
        "            # Save best model\n",
        "            if val_acc > best_val_acc and not torch.isnan(torch.tensor(val_acc)):\n",
        "                best_val_acc = val_acc\n",
        "                patience_counter = 0\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': scheduler.state_dict(),\n",
        "                    'best_val_acc': best_val_acc,\n",
        "                    'history': history\n",
        "                }, save_path)\n",
        "                print(f\"  üíæ New best model saved! Val Acc: {val_acc:.2f}%\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                print(f\"  ‚è≥ Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "            # Early stopping\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"\\nüõë Early stopping triggered! Best Val Acc: {best_val_acc:.2f}%\")\n",
        "                break\n",
        "        else:\n",
        "            print(f\"\\n‚ö†Ô∏è NaN loss detected in epoch {epoch+1}, continuing...\")\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Memory cleanup\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"\\n‚úÖ Stable training completed!\")\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "    print(f\"Model saved to: {save_path}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "# Create a fresh model instance with proper initialization\n",
        "print(\"üîÑ Creating fresh aggressive model with stability fixes...\")\n",
        "stable_aggressive_model = AggressiveCNN_LSTM_Transformer(\n",
        "    num_classes=4,\n",
        "    lstm_hidden=384,        # Slightly reduced from 512\n",
        "    transformer_dim=768,    # Slightly reduced from 1024\n",
        "    num_heads=12,          # Reduced from 16\n",
        "    num_layers=6           # Reduced from 8\n",
        ").to(device)\n",
        "\n",
        "print(\"üöÄ Starting STABLE training of aggressive model...\")\n",
        "stable_history = train_stable_aggressive_model(\n",
        "    stable_aggressive_model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    class_weights,\n",
        "    epochs=50,\n",
        "    initial_lr=5e-4,  # Much lower learning rate\n",
        "    save_path='saved_models/stable_aggressive_cnn_lstm_transformer.pth'\n",
        ")"
      ],
      "metadata": {
        "id": "fNJXxZ0Pvgy9",
        "outputId": "8fcbee86-e8b4-4579-c1a5-36ff046782cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "fNJXxZ0Pvgy9",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Creating fresh aggressive model with stability fixes...\n",
            "üöÄ Building AGGRESSIVE CNN-LSTM-Transformer for A100 40GB...\n",
            "‚Ä¢ Deep CNN feature extraction (6 blocks)\n",
            "‚Ä¢ Large LSTM temporal modeling (hidden: 384)\n",
            "‚Ä¢ Deep Transformer self-attention (dim: 768, heads: 12, layers: 6)\n",
            "‚Ä¢ Multi-scale feature fusion\n",
            "‚Ä¢ Advanced attention mechanisms\n",
            "‚úÖ AGGRESSIVE CNN-LSTM-Transformer architecture built!\n",
            "üöÄ Starting STABLE training of aggressive model...\n",
            "üîß STARTING STABLE AGGRESSIVE MODEL TRAINING...\n",
            "üõ°Ô∏è STABILITY FIXES APPLIED:\n",
            "‚Ä¢ Gradient clipping (max_norm=1.0)\n",
            "‚Ä¢ Lower learning rate (5e-4 ‚Üí 1e-4)\n",
            "‚Ä¢ Gradient accumulation (effective batch size x2)\n",
            "‚Ä¢ Weight initialization check\n",
            "‚Ä¢ NaN detection and recovery\n",
            "‚Ä¢ Mixed precision with loss scaling\n",
            "üîÑ Reinitializing model weights with smaller scale...\n",
            "Model check: 0 NaN params, 0 Inf params, 106,655,432 total\n",
            "‚úÖ Model weights are stable\n",
            "\n",
            "üéØ STABLE Training Configuration:\n",
            "‚Ä¢ Learning rate: 0.0005 (reduced)\n",
            "‚Ä¢ Weight decay: 1e-5 (reduced)\n",
            "‚Ä¢ Gradient clipping: max_norm=1.0\n",
            "‚Ä¢ Gradient accumulation: 2 steps\n",
            "‚Ä¢ Label smoothing: 0.1\n",
            "‚Ä¢ Patience: 20\n",
            "\n",
            "üìà Epoch 1/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1729208581.py:102: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(\n",
            "/tmp/ipython-input-1729208581.py:155: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   0/125 | Loss: 1.3884 | Acc: 18.75% | LR: 5.00e-04 | GradNorm: 0.000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1729208581.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üöÄ Starting STABLE training of aggressive model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m stable_history = train_stable_aggressive_model(\n\u001b[0m\u001b[1;32m    350\u001b[0m     \u001b[0mstable_aggressive_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1729208581.py\u001b[0m in \u001b[0;36mtrain_stable_aggressive_model\u001b[0;34m(model, train_loader, val_loader, class_weights, epochs, initial_lr, save_path)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# Track metrics (scale back the loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mactual_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mtrain_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mactual_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AITw-NWnvs_h"
      },
      "id": "AITw-NWnvs_h",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}